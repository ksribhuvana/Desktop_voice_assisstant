{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ksribhuvana/Desktop_voice_assisstant/blob/master/updated_chat_bot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "0OqQuDefWyzL",
        "outputId": "e26fd514-bced-40dd-8abb-53f6c4d95bf9"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-b34b71e8-3406-4af5-bad6-70c89c26032e\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-b34b71e8-3406-4af5-bad6-70c89c26032e\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving intents.json to intents.json\n"
          ]
        }
      ],
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Iw6dd9jbXGVN"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "with open('intents.json') as file:\n",
        "    intents = json.load(file, strict = False) # We don't read the file in strictly so we can use Python regular expressions like '/n' (this is very minor)\n",
        "intents = intents['intents'] # Get all of the individual intents from our dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JnfBOA5vXGXv",
        "outputId": "c0abd85b-2359-42a5-b1d8-c5591113a41f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{tag: greeting,\n",
            "patterns: ['Hi', 'How are you', 'Is anyone there?', 'Hello', 'Good day', 'Whats up'],\n",
            "responses: ['Hello!', 'Good to see you again!', 'Hi there, how can I help?'],\n",
            "context_set: ,\n",
            "\b\b\n",
            "},\n",
            "{tag: goodbye,\n",
            "patterns: ['cya', 'See you later', 'Goodbye', 'I am Leaving', 'Have a Good day'],\n",
            "responses: ['Sad to see you go :(', 'Talk to you later', 'Goodbye!'],\n",
            "context_set: ,\n",
            "\b\b\n",
            "},\n",
            "{tag: age,\n",
            "patterns: ['how old', 'how old is tim', 'what is your age', 'how old are you', 'age?'],\n",
            "responses: ['I am 18 years old!', '18 years young!'],\n",
            "context_set: ,\n",
            "\b\b\n",
            "},\n",
            "{tag: name,\n",
            "patterns: ['what is your name', 'what should I call you', 'whats your name?'],\n",
            "responses: ['You can call me Tim.', \"I'm Tim!\", \"I'm Tim aka Tech With Tim.\"],\n",
            "context_set: ,\n",
            "\b\b\n",
            "},\n",
            "{tag: shop,\n",
            "patterns: ['Id like to buy something', 'whats on the menu', 'what do you reccommend?', 'could i get something to eat'],\n",
            "responses: ['We sell chocolate chip cookies for $2!', 'Cookies are on the menu!'],\n",
            "context_set: ,\n",
            "\b\b\n",
            "},\n",
            "{tag: hours,\n",
            "patterns: ['when are you guys open', 'what are your hours', 'hours of operation'],\n",
            "responses: ['We are open 7am-4pm Monday-Friday!'],\n",
            "context_set: ,\n",
            "\b\b\n",
            "},\n",
            "\b\b]\n"
          ]
        }
      ],
      "source": [
        "print(\"[\", end = \"\")\n",
        "for intent in intents:\n",
        "  print(\"{\", end = \"\")\n",
        "  for key, value in intent.items():\n",
        "    print(\"{}: {},\".format(key, value))\n",
        "  print(\"\\b\\b\\n},\")\n",
        "print(\"\\b\\b]\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KjcTCjopXGah",
        "outputId": "50f1b812-1da2-46e1-b745-c3a8f10414e4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.17.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=3.10.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.11.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (71.0.4)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.64.1)\n",
            "Requirement already satisfied: tensorboard<2.18,>=2.17 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.17.0)\n",
            "Requirement already satisfied: keras>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.26.4)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.44.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (13.7.1)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (0.12.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2024.7.4)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.6)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow) (2.1.5)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow) (2.16.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->tensorflow) (0.1.2)\n",
            "Requirement already satisfied: tflearn in /usr/local/lib/python3.10/dist-packages (0.5.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from tflearn) (1.26.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from tflearn) (1.16.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from tflearn) (9.4.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install tensorflow\n",
        "!pip install tflearn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall tflearn tensorflow -y\n",
        "!pip install tflearn tensorflow"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 949
        },
        "id": "tgj_hujSXyu0",
        "outputId": "814389f7-ed8d-4b2b-9197-fe0ccaec05e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Skipping tflearn as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping tensorflow as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0mCollecting tflearn\n",
            "  Using cached tflearn-0.5.0-py3-none-any.whl\n",
            "Collecting tensorflow\n",
            "  Using cached tensorflow-2.17.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from tflearn) (1.26.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from tflearn) (1.16.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from tflearn) (9.4.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=3.10.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.11.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (71.0.4)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.64.1)\n",
            "Requirement already satisfied: tensorboard<2.18,>=2.17 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.17.0)\n",
            "Requirement already satisfied: keras>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.44.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (13.7.1)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (0.12.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2024.7.4)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.6)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow) (2.1.5)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow) (2.16.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->tensorflow) (0.1.2)\n",
            "Using cached tensorflow-2.17.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (601.3 MB)\n",
            "Installing collected packages: tflearn, tensorflow\n",
            "Successfully installed tensorflow-2.17.0 tflearn-0.5.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "tensorflow"
                ]
              },
              "id": "8f01e33d7d8b44129f56faa334f92dc8"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 618
        },
        "id": "WLaM5283XGdI",
        "outputId": "e4602834-206a-451e-8663-0d2cb15a5bb3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.10/dist-packages/tensorflow/python/compat/v2_compat.py:98: disable_resource_variables (from tensorflow.python.ops.resource_variables_toggle) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "cannot import name 'is_sequence' from 'tensorflow.python.util.nest' (/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/nest.py)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-977bcfe5eaf3>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtflearn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tflearn/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m# Predefined ops\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnormalization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mactivations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tflearn/layers/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mnormalization\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbatch_normalization\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_response_normalization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mestimator\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mregression\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mrecurrent\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlstm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgru\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msimple_rnn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbidirectional_rnn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mBasicRNNCell\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBasicLSTMCell\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGRUCell\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0membedding_ops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0membedding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tflearn/layers/recurrent.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcore_rnn_cell\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnest\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mis_sequence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'is_sequence' from 'tensorflow.python.util.nest' (/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/nest.py)",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "import tflearn\n",
        "import random\n",
        "import pickle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LNbmUl5qXGiE",
        "outputId": "507aafe6-2550-4534-95f9-9bc0cb5e911b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading collection 'all'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package abc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/abc.zip.\n",
            "[nltk_data]    | Downloading package alpino to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/alpino.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping\n",
            "[nltk_data]    |       taggers/averaged_perceptron_tagger_eng.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping\n",
            "[nltk_data]    |       taggers/averaged_perceptron_tagger_ru.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_rus to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping\n",
            "[nltk_data]    |       taggers/averaged_perceptron_tagger_rus.zip.\n",
            "[nltk_data]    | Downloading package basque_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/basque_grammars.zip.\n",
            "[nltk_data]    | Downloading package bcp47 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package biocreative_ppi to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/biocreative_ppi.zip.\n",
            "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/bllip_wsj_no_aux.zip.\n",
            "[nltk_data]    | Downloading package book_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/book_grammars.zip.\n",
            "[nltk_data]    | Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown.zip.\n",
            "[nltk_data]    | Downloading package brown_tei to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown_tei.zip.\n",
            "[nltk_data]    | Downloading package cess_cat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cess_cat.zip.\n",
            "[nltk_data]    | Downloading package cess_esp to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cess_esp.zip.\n",
            "[nltk_data]    | Downloading package chat80 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/chat80.zip.\n",
            "[nltk_data]    | Downloading package city_database to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/city_database.zip.\n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
            "[nltk_data]    | Downloading package comparative_sentences to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/comparative_sentences.zip.\n",
            "[nltk_data]    | Downloading package comtrans to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2000.zip.\n",
            "[nltk_data]    | Downloading package conll2002 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2002.zip.\n",
            "[nltk_data]    | Downloading package conll2007 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package crubadan to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/crubadan.zip.\n",
            "[nltk_data]    | Downloading package dependency_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dependency_treebank.zip.\n",
            "[nltk_data]    | Downloading package dolch to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dolch.zip.\n",
            "[nltk_data]    | Downloading package europarl_raw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/europarl_raw.zip.\n",
            "[nltk_data]    | Downloading package extended_omw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package floresta to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/floresta.zip.\n",
            "[nltk_data]    | Downloading package framenet_v15 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v15.zip.\n",
            "[nltk_data]    | Downloading package framenet_v17 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v17.zip.\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data]    | Downloading package ieer to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ieer.zip.\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
            "[nltk_data]    | Downloading package indian to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/indian.zip.\n",
            "[nltk_data]    | Downloading package jeita to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package kimmo to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/kimmo.zip.\n",
            "[nltk_data]    | Downloading package knbc to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package large_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/large_grammars.zip.\n",
            "[nltk_data]    | Downloading package lin_thesaurus to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/lin_thesaurus.zip.\n",
            "[nltk_data]    | Downloading package mac_morpho to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mac_morpho.zip.\n",
            "[nltk_data]    | Downloading package machado to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package masc_tagged to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker_tab to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker_tab.zip.\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/maxent_treebank_pos_tagger.zip.\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger_tab to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping\n",
            "[nltk_data]    |       taggers/maxent_treebank_pos_tagger_tab.zip.\n",
            "[nltk_data]    | Downloading package moses_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/moses_sample.zip.\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data]    | Downloading package mte_teip5 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mte_teip5.zip.\n",
            "[nltk_data]    | Downloading package mwa_ppdb to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping misc/mwa_ppdb.zip.\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/names.zip.\n",
            "[nltk_data]    | Downloading package nombank.1.0 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nonbreaking_prefixes.zip.\n",
            "[nltk_data]    | Downloading package nps_chat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nps_chat.zip.\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package opinion_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/opinion_lexicon.zip.\n",
            "[nltk_data]    | Downloading package panlex_swadesh to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package paradigms to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/paradigms.zip.\n",
            "[nltk_data]    | Downloading package pe08 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pe08.zip.\n",
            "[nltk_data]    | Downloading package perluniprops to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping misc/perluniprops.zip.\n",
            "[nltk_data]    | Downloading package pil to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pil.zip.\n",
            "[nltk_data]    | Downloading package pl196x to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pl196x.zip.\n",
            "[nltk_data]    | Downloading package porter_test to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping stemmers/porter_test.zip.\n",
            "[nltk_data]    | Downloading package ppattach to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ppattach.zip.\n",
            "[nltk_data]    | Downloading package problem_reports to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/problem_reports.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_1 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/product_reviews_1.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_2 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/product_reviews_2.zip.\n",
            "[nltk_data]    | Downloading package propbank to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package pros_cons to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pros_cons.zip.\n",
            "[nltk_data]    | Downloading package ptb to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ptb.zip.\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data]    | Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data]    | Downloading package qc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/qc.zip.\n",
            "[nltk_data]    | Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package rslp to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping stemmers/rslp.zip.\n",
            "[nltk_data]    | Downloading package rte to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/rte.zip.\n",
            "[nltk_data]    | Downloading package sample_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/sample_grammars.zip.\n",
            "[nltk_data]    | Downloading package semcor to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package senseval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/senseval.zip.\n",
            "[nltk_data]    | Downloading package sentence_polarity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sentence_polarity.zip.\n",
            "[nltk_data]    | Downloading package sentiwordnet to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sentiwordnet.zip.\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n",
            "[nltk_data]    | Downloading package sinica_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sinica_treebank.zip.\n",
            "[nltk_data]    | Downloading package smultron to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/smultron.zip.\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package spanish_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/spanish_grammars.zip.\n",
            "[nltk_data]    | Downloading package state_union to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/state_union.zip.\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data]    | Downloading package subjectivity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/subjectivity.zip.\n",
            "[nltk_data]    | Downloading package swadesh to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/swadesh.zip.\n",
            "[nltk_data]    | Downloading package switchboard to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/switchboard.zip.\n",
            "[nltk_data]    | Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping help/tagsets.zip.\n",
            "[nltk_data]    | Downloading package tagsets_json to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping help/tagsets_json.zip.\n",
            "[nltk_data]    | Downloading package timit to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/timit.zip.\n",
            "[nltk_data]    | Downloading package toolbox to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/toolbox.zip.\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n",
            "[nltk_data]    | Downloading package udhr to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr.zip.\n",
            "[nltk_data]    | Downloading package udhr2 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr2.zip.\n",
            "[nltk_data]    | Downloading package unicode_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/unicode_samples.zip.\n",
            "[nltk_data]    | Downloading package universal_tagset to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/universal_tagset.zip.\n",
            "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package vader_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package verbnet to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/verbnet.zip.\n",
            "[nltk_data]    | Downloading package verbnet3 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/verbnet3.zip.\n",
            "[nltk_data]    | Downloading package webtext to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/webtext.zip.\n",
            "[nltk_data]    | Downloading package wmt15_eval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/wmt15_eval.zip.\n",
            "[nltk_data]    | Downloading package word2vec_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/word2vec_sample.zip.\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet2021 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet2022 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet2022.zip.\n",
            "[nltk_data]    | Downloading package wordnet31 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/words.zip.\n",
            "[nltk_data]    | Downloading package ycoe to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ycoe.zip.\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection all\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('all')\n",
        "\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "stemmer = SnowballStemmer('english')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HRftd7OiXt4L"
      },
      "outputs": [],
      "source": [
        "retrain_model = True\n",
        "\n",
        "if retrain_model:\n",
        "    all_words = []\n",
        "    all_tags = [] #This will be a list of all the 'tag's associated with the intents\n",
        "    intent_patterns = [] #This will be a list containing all of the 'patterns' for each intent where each individual pattern is grouped together\n",
        "    intent_tags = []\n",
        "\n",
        "\n",
        "    #Here we fill in all of the lists above. Note that we tokenize the UT words in each pattern which means we split each pattern into individual words\n",
        "    for intent in intents:\n",
        "        for pattern in intent['patterns']:\n",
        "            words = nltk.word_tokenize(pattern)\n",
        "\n",
        "            all_words.extend(words)\n",
        "            intent_patterns.append(words)\n",
        "            intent_tags.append(intent['tag'])\n",
        "\n",
        "        all_tags.append(intent['tag'])\n",
        "\n",
        "    #Here we stem the words in all_words. This means that we reduce every word down to its root form or stem. This will prevent our chatbot from confusin\n",
        "    #Very similar words with eachother. For example, the chatbot might normally confuse the words 'running' and 'run' because they appear different even\n",
        "    #Though they effectively mean the same thing. Stemming will reduce both of these words down to their root form which would be 'run' so the chatbot will\n",
        "    #No longer be confused\n",
        "    all_words = [stemmer.stem(word.lower()) for word in all_words]\n",
        "    all_words = sorted(list(set(all_words)))\n",
        "\n",
        "    all_tags = sorted(all_tags)\n",
        "\n",
        "    x_train = []\n",
        "    y_train = []\n",
        "\n",
        "    y_empty = [0 for i in range(len(all_tags))]\n",
        "\n",
        "    #Here we are creating our training set input and output values for our deep learning algorithm\n",
        "    #We will do this by iterating through our intents and turning each one into a bag of words, or a vector that indicates which words are in each pattern.\n",
        "    #These bags of words will be the x values and the y values will be the intent that each bag of words is associated with.\n",
        "    #The machine learning will train on this data and will be able to determine which bag of words its corresponding intent.\n",
        "    for index, intent in enumerate(intent_patterns):\n",
        "        bag_of_words = []\n",
        "\n",
        "        intent_words = [stemmer.stem(word.lower()) for word in intent]\n",
        "\n",
        "        for word in all_words:\n",
        "            if word in intent_words:\n",
        "                bag_of_words.append(1)\n",
        "            else:\n",
        "                bag_of_words.append(0)\n",
        "\n",
        "        one_hot_encode_y = y_empty[:]\n",
        "        one_hot_encode_y[all_tags.index(intent_tags[index])] = 1\n",
        "\n",
        "        x_train.append(bag_of_words)\n",
        "        y_train.append(one_hot_encode_y)\n",
        "\n",
        "    #Here is the data we will be using to train our neural network later\n",
        "    x_train = np.array(x_train)\n",
        "    y_train = np.array(y_train)\n",
        "\n",
        "    #Here we just save our training data so we don't need to process it again if we just want to run our chatbot\n",
        "    with open('training_data.pickle', 'wb') as f:\n",
        "        pickle.dump((all_words, all_tags, x_train, y_train), f)\n",
        "else:\n",
        "    with open('training_data.pickle', 'rb') as f:\n",
        "        all_words, all_tags, x_train, y_train = pickle.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7kvU_ezhXt6h",
        "outputId": "cb2add32-2a85-4167-9118-69adc070d40a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.10/dist-packages/tflearn/initializations.py:164: calling TruncatedNormal.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "neural_net = tflearn.input_data(shape=[None, len(x_train[0])])\n",
        "neural_net = tflearn.fully_connected(neural_net, 16, activation=\"ReLU\")\n",
        "neural_net = tflearn.fully_connected(neural_net, 16, activation=\"ReLU\")\n",
        "neural_net = tflearn.fully_connected(neural_net, len(y_train[0]), activation=\"softmax\")\n",
        "neural_net = tflearn.regression(neural_net)\n",
        "\n",
        "model = tflearn.DNN(neural_net)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "AjJ31mZqXt9-",
        "outputId": "6904731f-b2c3-43c8-b14d-db5a490aefb4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "| Adam | epoch: 334 | loss: 0.10817 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 335  | total loss: \u001b[1m\u001b[32m0.10646\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 335 | loss: 0.10646 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 336  | total loss: \u001b[1m\u001b[32m0.10480\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 336 | loss: 0.10480 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 337  | total loss: \u001b[1m\u001b[32m0.10317\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 337 | loss: 0.10317 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 338  | total loss: \u001b[1m\u001b[32m0.10157\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 338 | loss: 0.10157 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 339  | total loss: \u001b[1m\u001b[32m0.10000\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 339 | loss: 0.10000 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 340  | total loss: \u001b[1m\u001b[32m0.09847\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 340 | loss: 0.09847 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 341  | total loss: \u001b[1m\u001b[32m0.09697\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 341 | loss: 0.09697 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 342  | total loss: \u001b[1m\u001b[32m0.09550\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 342 | loss: 0.09550 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 343  | total loss: \u001b[1m\u001b[32m0.09406\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 343 | loss: 0.09406 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 344  | total loss: \u001b[1m\u001b[32m0.09265\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 344 | loss: 0.09265 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 345  | total loss: \u001b[1m\u001b[32m0.09127\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 345 | loss: 0.09127 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 346  | total loss: \u001b[1m\u001b[32m0.08991\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 346 | loss: 0.08991 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 347  | total loss: \u001b[1m\u001b[32m0.08859\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 347 | loss: 0.08859 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 348  | total loss: \u001b[1m\u001b[32m0.08729\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 348 | loss: 0.08729 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 349  | total loss: \u001b[1m\u001b[32m0.08601\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 349 | loss: 0.08601 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 350  | total loss: \u001b[1m\u001b[32m0.08476\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 350 | loss: 0.08476 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 351  | total loss: \u001b[1m\u001b[32m0.08354\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 351 | loss: 0.08354 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 352  | total loss: \u001b[1m\u001b[32m0.08234\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 352 | loss: 0.08234 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 353  | total loss: \u001b[1m\u001b[32m0.08116\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 353 | loss: 0.08116 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 354  | total loss: \u001b[1m\u001b[32m0.08001\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 354 | loss: 0.08001 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 355  | total loss: \u001b[1m\u001b[32m0.07887\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 355 | loss: 0.07887 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 356  | total loss: \u001b[1m\u001b[32m0.07777\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 356 | loss: 0.07777 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 357  | total loss: \u001b[1m\u001b[32m0.07668\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 357 | loss: 0.07668 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 358  | total loss: \u001b[1m\u001b[32m0.07561\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 358 | loss: 0.07561 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 359  | total loss: \u001b[1m\u001b[32m0.07456\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 359 | loss: 0.07456 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 360  | total loss: \u001b[1m\u001b[32m0.07354\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 360 | loss: 0.07354 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 361  | total loss: \u001b[1m\u001b[32m0.07253\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 361 | loss: 0.07253 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 362  | total loss: \u001b[1m\u001b[32m0.66427\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 362 | loss: 0.66427 - acc: 0.9192 -- iter: 26/26\n",
            "--\n",
            "Training Step: 363  | total loss: \u001b[1m\u001b[32m0.60419\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 363 | loss: 0.60419 - acc: 0.9273 -- iter: 26/26\n",
            "--\n",
            "Training Step: 364  | total loss: \u001b[1m\u001b[32m1.24648\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 364 | loss: 1.24648 - acc: 0.8538 -- iter: 26/26\n",
            "--\n",
            "Training Step: 365  | total loss: \u001b[1m\u001b[32m1.12847\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 365 | loss: 1.12847 - acc: 0.8684 -- iter: 26/26\n",
            "--\n",
            "Training Step: 366  | total loss: \u001b[1m\u001b[32m1.02246\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 366 | loss: 1.02246 - acc: 0.8816 -- iter: 26/26\n",
            "--\n",
            "Training Step: 367  | total loss: \u001b[1m\u001b[32m0.92725\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 367 | loss: 0.92725 - acc: 0.8934 -- iter: 26/26\n",
            "--\n",
            "Training Step: 368  | total loss: \u001b[1m\u001b[32m1.47113\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 368 | loss: 1.47113 - acc: 0.8156 -- iter: 26/26\n",
            "--\n",
            "Training Step: 369  | total loss: \u001b[1m\u001b[32m1.33152\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 369 | loss: 1.33152 - acc: 0.8341 -- iter: 26/26\n",
            "--\n",
            "Training Step: 370  | total loss: \u001b[1m\u001b[32m1.20614\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 370 | loss: 1.20614 - acc: 0.8507 -- iter: 26/26\n",
            "--\n",
            "Training Step: 371  | total loss: \u001b[1m\u001b[32m1.09356\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 371 | loss: 1.09356 - acc: 0.8656 -- iter: 26/26\n",
            "--\n",
            "Training Step: 372  | total loss: \u001b[1m\u001b[32m1.52530\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 372 | loss: 1.52530 - acc: 0.7906 -- iter: 26/26\n",
            "--\n",
            "Training Step: 373  | total loss: \u001b[1m\u001b[32m1.38134\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 373 | loss: 1.38134 - acc: 0.8115 -- iter: 26/26\n",
            "--\n",
            "Training Step: 374  | total loss: \u001b[1m\u001b[32m1.25208\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 374 | loss: 1.25208 - acc: 0.8304 -- iter: 26/26\n",
            "--\n",
            "Training Step: 375  | total loss: \u001b[1m\u001b[32m1.13599\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 375 | loss: 1.13599 - acc: 0.8473 -- iter: 26/26\n",
            "--\n",
            "Training Step: 376  | total loss: \u001b[1m\u001b[32m1.03174\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 376 | loss: 1.03174 - acc: 0.8626 -- iter: 26/26\n",
            "--\n",
            "Training Step: 377  | total loss: \u001b[1m\u001b[32m0.93811\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 377 | loss: 0.93811 - acc: 0.8763 -- iter: 26/26\n",
            "--\n",
            "Training Step: 378  | total loss: \u001b[1m\u001b[32m1.50277\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 378 | loss: 1.50277 - acc: 0.8118 -- iter: 26/26\n",
            "--\n",
            "Training Step: 379  | total loss: \u001b[1m\u001b[32m1.36250\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 379 | loss: 1.36250 - acc: 0.8306 -- iter: 26/26\n",
            "--\n",
            "Training Step: 380  | total loss: \u001b[1m\u001b[32m1.23653\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 380 | loss: 1.23653 - acc: 0.8475 -- iter: 26/26\n",
            "--\n",
            "Training Step: 381  | total loss: \u001b[1m\u001b[32m1.12338\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 381 | loss: 1.12338 - acc: 0.8628 -- iter: 26/26\n",
            "--\n",
            "Training Step: 382  | total loss: \u001b[1m\u001b[32m1.56252\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 382 | loss: 1.56252 - acc: 0.7957 -- iter: 26/26\n",
            "--\n",
            "Training Step: 383  | total loss: \u001b[1m\u001b[32m1.41729\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 383 | loss: 1.41729 - acc: 0.8162 -- iter: 26/26\n",
            "--\n",
            "Training Step: 384  | total loss: \u001b[1m\u001b[32m1.85824\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 384 | loss: 1.85824 - acc: 0.7461 -- iter: 26/26\n",
            "--\n",
            "Training Step: 385  | total loss: \u001b[1m\u001b[32m1.68411\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 385 | loss: 1.68411 - acc: 0.7715 -- iter: 26/26\n",
            "--\n",
            "Training Step: 386  | total loss: \u001b[1m\u001b[32m1.52774\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 386 | loss: 1.52774 - acc: 0.7943 -- iter: 26/26\n",
            "--\n",
            "Training Step: 387  | total loss: \u001b[1m\u001b[32m1.38730\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 387 | loss: 1.38730 - acc: 0.8149 -- iter: 26/26\n",
            "--\n",
            "Training Step: 388  | total loss: \u001b[1m\u001b[32m1.80997\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 388 | loss: 1.80997 - acc: 0.7411 -- iter: 26/26\n",
            "--\n",
            "Training Step: 389  | total loss: \u001b[1m\u001b[32m1.64200\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 389 | loss: 1.64200 - acc: 0.7670 -- iter: 26/26\n",
            "--\n",
            "Training Step: 390  | total loss: \u001b[1m\u001b[32m1.93466\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 390 | loss: 1.93466 - acc: 0.7211 -- iter: 26/26\n",
            "--\n",
            "Training Step: 391  | total loss: \u001b[1m\u001b[32m1.75510\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 391 | loss: 1.75510 - acc: 0.7490 -- iter: 26/26\n",
            "--\n",
            "Training Step: 392  | total loss: \u001b[1m\u001b[32m2.14519\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 392 | loss: 2.14519 - acc: 0.6779 -- iter: 26/26\n",
            "--\n",
            "Training Step: 393  | total loss: \u001b[1m\u001b[32m1.94559\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 393 | loss: 1.94559 - acc: 0.7101 -- iter: 26/26\n",
            "--\n",
            "Training Step: 394  | total loss: \u001b[1m\u001b[32m1.76648\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 394 | loss: 1.76648 - acc: 0.7391 -- iter: 26/26\n",
            "--\n",
            "Training Step: 395  | total loss: \u001b[1m\u001b[32m1.60574\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 395 | loss: 1.60574 - acc: 0.7652 -- iter: 26/26\n",
            "--\n",
            "Training Step: 396  | total loss: \u001b[1m\u001b[32m1.93703\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 396 | loss: 1.93703 - acc: 0.7041 -- iter: 26/26\n",
            "--\n",
            "Training Step: 397  | total loss: \u001b[1m\u001b[32m1.76016\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 397 | loss: 1.76016 - acc: 0.7337 -- iter: 26/26\n",
            "--\n",
            "Training Step: 398  | total loss: \u001b[1m\u001b[32m2.11904\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 398 | loss: 2.11904 - acc: 0.6718 -- iter: 26/26\n",
            "--\n",
            "Training Step: 399  | total loss: \u001b[1m\u001b[32m1.92501\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 399 | loss: 1.92501 - acc: 0.7046 -- iter: 26/26\n",
            "--\n",
            "Training Step: 400  | total loss: \u001b[1m\u001b[32m1.75089\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 400 | loss: 1.75089 - acc: 0.7342 -- iter: 26/26\n",
            "--\n",
            "Training Step: 401  | total loss: \u001b[1m\u001b[32m1.59460\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 401 | loss: 1.59460 - acc: 0.7608 -- iter: 26/26\n",
            "--\n",
            "Training Step: 402  | total loss: \u001b[1m\u001b[32m1.45426\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 402 | loss: 1.45426 - acc: 0.7847 -- iter: 26/26\n",
            "--\n",
            "Training Step: 403  | total loss: \u001b[1m\u001b[32m1.32822\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 403 | loss: 1.32822 - acc: 0.8062 -- iter: 26/26\n",
            "--\n",
            "Training Step: 404  | total loss: \u001b[1m\u001b[32m1.21498\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 404 | loss: 1.21498 - acc: 0.8256 -- iter: 26/26\n",
            "--\n",
            "Training Step: 405  | total loss: \u001b[1m\u001b[32m1.11318\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 405 | loss: 1.11318 - acc: 0.8430 -- iter: 26/26\n",
            "--\n",
            "Training Step: 406  | total loss: \u001b[1m\u001b[32m1.02165\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 406 | loss: 1.02165 - acc: 0.8587 -- iter: 26/26\n",
            "--\n",
            "Training Step: 407  | total loss: \u001b[1m\u001b[32m0.93930\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 407 | loss: 0.93930 - acc: 0.8729 -- iter: 26/26\n",
            "--\n",
            "Training Step: 408  | total loss: \u001b[1m\u001b[32m1.15088\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 408 | loss: 1.15088 - acc: 0.8125 -- iter: 26/26\n",
            "--\n",
            "Training Step: 409  | total loss: \u001b[1m\u001b[32m1.05571\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 409 | loss: 1.05571 - acc: 0.8312 -- iter: 26/26\n",
            "--\n",
            "Training Step: 410  | total loss: \u001b[1m\u001b[32m1.34708\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 410 | loss: 1.34708 - acc: 0.7789 -- iter: 26/26\n",
            "--\n",
            "Training Step: 411  | total loss: \u001b[1m\u001b[32m1.23260\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 411 | loss: 1.23260 - acc: 0.8010 -- iter: 26/26\n",
            "--\n",
            "Training Step: 412  | total loss: \u001b[1m\u001b[32m1.12975\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 412 | loss: 1.12975 - acc: 0.8209 -- iter: 26/26\n",
            "--\n",
            "Training Step: 413  | total loss: \u001b[1m\u001b[32m1.03732\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 413 | loss: 1.03732 - acc: 0.8388 -- iter: 26/26\n",
            "--\n",
            "Training Step: 414  | total loss: \u001b[1m\u001b[32m1.38895\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 414 | loss: 1.38895 - acc: 0.7665 -- iter: 26/26\n",
            "--\n",
            "Training Step: 415  | total loss: \u001b[1m\u001b[32m1.27099\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 415 | loss: 1.27099 - acc: 0.7898 -- iter: 26/26\n",
            "--\n",
            "Training Step: 416  | total loss: \u001b[1m\u001b[32m1.16506\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 416 | loss: 1.16506 - acc: 0.8108 -- iter: 26/26\n",
            "--\n",
            "Training Step: 417  | total loss: \u001b[1m\u001b[32m1.06991\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 417 | loss: 1.06991 - acc: 0.8298 -- iter: 26/26\n",
            "--\n",
            "Training Step: 418  | total loss: \u001b[1m\u001b[32m0.98440\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 418 | loss: 0.98440 - acc: 0.8468 -- iter: 26/26\n",
            "--\n",
            "Training Step: 419  | total loss: \u001b[1m\u001b[32m0.90754\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 419 | loss: 0.90754 - acc: 0.8621 -- iter: 26/26\n",
            "--\n",
            "Training Step: 420  | total loss: \u001b[1m\u001b[32m1.23859\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 420 | loss: 1.23859 - acc: 0.7951 -- iter: 26/26\n",
            "--\n",
            "Training Step: 421  | total loss: \u001b[1m\u001b[32m1.13655\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 421 | loss: 1.13655 - acc: 0.8156 -- iter: 26/26\n",
            "--\n",
            "Training Step: 422  | total loss: \u001b[1m\u001b[32m1.49015\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 422 | loss: 1.49015 - acc: 0.7417 -- iter: 26/26\n",
            "--\n",
            "Training Step: 423  | total loss: \u001b[1m\u001b[32m1.36343\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 423 | loss: 1.36343 - acc: 0.7676 -- iter: 26/26\n",
            "--\n",
            "Training Step: 424  | total loss: \u001b[1m\u001b[32m1.63352\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 424 | loss: 1.63352 - acc: 0.7062 -- iter: 26/26\n",
            "--\n",
            "Training Step: 425  | total loss: \u001b[1m\u001b[32m1.49316\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 425 | loss: 1.49316 - acc: 0.7356 -- iter: 26/26\n",
            "--\n",
            "Training Step: 426  | total loss: \u001b[1m\u001b[32m1.80359\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 426 | loss: 1.80359 - acc: 0.6774 -- iter: 26/26\n",
            "--\n",
            "Training Step: 427  | total loss: \u001b[1m\u001b[32m1.64710\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 427 | loss: 1.64710 - acc: 0.7097 -- iter: 26/26\n",
            "--\n",
            "Training Step: 428  | total loss: \u001b[1m\u001b[32m1.92802\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 428 | loss: 1.92802 - acc: 0.6425 -- iter: 26/26\n",
            "--\n",
            "Training Step: 429  | total loss: \u001b[1m\u001b[32m1.76012\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 429 | loss: 1.76012 - acc: 0.6783 -- iter: 26/26\n",
            "--\n",
            "Training Step: 430  | total loss: \u001b[1m\u001b[32m1.60951\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 430 | loss: 1.60951 - acc: 0.7105 -- iter: 26/26\n",
            "--\n",
            "Training Step: 431  | total loss: \u001b[1m\u001b[32m1.47439\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 431 | loss: 1.47439 - acc: 0.7394 -- iter: 26/26\n",
            "--\n",
            "Training Step: 432  | total loss: \u001b[1m\u001b[32m1.35314\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 432 | loss: 1.35314 - acc: 0.7655 -- iter: 26/26\n",
            "--\n",
            "Training Step: 433  | total loss: \u001b[1m\u001b[32m1.24430\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 433 | loss: 1.24430 - acc: 0.7889 -- iter: 26/26\n",
            "--\n",
            "Training Step: 434  | total loss: \u001b[1m\u001b[32m1.14656\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 434 | loss: 1.14656 - acc: 0.8100 -- iter: 26/26\n",
            "--\n",
            "Training Step: 435  | total loss: \u001b[1m\u001b[32m1.05875\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 435 | loss: 1.05875 - acc: 0.8290 -- iter: 26/26\n",
            "--\n",
            "Training Step: 436  | total loss: \u001b[1m\u001b[32m1.31491\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 436 | loss: 1.31491 - acc: 0.7692 -- iter: 26/26\n",
            "--\n",
            "Training Step: 437  | total loss: \u001b[1m\u001b[32m1.21062\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 437 | loss: 1.21062 - acc: 0.7923 -- iter: 26/26\n",
            "--\n",
            "Training Step: 438  | total loss: \u001b[1m\u001b[32m1.11695\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 438 | loss: 1.11695 - acc: 0.8131 -- iter: 26/26\n",
            "--\n",
            "Training Step: 439  | total loss: \u001b[1m\u001b[32m1.03278\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 439 | loss: 1.03278 - acc: 0.8317 -- iter: 26/26\n",
            "--\n",
            "Training Step: 440  | total loss: \u001b[1m\u001b[32m0.95711\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 440 | loss: 0.95711 - acc: 0.8486 -- iter: 26/26\n",
            "--\n",
            "Training Step: 441  | total loss: \u001b[1m\u001b[32m0.88904\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 441 | loss: 0.88904 - acc: 0.8637 -- iter: 26/26\n",
            "--\n",
            "Training Step: 442  | total loss: \u001b[1m\u001b[32m0.82776\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 442 | loss: 0.82776 - acc: 0.8773 -- iter: 26/26\n",
            "--\n",
            "Training Step: 443  | total loss: \u001b[1m\u001b[32m0.77255\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 443 | loss: 0.77255 - acc: 0.8896 -- iter: 26/26\n",
            "--\n",
            "Training Step: 444  | total loss: \u001b[1m\u001b[32m0.72278\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 444 | loss: 0.72278 - acc: 0.9006 -- iter: 26/26\n",
            "--\n",
            "Training Step: 445  | total loss: \u001b[1m\u001b[32m0.67787\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 445 | loss: 0.67787 - acc: 0.9106 -- iter: 26/26\n",
            "--\n",
            "Training Step: 446  | total loss: \u001b[1m\u001b[32m1.03658\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 446 | loss: 1.03658 - acc: 0.8388 -- iter: 26/26\n",
            "--\n",
            "Training Step: 447  | total loss: \u001b[1m\u001b[32m0.96020\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 447 | loss: 0.96020 - acc: 0.8549 -- iter: 26/26\n",
            "--\n",
            "Training Step: 448  | total loss: \u001b[1m\u001b[32m1.25332\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 448 | loss: 1.25332 - acc: 0.7848 -- iter: 26/26\n",
            "--\n",
            "Training Step: 449  | total loss: \u001b[1m\u001b[32m1.15545\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 449 | loss: 1.15545 - acc: 0.8063 -- iter: 26/26\n",
            "--\n",
            "Training Step: 450  | total loss: \u001b[1m\u001b[32m1.37910\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 450 | loss: 1.37910 - acc: 0.7449 -- iter: 26/26\n",
            "--\n",
            "Training Step: 451  | total loss: \u001b[1m\u001b[32m1.26903\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 451 | loss: 1.26903 - acc: 0.7704 -- iter: 26/26\n",
            "--\n",
            "Training Step: 452  | total loss: \u001b[1m\u001b[32m1.51482\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 452 | loss: 1.51482 - acc: 0.6972 -- iter: 26/26\n",
            "--\n",
            "Training Step: 453  | total loss: \u001b[1m\u001b[32m1.39171\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 453 | loss: 1.39171 - acc: 0.7275 -- iter: 26/26\n",
            "--\n",
            "Training Step: 454  | total loss: \u001b[1m\u001b[32m1.68575\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 454 | loss: 1.68575 - acc: 0.6663 -- iter: 26/26\n",
            "--\n",
            "Training Step: 455  | total loss: \u001b[1m\u001b[32m1.54627\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 455 | loss: 1.54627 - acc: 0.6997 -- iter: 26/26\n",
            "--\n",
            "Training Step: 456  | total loss: \u001b[1m\u001b[32m1.42111\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 456 | loss: 1.42111 - acc: 0.7297 -- iter: 26/26\n",
            "--\n",
            "Training Step: 457  | total loss: \u001b[1m\u001b[32m1.30876\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 457 | loss: 1.30876 - acc: 0.7567 -- iter: 26/26\n",
            "--\n",
            "Training Step: 458  | total loss: \u001b[1m\u001b[32m1.48135\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 458 | loss: 1.48135 - acc: 0.7041 -- iter: 26/26\n",
            "--\n",
            "Training Step: 459  | total loss: \u001b[1m\u001b[32m1.36357\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 459 | loss: 1.36357 - acc: 0.7337 -- iter: 26/26\n",
            "--\n",
            "Training Step: 460  | total loss: \u001b[1m\u001b[32m1.58102\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 460 | loss: 1.58102 - acc: 0.6642 -- iter: 26/26\n",
            "--\n",
            "Training Step: 461  | total loss: \u001b[1m\u001b[32m1.45397\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 461 | loss: 1.45397 - acc: 0.6978 -- iter: 26/26\n",
            "--\n",
            "Training Step: 462  | total loss: \u001b[1m\u001b[32m1.67895\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 462 | loss: 1.67895 - acc: 0.6395 -- iter: 26/26\n",
            "--\n",
            "Training Step: 463  | total loss: \u001b[1m\u001b[32m1.54295\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 463 | loss: 1.54295 - acc: 0.6756 -- iter: 26/26\n",
            "--\n",
            "Training Step: 464  | total loss: \u001b[1m\u001b[32m1.42096\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 464 | loss: 1.42096 - acc: 0.7080 -- iter: 26/26\n",
            "--\n",
            "Training Step: 465  | total loss: \u001b[1m\u001b[32m1.31151\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 465 | loss: 1.31151 - acc: 0.7372 -- iter: 26/26\n",
            "--\n",
            "Training Step: 466  | total loss: \u001b[1m\u001b[32m1.21325\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 466 | loss: 1.21325 - acc: 0.7635 -- iter: 26/26\n",
            "--\n",
            "Training Step: 467  | total loss: \u001b[1m\u001b[32m1.12499\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 467 | loss: 1.12499 - acc: 0.7871 -- iter: 26/26\n",
            "--\n",
            "Training Step: 468  | total loss: \u001b[1m\u001b[32m1.04567\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 468 | loss: 1.04567 - acc: 0.8084 -- iter: 26/26\n",
            "--\n",
            "Training Step: 469  | total loss: \u001b[1m\u001b[32m0.97433\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 469 | loss: 0.97433 - acc: 0.8276 -- iter: 26/26\n",
            "--\n",
            "Training Step: 470  | total loss: \u001b[1m\u001b[32m0.91011\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 470 | loss: 0.91011 - acc: 0.8448 -- iter: 26/26\n",
            "--\n",
            "Training Step: 471  | total loss: \u001b[1m\u001b[32m0.85226\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 471 | loss: 0.85226 - acc: 0.8603 -- iter: 26/26\n",
            "--\n",
            "Training Step: 472  | total loss: \u001b[1m\u001b[32m1.11450\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 472 | loss: 1.11450 - acc: 0.7820 -- iter: 26/26\n",
            "--\n",
            "Training Step: 473  | total loss: \u001b[1m\u001b[32m1.03618\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 473 | loss: 1.03618 - acc: 0.8038 -- iter: 26/26\n",
            "--\n",
            "Training Step: 474  | total loss: \u001b[1m\u001b[32m0.96571\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 474 | loss: 0.96571 - acc: 0.8234 -- iter: 26/26\n",
            "--\n",
            "Training Step: 475  | total loss: \u001b[1m\u001b[32m0.90225\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 475 | loss: 0.90225 - acc: 0.8411 -- iter: 26/26\n",
            "--\n",
            "Training Step: 476  | total loss: \u001b[1m\u001b[32m1.16049\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 476 | loss: 1.16049 - acc: 0.7685 -- iter: 26/26\n",
            "--\n",
            "Training Step: 477  | total loss: \u001b[1m\u001b[32m1.07761\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 477 | loss: 1.07761 - acc: 0.7917 -- iter: 26/26\n",
            "--\n",
            "Training Step: 478  | total loss: \u001b[1m\u001b[32m1.28289\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 478 | loss: 1.28289 - acc: 0.7317 -- iter: 26/26\n",
            "--\n",
            "Training Step: 479  | total loss: \u001b[1m\u001b[32m1.18803\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 479 | loss: 1.18803 - acc: 0.7586 -- iter: 26/26\n",
            "--\n",
            "Training Step: 480  | total loss: \u001b[1m\u001b[32m1.10280\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 480 | loss: 1.10280 - acc: 0.7827 -- iter: 26/26\n",
            "--\n",
            "Training Step: 481  | total loss: \u001b[1m\u001b[32m1.02617\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 481 | loss: 1.02617 - acc: 0.8044 -- iter: 26/26\n",
            "--\n",
            "Training Step: 482  | total loss: \u001b[1m\u001b[32m1.31265\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 482 | loss: 1.31265 - acc: 0.7355 -- iter: 26/26\n",
            "--\n",
            "Training Step: 483  | total loss: \u001b[1m\u001b[32m1.21528\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 483 | loss: 1.21528 - acc: 0.7620 -- iter: 26/26\n",
            "--\n",
            "Training Step: 484  | total loss: \u001b[1m\u001b[32m1.39244\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 484 | loss: 1.39244 - acc: 0.7089 -- iter: 26/26\n",
            "--\n",
            "Training Step: 485  | total loss: \u001b[1m\u001b[32m1.28749\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 485 | loss: 1.28749 - acc: 0.7380 -- iter: 26/26\n",
            "--\n",
            "Training Step: 486  | total loss: \u001b[1m\u001b[32m1.46501\u001b[0m\u001b[0m | time: 0.018s\n",
            "| Adam | epoch: 486 | loss: 1.46501 - acc: 0.6872 -- iter: 26/26\n",
            "--\n",
            "Training Step: 487  | total loss: \u001b[1m\u001b[32m1.35331\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 487 | loss: 1.35331 - acc: 0.7185 -- iter: 26/26\n",
            "--\n",
            "Training Step: 488  | total loss: \u001b[1m\u001b[32m1.50923\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 488 | loss: 1.50923 - acc: 0.6659 -- iter: 26/26\n",
            "--\n",
            "Training Step: 489  | total loss: \u001b[1m\u001b[32m1.39369\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 489 | loss: 1.39369 - acc: 0.6993 -- iter: 26/26\n",
            "--\n",
            "Training Step: 490  | total loss: \u001b[1m\u001b[32m1.58001\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 490 | loss: 1.58001 - acc: 0.6409 -- iter: 26/26\n",
            "--\n",
            "Training Step: 491  | total loss: \u001b[1m\u001b[32m1.45804\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 491 | loss: 1.45804 - acc: 0.6768 -- iter: 26/26\n",
            "--\n",
            "Training Step: 492  | total loss: \u001b[1m\u001b[32m1.65647\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 492 | loss: 1.65647 - acc: 0.6168 -- iter: 26/26\n",
            "--\n",
            "Training Step: 493  | total loss: \u001b[1m\u001b[32m1.52759\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 493 | loss: 1.52759 - acc: 0.6552 -- iter: 26/26\n",
            "--\n",
            "Training Step: 494  | total loss: \u001b[1m\u001b[32m1.41195\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 494 | loss: 1.41195 - acc: 0.6896 -- iter: 26/26\n",
            "--\n",
            "Training Step: 495  | total loss: \u001b[1m\u001b[32m1.30815\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 495 | loss: 1.30815 - acc: 0.7207 -- iter: 26/26\n",
            "--\n",
            "Training Step: 496  | total loss: \u001b[1m\u001b[32m1.21493\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 496 | loss: 1.21493 - acc: 0.7486 -- iter: 26/26\n",
            "--\n",
            "Training Step: 497  | total loss: \u001b[1m\u001b[32m1.13116\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 497 | loss: 1.13116 - acc: 0.7737 -- iter: 26/26\n",
            "--\n",
            "Training Step: 498  | total loss: \u001b[1m\u001b[32m0.98806\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 498 | loss: 0.98806 - acc: 0.7964 -- iter: 26/26\n",
            "--\n",
            "Training Step: 499  | total loss: \u001b[1m\u001b[32m0.98806\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 499 | loss: 0.98806 - acc: 0.8167 -- iter: 26/26\n",
            "--\n",
            "Training Step: 500  | total loss: \u001b[1m\u001b[32m1.16819\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 500 | loss: 1.16819 - acc: 0.7754 -- iter: 26/26\n",
            "--\n",
            "Training Step: 501  | total loss: \u001b[1m\u001b[32m1.08924\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 501 | loss: 1.08924 - acc: 0.7754 -- iter: 26/26\n",
            "--\n",
            "Training Step: 502  | total loss: \u001b[1m\u001b[32m1.17949\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 502 | loss: 1.17949 - acc: 0.7132 -- iter: 26/26\n",
            "--\n",
            "Training Step: 503  | total loss: \u001b[1m\u001b[32m1.09973\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 503 | loss: 1.09973 - acc: 0.7419 -- iter: 26/26\n",
            "--\n",
            "Training Step: 504  | total loss: \u001b[1m\u001b[32m1.09973\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 504 | loss: 1.09973 - acc: 0.7677 -- iter: 26/26\n",
            "--\n",
            "Training Step: 505  | total loss: \u001b[1m\u001b[32m1.02800\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 505 | loss: 1.02800 - acc: 0.7910 -- iter: 26/26\n",
            "--\n",
            "Training Step: 506  | total loss: \u001b[1m\u001b[32m1.06108\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 506 | loss: 1.06108 - acc: 0.7465 -- iter: 26/26\n",
            "--\n",
            "Training Step: 507  | total loss: \u001b[1m\u001b[32m1.06108\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 507 | loss: 1.06108 - acc: 0.7718 -- iter: 26/26\n",
            "--\n",
            "Training Step: 508  | total loss: \u001b[1m\u001b[32m1.28025\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 508 | loss: 1.28025 - acc: 0.7062 -- iter: 26/26\n",
            "--\n",
            "Training Step: 509  | total loss: \u001b[1m\u001b[32m1.19074\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 509 | loss: 1.19074 - acc: 0.7356 -- iter: 26/26\n",
            "--\n",
            "Training Step: 510  | total loss: \u001b[1m\u001b[32m1.03797\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 510 | loss: 1.03797 - acc: 0.7620 -- iter: 26/26\n",
            "--\n",
            "Training Step: 511  | total loss: \u001b[1m\u001b[32m1.03797\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 511 | loss: 1.03797 - acc: 0.7858 -- iter: 26/26\n",
            "--\n",
            "Training Step: 512  | total loss: \u001b[1m\u001b[32m0.97287\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 512 | loss: 0.97287 - acc: 0.8072 -- iter: 26/26\n",
            "--\n",
            "Training Step: 513  | total loss: \u001b[1m\u001b[32m0.91423\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 513 | loss: 0.91423 - acc: 0.8265 -- iter: 26/26\n",
            "--\n",
            "Training Step: 514  | total loss: \u001b[1m\u001b[32m0.86136\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 514 | loss: 0.86136 - acc: 0.8439 -- iter: 26/26\n",
            "--\n",
            "Training Step: 515  | total loss: \u001b[1m\u001b[32m0.81365\u001b[0m\u001b[0m | time: 0.021s\n",
            "| Adam | epoch: 515 | loss: 0.81365 - acc: 0.8595 -- iter: 26/26\n",
            "--\n",
            "Training Step: 516  | total loss: \u001b[1m\u001b[32m0.77055\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 516 | loss: 0.77055 - acc: 0.8735 -- iter: 26/26\n",
            "--\n",
            "Training Step: 517  | total loss: \u001b[1m\u001b[32m0.73156\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 517 | loss: 0.73156 - acc: 0.8862 -- iter: 26/26\n",
            "--\n",
            "Training Step: 518  | total loss: \u001b[1m\u001b[32m0.69624\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 518 | loss: 0.69624 - acc: 0.9078 -- iter: 26/26\n",
            "--\n",
            "Training Step: 519  | total loss: \u001b[1m\u001b[32m0.66421\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 519 | loss: 0.66421 - acc: 0.9078 -- iter: 26/26\n",
            "--\n",
            "Training Step: 520  | total loss: \u001b[1m\u001b[32m0.63511\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 520 | loss: 0.63511 - acc: 0.9170 -- iter: 26/26\n",
            "--\n",
            "Training Step: 521  | total loss: \u001b[1m\u001b[32m0.60864\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 521 | loss: 0.60864 - acc: 0.9253 -- iter: 26/26\n",
            "--\n",
            "Training Step: 522  | total loss: \u001b[1m\u001b[32m0.58452\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 522 | loss: 0.58452 - acc: 0.9328 -- iter: 26/26\n",
            "--\n",
            "Training Step: 523  | total loss: \u001b[1m\u001b[32m0.54235\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 523 | loss: 0.54235 - acc: 0.9395 -- iter: 26/26\n",
            "--\n",
            "Training Step: 524  | total loss: \u001b[1m\u001b[32m0.54235\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 524 | loss: 0.54235 - acc: 0.9456 -- iter: 26/26\n",
            "--\n",
            "Training Step: 525  | total loss: \u001b[1m\u001b[32m0.52390\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 525 | loss: 0.52390 - acc: 0.9510 -- iter: 26/26\n",
            "--\n",
            "Training Step: 526  | total loss: \u001b[1m\u001b[32m0.50695\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 526 | loss: 0.50695 - acc: 0.9559 -- iter: 26/26\n",
            "--\n",
            "Training Step: 527  | total loss: \u001b[1m\u001b[32m0.49137\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 527 | loss: 0.49137 - acc: 0.9643 -- iter: 26/26\n",
            "--\n",
            "Training Step: 528  | total loss: \u001b[1m\u001b[32m0.47699\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 528 | loss: 0.47699 - acc: 0.9643 -- iter: 26/26\n",
            "--\n",
            "Training Step: 529  | total loss: \u001b[1m\u001b[32m0.46372\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 529 | loss: 0.46372 - acc: 0.9711 -- iter: 26/26\n",
            "--\n",
            "Training Step: 530  | total loss: \u001b[1m\u001b[32m0.45142\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 530 | loss: 0.45142 - acc: 0.9711 -- iter: 26/26\n",
            "--\n",
            "Training Step: 531  | total loss: \u001b[1m\u001b[32m0.44000\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 531 | loss: 0.44000 - acc: 0.9740 -- iter: 26/26\n",
            "--\n",
            "Training Step: 532  | total loss: \u001b[1m\u001b[32m0.42938\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 532 | loss: 0.42938 - acc: 0.9766 -- iter: 26/26\n",
            "--\n",
            "Training Step: 533  | total loss: \u001b[1m\u001b[32m0.41021\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 533 | loss: 0.41021 - acc: 0.9789 -- iter: 26/26\n",
            "--\n",
            "Training Step: 534  | total loss: \u001b[1m\u001b[32m0.40152\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 534 | loss: 0.40152 - acc: 0.9810 -- iter: 26/26\n",
            "--\n",
            "Training Step: 535  | total loss: \u001b[1m\u001b[32m0.40152\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 535 | loss: 0.40152 - acc: 0.9829 -- iter: 26/26\n",
            "--\n",
            "Training Step: 536  | total loss: \u001b[1m\u001b[32m0.38568\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 536 | loss: 0.38568 - acc: 0.9846 -- iter: 26/26\n",
            "--\n",
            "Training Step: 537  | total loss: \u001b[1m\u001b[32m0.38568\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 537 | loss: 0.38568 - acc: 0.9875 -- iter: 26/26\n",
            "--\n",
            "Training Step: 538  | total loss: \u001b[1m\u001b[32m0.37842\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 538 | loss: 0.37842 - acc: 0.9875 -- iter: 26/26\n",
            "--\n",
            "Training Step: 539  | total loss: \u001b[1m\u001b[32m0.37155\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 539 | loss: 0.37155 - acc: 0.9888 -- iter: 26/26\n",
            "--\n",
            "Training Step: 540  | total loss: \u001b[1m\u001b[32m0.36503\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 540 | loss: 0.36503 - acc: 0.9899 -- iter: 26/26\n",
            "--\n",
            "Training Step: 541  | total loss: \u001b[1m\u001b[32m0.35293\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 541 | loss: 0.35293 - acc: 0.9909 -- iter: 26/26\n",
            "--\n",
            "Training Step: 542  | total loss: \u001b[1m\u001b[32m0.35293\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 542 | loss: 0.35293 - acc: 0.9918 -- iter: 26/26\n",
            "--\n",
            "Training Step: 543  | total loss: \u001b[1m\u001b[32m0.34729\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 543 | loss: 0.34729 - acc: 0.9926 -- iter: 26/26\n",
            "--\n",
            "Training Step: 544  | total loss: \u001b[1m\u001b[32m0.33673\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 544 | loss: 0.33673 - acc: 0.9934 -- iter: 26/26\n",
            "--\n",
            "Training Step: 545  | total loss: \u001b[1m\u001b[32m0.33673\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 545 | loss: 0.33673 - acc: 0.9940 -- iter: 26/26\n",
            "--\n",
            "Training Step: 546  | total loss: \u001b[1m\u001b[32m0.32701\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 546 | loss: 0.32701 - acc: 0.9946 -- iter: 26/26\n",
            "--\n",
            "Training Step: 547  | total loss: \u001b[1m\u001b[32m0.32701\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 547 | loss: 0.32701 - acc: 0.9952 -- iter: 26/26\n",
            "--\n",
            "Training Step: 548  | total loss: \u001b[1m\u001b[32m0.32242\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 548 | loss: 0.32242 - acc: 0.9957 -- iter: 26/26\n",
            "--\n",
            "Training Step: 549  | total loss: \u001b[1m\u001b[32m0.31800\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 549 | loss: 0.31800 - acc: 0.9961 -- iter: 26/26\n",
            "--\n",
            "Training Step: 550  | total loss: \u001b[1m\u001b[32m0.31373\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 550 | loss: 0.31373 - acc: 0.9965 -- iter: 26/26\n",
            "--\n",
            "Training Step: 551  | total loss: \u001b[1m\u001b[32m0.30559\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 551 | loss: 0.30559 - acc: 0.9968 -- iter: 26/26\n",
            "--\n",
            "Training Step: 552  | total loss: \u001b[1m\u001b[32m0.30559\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 552 | loss: 0.30559 - acc: 0.9972 -- iter: 26/26\n",
            "--\n",
            "Training Step: 553  | total loss: \u001b[1m\u001b[32m0.30171\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 553 | loss: 0.30171 - acc: 0.9974 -- iter: 26/26\n",
            "--\n",
            "Training Step: 554  | total loss: \u001b[1m\u001b[32m0.29794\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 554 | loss: 0.29794 - acc: 0.9977 -- iter: 26/26\n",
            "--\n",
            "Training Step: 555  | total loss: \u001b[1m\u001b[32m0.29072\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 555 | loss: 0.29072 - acc: 0.9979 -- iter: 26/26\n",
            "--\n",
            "Training Step: 556  | total loss: \u001b[1m\u001b[32m0.28725\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 556 | loss: 0.28725 - acc: 0.9981 -- iter: 26/26\n",
            "--\n",
            "Training Step: 557  | total loss: \u001b[1m\u001b[32m0.28387\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 557 | loss: 0.28387 - acc: 0.9983 -- iter: 26/26\n",
            "--\n",
            "Training Step: 558  | total loss: \u001b[1m\u001b[32m0.28387\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 558 | loss: 0.28387 - acc: 0.9985 -- iter: 26/26\n",
            "--\n",
            "Training Step: 559  | total loss: \u001b[1m\u001b[32m0.28056\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 559 | loss: 0.28056 - acc: 0.9986 -- iter: 26/26\n",
            "--\n",
            "Training Step: 560  | total loss: \u001b[1m\u001b[32m0.27734\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 560 | loss: 0.27734 - acc: 0.9988 -- iter: 26/26\n",
            "--\n",
            "Training Step: 561  | total loss: \u001b[1m\u001b[32m0.27419\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 561 | loss: 0.27419 - acc: 0.9989 -- iter: 26/26\n",
            "--\n",
            "Training Step: 562  | total loss: \u001b[1m\u001b[32m0.26809\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 562 | loss: 0.26809 - acc: 0.9990 -- iter: 26/26\n",
            "--\n",
            "Training Step: 563  | total loss: \u001b[1m\u001b[32m0.26514\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 563 | loss: 0.26514 - acc: 0.9991 -- iter: 26/26\n",
            "--\n",
            "Training Step: 564  | total loss: \u001b[1m\u001b[32m0.26224\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 564 | loss: 0.26224 - acc: 0.9992 -- iter: 26/26\n",
            "--\n",
            "Training Step: 565  | total loss: \u001b[1m\u001b[32m0.26224\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 565 | loss: 0.26224 - acc: 0.9993 -- iter: 26/26\n",
            "--\n",
            "Training Step: 566  | total loss: \u001b[1m\u001b[32m0.25940\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 566 | loss: 0.25940 - acc: 0.9993 -- iter: 26/26\n",
            "--\n",
            "Training Step: 567  | total loss: \u001b[1m\u001b[32m0.25662\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 567 | loss: 0.25662 - acc: 0.9995 -- iter: 26/26\n",
            "--\n",
            "Training Step: 568  | total loss: \u001b[1m\u001b[32m0.25389\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 568 | loss: 0.25389 - acc: 0.9995 -- iter: 26/26\n",
            "--\n",
            "Training Step: 569  | total loss: \u001b[1m\u001b[32m0.25121\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 569 | loss: 0.25121 - acc: 0.9995 -- iter: 26/26\n",
            "--\n",
            "Training Step: 570  | total loss: \u001b[1m\u001b[32m0.24858\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 570 | loss: 0.24858 - acc: 0.9996 -- iter: 26/26\n",
            "--\n",
            "Training Step: 571  | total loss: \u001b[1m\u001b[32m0.24600\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 571 | loss: 0.24600 - acc: 0.9996 -- iter: 26/26\n",
            "--\n",
            "Training Step: 572  | total loss: \u001b[1m\u001b[32m0.24347\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 572 | loss: 0.24347 - acc: 0.9997 -- iter: 26/26\n",
            "--\n",
            "Training Step: 573  | total loss: \u001b[1m\u001b[32m0.24097\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 573 | loss: 0.24097 - acc: 0.9997 -- iter: 26/26\n",
            "--\n",
            "Training Step: 574  | total loss: \u001b[1m\u001b[32m0.23852\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 574 | loss: 0.23852 - acc: 0.9997 -- iter: 26/26\n",
            "--\n",
            "Training Step: 575  | total loss: \u001b[1m\u001b[32m0.23611\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 575 | loss: 0.23611 - acc: 0.9997 -- iter: 26/26\n",
            "--\n",
            "Training Step: 576  | total loss: \u001b[1m\u001b[32m0.23141\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 576 | loss: 0.23141 - acc: 0.9998 -- iter: 26/26\n",
            "--\n",
            "Training Step: 577  | total loss: \u001b[1m\u001b[32m0.23141\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 577 | loss: 0.23141 - acc: 0.9998 -- iter: 26/26\n",
            "--\n",
            "Training Step: 578  | total loss: \u001b[1m\u001b[32m0.22685\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 578 | loss: 0.22685 - acc: 0.9998 -- iter: 26/26\n",
            "--\n",
            "Training Step: 579  | total loss: \u001b[1m\u001b[32m0.22462\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 579 | loss: 0.22462 - acc: 0.9998 -- iter: 26/26\n",
            "--\n",
            "Training Step: 580  | total loss: \u001b[1m\u001b[32m0.22462\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 580 | loss: 0.22462 - acc: 0.9999 -- iter: 26/26\n",
            "--\n",
            "Training Step: 581  | total loss: \u001b[1m\u001b[32m0.22243\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 581 | loss: 0.22243 - acc: 0.9999 -- iter: 26/26\n",
            "--\n",
            "Training Step: 582  | total loss: \u001b[1m\u001b[32m0.22027\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 582 | loss: 0.22027 - acc: 0.9999 -- iter: 26/26\n",
            "--\n",
            "Training Step: 583  | total loss: \u001b[1m\u001b[32m0.21814\u001b[0m\u001b[0m | time: 0.013s\n",
            "| Adam | epoch: 583 | loss: 0.21814 - acc: 0.9999 -- iter: 26/26\n",
            "--\n",
            "Training Step: 584  | total loss: \u001b[1m\u001b[32m0.21398\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 584 | loss: 0.21398 - acc: 0.9999 -- iter: 26/26\n",
            "--\n",
            "Training Step: 585  | total loss: \u001b[1m\u001b[32m0.21398\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 585 | loss: 0.21398 - acc: 0.9999 -- iter: 26/26\n",
            "--\n",
            "Training Step: 586  | total loss: \u001b[1m\u001b[32m0.21195\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 586 | loss: 0.21195 - acc: 0.9999 -- iter: 26/26\n",
            "--\n",
            "Training Step: 587  | total loss: \u001b[1m\u001b[32m0.20797\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 587 | loss: 0.20797 - acc: 0.9999 -- iter: 26/26\n",
            "--\n",
            "Training Step: 588  | total loss: \u001b[1m\u001b[32m0.20602\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 588 | loss: 0.20602 - acc: 0.9999 -- iter: 26/26\n",
            "--\n",
            "Training Step: 589  | total loss: \u001b[1m\u001b[32m0.20602\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 589 | loss: 0.20602 - acc: 0.9999 -- iter: 26/26\n",
            "--\n",
            "Training Step: 590  | total loss: \u001b[1m\u001b[32m0.20410\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 590 | loss: 0.20410 - acc: 0.9999 -- iter: 26/26\n",
            "--\n",
            "Training Step: 591  | total loss: \u001b[1m\u001b[32m0.20220\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 591 | loss: 0.20220 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 592  | total loss: \u001b[1m\u001b[32m0.20033\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 592 | loss: 0.20033 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 593  | total loss: \u001b[1m\u001b[32m0.19849\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 593 | loss: 0.19849 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 594  | total loss: \u001b[1m\u001b[32m0.19667\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 594 | loss: 0.19667 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 595  | total loss: \u001b[1m\u001b[32m0.19487\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 595 | loss: 0.19487 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 596  | total loss: \u001b[1m\u001b[32m0.19136\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 596 | loss: 0.19136 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 597  | total loss: \u001b[1m\u001b[32m0.18964\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 597 | loss: 0.18964 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 598  | total loss: \u001b[1m\u001b[32m0.18964\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 598 | loss: 0.18964 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 599  | total loss: \u001b[1m\u001b[32m0.18794\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 599 | loss: 0.18794 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 600  | total loss: \u001b[1m\u001b[32m0.18626\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 600 | loss: 0.18626 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 601  | total loss: \u001b[1m\u001b[32m0.18461\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 601 | loss: 0.18461 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 602  | total loss: \u001b[1m\u001b[32m0.18297\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 602 | loss: 0.18297 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 603  | total loss: \u001b[1m\u001b[32m0.18136\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 603 | loss: 0.18136 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 604  | total loss: \u001b[1m\u001b[32m0.17977\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 604 | loss: 0.17977 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 605  | total loss: \u001b[1m\u001b[32m0.17820\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 605 | loss: 0.17820 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 606  | total loss: \u001b[1m\u001b[32m0.17664\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 606 | loss: 0.17664 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 607  | total loss: \u001b[1m\u001b[32m0.17511\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 607 | loss: 0.17511 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 608  | total loss: \u001b[1m\u001b[32m0.17360\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 608 | loss: 0.17360 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 609  | total loss: \u001b[1m\u001b[32m0.17210\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 609 | loss: 0.17210 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 610  | total loss: \u001b[1m\u001b[32m0.16917\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 610 | loss: 0.16917 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 611  | total loss: \u001b[1m\u001b[32m0.16917\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 611 | loss: 0.16917 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 612  | total loss: \u001b[1m\u001b[32m0.16773\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 612 | loss: 0.16773 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 613  | total loss: \u001b[1m\u001b[32m0.16490\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 613 | loss: 0.16490 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 614  | total loss: \u001b[1m\u001b[32m0.16351\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 614 | loss: 0.16351 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 615  | total loss: \u001b[1m\u001b[32m0.16351\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 615 | loss: 0.16351 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 616  | total loss: \u001b[1m\u001b[32m0.16214\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 616 | loss: 0.16214 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 617  | total loss: \u001b[1m\u001b[32m0.16078\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 617 | loss: 0.16078 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 618  | total loss: \u001b[1m\u001b[32m0.15812\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 618 | loss: 0.15812 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 619  | total loss: \u001b[1m\u001b[32m0.15681\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 619 | loss: 0.15681 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 620  | total loss: \u001b[1m\u001b[32m0.15681\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 620 | loss: 0.15681 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 621  | total loss: \u001b[1m\u001b[32m0.15424\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 621 | loss: 0.15424 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 622  | total loss: \u001b[1m\u001b[32m0.15298\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 622 | loss: 0.15298 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 623  | total loss: \u001b[1m\u001b[32m0.15173\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 623 | loss: 0.15173 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 624  | total loss: \u001b[1m\u001b[32m0.15173\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 624 | loss: 0.15173 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 625  | total loss: \u001b[1m\u001b[32m0.14928\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 625 | loss: 0.14928 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 626  | total loss: \u001b[1m\u001b[32m0.14808\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 626 | loss: 0.14808 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 627  | total loss: \u001b[1m\u001b[32m0.14689\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 627 | loss: 0.14689 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 628  | total loss: \u001b[1m\u001b[32m0.14689\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 628 | loss: 0.14689 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 629  | total loss: \u001b[1m\u001b[32m0.14571\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 629 | loss: 0.14571 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 630  | total loss: \u001b[1m\u001b[32m0.14454\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 630 | loss: 0.14454 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 631  | total loss: \u001b[1m\u001b[32m0.14339\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 631 | loss: 0.14339 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 632  | total loss: \u001b[1m\u001b[32m0.14113\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 632 | loss: 0.14113 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 633  | total loss: \u001b[1m\u001b[32m0.14113\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 633 | loss: 0.14113 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 634  | total loss: \u001b[1m\u001b[32m0.13892\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 634 | loss: 0.13892 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 635  | total loss: \u001b[1m\u001b[32m0.13892\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 635 | loss: 0.13892 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 636  | total loss: \u001b[1m\u001b[32m0.13783\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 636 | loss: 0.13783 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 637  | total loss: \u001b[1m\u001b[32m0.13569\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 637 | loss: 0.13569 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 638  | total loss: \u001b[1m\u001b[32m0.13569\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 638 | loss: 0.13569 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 639  | total loss: \u001b[1m\u001b[32m0.13360\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 639 | loss: 0.13360 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 640  | total loss: \u001b[1m\u001b[32m0.13360\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 640 | loss: 0.13360 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 641  | total loss: \u001b[1m\u001b[32m0.13257\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 641 | loss: 0.13257 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 642  | total loss: \u001b[1m\u001b[32m0.13054\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 642 | loss: 0.13054 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 643  | total loss: \u001b[1m\u001b[32m0.13054\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 643 | loss: 0.13054 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 644  | total loss: \u001b[1m\u001b[32m0.12955\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 644 | loss: 0.12955 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 645  | total loss: \u001b[1m\u001b[32m0.12856\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 645 | loss: 0.12856 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 646  | total loss: \u001b[1m\u001b[32m0.12759\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 646 | loss: 0.12759 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 647  | total loss: \u001b[1m\u001b[32m0.12663\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 647 | loss: 0.12663 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 648  | total loss: \u001b[1m\u001b[32m0.12567\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 648 | loss: 0.12567 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 649  | total loss: \u001b[1m\u001b[32m0.12473\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 649 | loss: 0.12473 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 650  | total loss: \u001b[1m\u001b[32m0.12379\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 650 | loss: 0.12379 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 651  | total loss: \u001b[1m\u001b[32m0.12287\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 651 | loss: 0.12287 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 652  | total loss: \u001b[1m\u001b[32m0.12196\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 652 | loss: 0.12196 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 653  | total loss: \u001b[1m\u001b[32m0.12105\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 653 | loss: 0.12105 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 654  | total loss: \u001b[1m\u001b[32m0.12016\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 654 | loss: 0.12016 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 655  | total loss: \u001b[1m\u001b[32m0.11927\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 655 | loss: 0.11927 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 656  | total loss: \u001b[1m\u001b[32m0.11839\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 656 | loss: 0.11839 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 657  | total loss: \u001b[1m\u001b[32m0.11752\u001b[0m\u001b[0m | time: 0.012s\n",
            "| Adam | epoch: 657 | loss: 0.11752 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 658  | total loss: \u001b[1m\u001b[32m0.11667\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 658 | loss: 0.11667 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 659  | total loss: \u001b[1m\u001b[32m0.11582\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 659 | loss: 0.11582 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 660  | total loss: \u001b[1m\u001b[32m0.11497\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 660 | loss: 0.11497 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 661  | total loss: \u001b[1m\u001b[32m0.11414\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 661 | loss: 0.11414 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 662  | total loss: \u001b[1m\u001b[32m0.11332\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 662 | loss: 0.11332 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 663  | total loss: \u001b[1m\u001b[32m0.11250\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 663 | loss: 0.11250 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 664  | total loss: \u001b[1m\u001b[32m0.11170\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 664 | loss: 0.11170 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 665  | total loss: \u001b[1m\u001b[32m0.11090\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 665 | loss: 0.11090 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 666  | total loss: \u001b[1m\u001b[32m0.11011\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 666 | loss: 0.11011 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 667  | total loss: \u001b[1m\u001b[32m0.10932\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 667 | loss: 0.10932 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 668  | total loss: \u001b[1m\u001b[32m0.10855\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 668 | loss: 0.10855 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 669  | total loss: \u001b[1m\u001b[32m0.10778\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 669 | loss: 0.10778 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 670  | total loss: \u001b[1m\u001b[32m0.10702\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 670 | loss: 0.10702 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 671  | total loss: \u001b[1m\u001b[32m0.10627\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 671 | loss: 0.10627 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 672  | total loss: \u001b[1m\u001b[32m0.10552\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 672 | loss: 0.10552 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 673  | total loss: \u001b[1m\u001b[32m0.10479\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 673 | loss: 0.10479 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 674  | total loss: \u001b[1m\u001b[32m0.10406\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 674 | loss: 0.10406 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 675  | total loss: \u001b[1m\u001b[32m0.10333\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 675 | loss: 0.10333 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 676  | total loss: \u001b[1m\u001b[32m0.10262\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 676 | loss: 0.10262 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 677  | total loss: \u001b[1m\u001b[32m0.10191\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 677 | loss: 0.10191 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 678  | total loss: \u001b[1m\u001b[32m0.10121\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 678 | loss: 0.10121 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 679  | total loss: \u001b[1m\u001b[32m0.10051\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 679 | loss: 0.10051 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 680  | total loss: \u001b[1m\u001b[32m0.09982\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 680 | loss: 0.09982 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 681  | total loss: \u001b[1m\u001b[32m0.09914\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 681 | loss: 0.09914 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 682  | total loss: \u001b[1m\u001b[32m0.09847\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 682 | loss: 0.09847 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 683  | total loss: \u001b[1m\u001b[32m0.09780\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 683 | loss: 0.09780 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 684  | total loss: \u001b[1m\u001b[32m0.09714\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 684 | loss: 0.09714 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 685  | total loss: \u001b[1m\u001b[32m0.09648\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 685 | loss: 0.09648 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 686  | total loss: \u001b[1m\u001b[32m0.09583\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 686 | loss: 0.09583 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 687  | total loss: \u001b[1m\u001b[32m0.09519\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 687 | loss: 0.09519 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 688  | total loss: \u001b[1m\u001b[32m0.09455\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 688 | loss: 0.09455 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 689  | total loss: \u001b[1m\u001b[32m0.09392\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 689 | loss: 0.09392 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 690  | total loss: \u001b[1m\u001b[32m0.09329\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 690 | loss: 0.09329 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 691  | total loss: \u001b[1m\u001b[32m0.09267\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 691 | loss: 0.09267 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 692  | total loss: \u001b[1m\u001b[32m0.09206\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 692 | loss: 0.09206 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 693  | total loss: \u001b[1m\u001b[32m0.09145\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 693 | loss: 0.09145 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 694  | total loss: \u001b[1m\u001b[32m0.09085\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 694 | loss: 0.09085 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 695  | total loss: \u001b[1m\u001b[32m0.09025\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 695 | loss: 0.09025 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 696  | total loss: \u001b[1m\u001b[32m0.57296\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 696 | loss: 0.57296 - acc: 0.9115 -- iter: 26/26\n",
            "--\n",
            "Training Step: 697  | total loss: \u001b[1m\u001b[32m0.52414\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 697 | loss: 0.52414 - acc: 0.9204 -- iter: 26/26\n",
            "--\n",
            "Training Step: 698  | total loss: \u001b[1m\u001b[32m0.48022\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 698 | loss: 0.48022 - acc: 0.9283 -- iter: 26/26\n",
            "--\n",
            "Training Step: 699  | total loss: \u001b[1m\u001b[32m0.44072\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 699 | loss: 0.44072 - acc: 0.9355 -- iter: 26/26\n",
            "--\n",
            "Training Step: 700  | total loss: \u001b[1m\u001b[32m0.81850\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 700 | loss: 0.81850 - acc: 0.8650 -- iter: 26/26\n",
            "--\n",
            "Training Step: 701  | total loss: \u001b[1m\u001b[32m0.74527\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 701 | loss: 0.74527 - acc: 0.8785 -- iter: 26/26\n",
            "--\n",
            "Training Step: 702  | total loss: \u001b[1m\u001b[32m0.67944\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 702 | loss: 0.67944 - acc: 0.8907 -- iter: 26/26\n",
            "--\n",
            "Training Step: 703  | total loss: \u001b[1m\u001b[32m0.62027\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 703 | loss: 0.62027 - acc: 0.9016 -- iter: 26/26\n",
            "--\n",
            "Training Step: 704  | total loss: \u001b[1m\u001b[32m1.01359\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 704 | loss: 1.01359 - acc: 0.8345 -- iter: 26/26\n",
            "--\n",
            "Training Step: 705  | total loss: \u001b[1m\u001b[32m0.92118\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 705 | loss: 0.92118 - acc: 0.8511 -- iter: 26/26\n",
            "--\n",
            "Training Step: 706  | total loss: \u001b[1m\u001b[32m0.83813\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 706 | loss: 0.83813 - acc: 0.8660 -- iter: 26/26\n",
            "--\n",
            "Training Step: 707  | total loss: \u001b[1m\u001b[32m0.76349\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 707 | loss: 0.76349 - acc: 0.8794 -- iter: 26/26\n",
            "--\n",
            "Training Step: 708  | total loss: \u001b[1m\u001b[32m1.21602\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 708 | loss: 1.21602 - acc: 0.8068 -- iter: 26/26\n",
            "--\n",
            "Training Step: 709  | total loss: \u001b[1m\u001b[32m1.10386\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 709 | loss: 1.10386 - acc: 0.8261 -- iter: 26/26\n",
            "--\n",
            "Training Step: 710  | total loss: \u001b[1m\u001b[32m1.38365\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 710 | loss: 1.38365 - acc: 0.7666 -- iter: 26/26\n",
            "--\n",
            "Training Step: 711  | total loss: \u001b[1m\u001b[32m1.25513\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 711 | loss: 1.25513 - acc: 0.7899 -- iter: 26/26\n",
            "--\n",
            "Training Step: 712  | total loss: \u001b[1m\u001b[32m1.62100\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 712 | loss: 1.62100 - acc: 0.7263 -- iter: 26/26\n",
            "--\n",
            "Training Step: 713  | total loss: \u001b[1m\u001b[32m1.46925\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 713 | loss: 1.46925 - acc: 0.7537 -- iter: 26/26\n",
            "--\n",
            "Training Step: 714  | total loss: \u001b[1m\u001b[32m1.85239\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 714 | loss: 1.85239 - acc: 0.6899 -- iter: 26/26\n",
            "--\n",
            "Training Step: 715  | total loss: \u001b[1m\u001b[32m1.67813\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 715 | loss: 1.67813 - acc: 0.7209 -- iter: 26/26\n",
            "--\n",
            "Training Step: 716  | total loss: \u001b[1m\u001b[32m1.97457\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 716 | loss: 1.97457 - acc: 0.6680 -- iter: 26/26\n",
            "--\n",
            "Training Step: 717  | total loss: \u001b[1m\u001b[32m1.78887\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 717 | loss: 1.78887 - acc: 0.7012 -- iter: 26/26\n",
            "--\n",
            "Training Step: 718  | total loss: \u001b[1m\u001b[32m1.62213\u001b[0m\u001b[0m | time: 0.012s\n",
            "| Adam | epoch: 718 | loss: 1.62213 - acc: 0.7311 -- iter: 26/26\n",
            "--\n",
            "Training Step: 719  | total loss: \u001b[1m\u001b[32m1.47243\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 719 | loss: 1.47243 - acc: 0.7580 -- iter: 26/26\n",
            "--\n",
            "Training Step: 720  | total loss: \u001b[1m\u001b[32m1.33802\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 720 | loss: 1.33802 - acc: 0.7822 -- iter: 26/26\n",
            "--\n",
            "Training Step: 721  | total loss: \u001b[1m\u001b[32m1.21734\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 721 | loss: 1.21734 - acc: 0.8040 -- iter: 26/26\n",
            "--\n",
            "Training Step: 722  | total loss: \u001b[1m\u001b[32m1.10897\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 722 | loss: 1.10897 - acc: 0.8236 -- iter: 26/26\n",
            "--\n",
            "Training Step: 723  | total loss: \u001b[1m\u001b[32m1.01164\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 723 | loss: 1.01164 - acc: 0.8412 -- iter: 26/26\n",
            "--\n",
            "Training Step: 724  | total loss: \u001b[1m\u001b[32m0.92421\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 724 | loss: 0.92421 - acc: 0.8571 -- iter: 26/26\n",
            "--\n",
            "Training Step: 725  | total loss: \u001b[1m\u001b[32m0.84566\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 725 | loss: 0.84566 - acc: 0.8714 -- iter: 26/26\n",
            "--\n",
            "Training Step: 726  | total loss: \u001b[1m\u001b[32m0.77506\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 726 | loss: 0.77506 - acc: 0.8842 -- iter: 26/26\n",
            "--\n",
            "Training Step: 727  | total loss: \u001b[1m\u001b[32m0.71160\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 727 | loss: 0.71160 - acc: 0.8958 -- iter: 26/26\n",
            "--\n",
            "Training Step: 728  | total loss: \u001b[1m\u001b[32m0.65453\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 728 | loss: 0.65453 - acc: 0.9062 -- iter: 26/26\n",
            "--\n",
            "Training Step: 729  | total loss: \u001b[1m\u001b[32m0.60319\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 729 | loss: 0.60319 - acc: 0.9156 -- iter: 26/26\n",
            "--\n",
            "Training Step: 730  | total loss: \u001b[1m\u001b[32m0.55699\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 730 | loss: 0.55699 - acc: 0.9241 -- iter: 26/26\n",
            "--\n",
            "Training Step: 731  | total loss: \u001b[1m\u001b[32m0.51539\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 731 | loss: 0.51539 - acc: 0.9316 -- iter: 26/26\n",
            "--\n",
            "Training Step: 732  | total loss: \u001b[1m\u001b[32m0.47793\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 732 | loss: 0.47793 - acc: 0.9385 -- iter: 26/26\n",
            "--\n",
            "Training Step: 733  | total loss: \u001b[1m\u001b[32m0.44416\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 733 | loss: 0.44416 - acc: 0.9446 -- iter: 26/26\n",
            "--\n",
            "Training Step: 734  | total loss: \u001b[1m\u001b[32m0.41372\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 734 | loss: 0.41372 - acc: 0.9502 -- iter: 26/26\n",
            "--\n",
            "Training Step: 735  | total loss: \u001b[1m\u001b[32m0.36147\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 735 | loss: 0.36147 - acc: 0.9552 -- iter: 26/26\n",
            "--\n",
            "Training Step: 736  | total loss: \u001b[1m\u001b[32m0.36147\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 736 | loss: 0.36147 - acc: 0.9596 -- iter: 26/26\n",
            "--\n",
            "Training Step: 737  | total loss: \u001b[1m\u001b[32m0.33908\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 737 | loss: 0.33908 - acc: 0.9637 -- iter: 26/26\n",
            "--\n",
            "Training Step: 738  | total loss: \u001b[1m\u001b[32m0.31884\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 738 | loss: 0.31884 - acc: 0.9673 -- iter: 26/26\n",
            "--\n",
            "Training Step: 739  | total loss: \u001b[1m\u001b[32m0.30054\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 739 | loss: 0.30054 - acc: 0.9706 -- iter: 26/26\n",
            "--\n",
            "Training Step: 740  | total loss: \u001b[1m\u001b[32m0.28398\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 740 | loss: 0.28398 - acc: 0.9735 -- iter: 26/26\n",
            "--\n",
            "Training Step: 741  | total loss: \u001b[1m\u001b[32m0.26897\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 741 | loss: 0.26897 - acc: 0.9762 -- iter: 26/26\n",
            "--\n",
            "Training Step: 742  | total loss: \u001b[1m\u001b[32m0.25537\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 742 | loss: 0.25537 - acc: 0.9786 -- iter: 26/26\n",
            "--\n",
            "Training Step: 743  | total loss: \u001b[1m\u001b[32m0.24303\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 743 | loss: 0.24303 - acc: 0.9807 -- iter: 26/26\n",
            "--\n",
            "Training Step: 744  | total loss: \u001b[1m\u001b[32m0.23183\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 744 | loss: 0.23183 - acc: 0.9826 -- iter: 26/26\n",
            "--\n",
            "Training Step: 745  | total loss: \u001b[1m\u001b[32m0.21237\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 745 | loss: 0.21237 - acc: 0.9844 -- iter: 26/26\n",
            "--\n",
            "Training Step: 746  | total loss: \u001b[1m\u001b[32m0.21237\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 746 | loss: 0.21237 - acc: 0.9859 -- iter: 26/26\n",
            "--\n",
            "Training Step: 747  | total loss: \u001b[1m\u001b[32m0.20394\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 747 | loss: 0.20394 - acc: 0.9873 -- iter: 26/26\n",
            "--\n",
            "Training Step: 748  | total loss: \u001b[1m\u001b[32m0.19624\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 748 | loss: 0.19624 - acc: 0.9886 -- iter: 26/26\n",
            "--\n",
            "Training Step: 749  | total loss: \u001b[1m\u001b[32m0.18279\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 749 | loss: 0.18279 - acc: 0.9897 -- iter: 26/26\n",
            "--\n",
            "Training Step: 750  | total loss: \u001b[1m\u001b[32m0.18279\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 750 | loss: 0.18279 - acc: 0.9908 -- iter: 26/26\n",
            "--\n",
            "Training Step: 751  | total loss: \u001b[1m\u001b[32m0.17692\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 751 | loss: 0.17692 - acc: 0.9917 -- iter: 26/26\n",
            "--\n",
            "Training Step: 752  | total loss: \u001b[1m\u001b[32m0.17153\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 752 | loss: 0.17153 - acc: 0.9925 -- iter: 26/26\n",
            "--\n",
            "Training Step: 753  | total loss: \u001b[1m\u001b[32m0.16658\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 753 | loss: 0.16658 - acc: 0.9933 -- iter: 26/26\n",
            "--\n",
            "Training Step: 754  | total loss: \u001b[1m\u001b[32m0.16204\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 754 | loss: 0.16204 - acc: 0.9939 -- iter: 26/26\n",
            "--\n",
            "Training Step: 755  | total loss: \u001b[1m\u001b[32m0.15785\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 755 | loss: 0.15785 - acc: 0.9945 -- iter: 26/26\n",
            "--\n",
            "Training Step: 756  | total loss: \u001b[1m\u001b[32m0.15398\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 756 | loss: 0.15398 - acc: 0.9951 -- iter: 26/26\n",
            "--\n",
            "Training Step: 757  | total loss: \u001b[1m\u001b[32m0.15041\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 757 | loss: 0.15041 - acc: 0.9956 -- iter: 26/26\n",
            "--\n",
            "Training Step: 758  | total loss: \u001b[1m\u001b[32m0.14709\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 758 | loss: 0.14709 - acc: 0.9960 -- iter: 26/26\n",
            "--\n",
            "Training Step: 759  | total loss: \u001b[1m\u001b[32m0.14402\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 759 | loss: 0.14402 - acc: 0.9964 -- iter: 26/26\n",
            "--\n",
            "Training Step: 760  | total loss: \u001b[1m\u001b[32m0.14116\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 760 | loss: 0.14116 - acc: 0.9968 -- iter: 26/26\n",
            "--\n",
            "Training Step: 761  | total loss: \u001b[1m\u001b[32m0.13849\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 761 | loss: 0.13849 - acc: 0.9971 -- iter: 26/26\n",
            "--\n",
            "Training Step: 762  | total loss: \u001b[1m\u001b[32m0.13600\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 762 | loss: 0.13600 - acc: 0.9974 -- iter: 26/26\n",
            "--\n",
            "Training Step: 763  | total loss: \u001b[1m\u001b[32m0.13367\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 763 | loss: 0.13367 - acc: 0.9977 -- iter: 26/26\n",
            "--\n",
            "Training Step: 764  | total loss: \u001b[1m\u001b[32m0.13148\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 764 | loss: 0.13148 - acc: 0.9979 -- iter: 26/26\n",
            "--\n",
            "Training Step: 765  | total loss: \u001b[1m\u001b[32m0.12941\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 765 | loss: 0.12941 - acc: 0.9981 -- iter: 26/26\n",
            "--\n",
            "Training Step: 766  | total loss: \u001b[1m\u001b[32m0.12747\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 766 | loss: 0.12747 - acc: 0.9983 -- iter: 26/26\n",
            "--\n",
            "Training Step: 767  | total loss: \u001b[1m\u001b[32m0.12564\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 767 | loss: 0.12564 - acc: 0.9985 -- iter: 26/26\n",
            "--\n",
            "Training Step: 768  | total loss: \u001b[1m\u001b[32m0.12390\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 768 | loss: 0.12390 - acc: 0.9986 -- iter: 26/26\n",
            "--\n",
            "Training Step: 769  | total loss: \u001b[1m\u001b[32m0.12226\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 769 | loss: 0.12226 - acc: 0.9988 -- iter: 26/26\n",
            "--\n",
            "Training Step: 770  | total loss: \u001b[1m\u001b[32m0.12069\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 770 | loss: 0.12069 - acc: 0.9989 -- iter: 26/26\n",
            "--\n",
            "Training Step: 771  | total loss: \u001b[1m\u001b[32m0.11777\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 771 | loss: 0.11777 - acc: 0.9990 -- iter: 26/26\n",
            "--\n",
            "Training Step: 772  | total loss: \u001b[1m\u001b[32m0.11777\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 772 | loss: 0.11777 - acc: 0.9991 -- iter: 26/26\n",
            "--\n",
            "Training Step: 773  | total loss: \u001b[1m\u001b[32m0.11641\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 773 | loss: 0.11641 - acc: 0.9992 -- iter: 26/26\n",
            "--\n",
            "Training Step: 774  | total loss: \u001b[1m\u001b[32m0.11510\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 774 | loss: 0.11510 - acc: 0.9993 -- iter: 26/26\n",
            "--\n",
            "Training Step: 775  | total loss: \u001b[1m\u001b[32m0.11384\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 775 | loss: 0.11384 - acc: 0.9993 -- iter: 26/26\n",
            "--\n",
            "Training Step: 776  | total loss: \u001b[1m\u001b[32m0.11147\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 776 | loss: 0.11147 - acc: 0.9995 -- iter: 26/26\n",
            "--\n",
            "Training Step: 777  | total loss: \u001b[1m\u001b[32m0.11147\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 777 | loss: 0.11147 - acc: 0.9995 -- iter: 26/26\n",
            "--\n",
            "Training Step: 778  | total loss: \u001b[1m\u001b[32m0.11034\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 778 | loss: 0.11034 - acc: 0.9995 -- iter: 26/26\n",
            "--\n",
            "Training Step: 779  | total loss: \u001b[1m\u001b[32m0.10925\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 779 | loss: 0.10925 - acc: 0.9996 -- iter: 26/26\n",
            "--\n",
            "Training Step: 780  | total loss: \u001b[1m\u001b[32m0.10820\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 780 | loss: 0.10820 - acc: 0.9996 -- iter: 26/26\n",
            "--\n",
            "Training Step: 781  | total loss: \u001b[1m\u001b[32m0.10717\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 781 | loss: 0.10717 - acc: 0.9996 -- iter: 26/26\n",
            "--\n",
            "Training Step: 782  | total loss: \u001b[1m\u001b[32m0.10618\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 782 | loss: 0.10618 - acc: 0.9997 -- iter: 26/26\n",
            "--\n",
            "Training Step: 783  | total loss: \u001b[1m\u001b[32m0.10521\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 783 | loss: 0.10521 - acc: 0.9997 -- iter: 26/26\n",
            "--\n",
            "Training Step: 784  | total loss: \u001b[1m\u001b[32m0.10427\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 784 | loss: 0.10427 - acc: 0.9997 -- iter: 26/26\n",
            "--\n",
            "Training Step: 785  | total loss: \u001b[1m\u001b[32m0.10335\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 785 | loss: 0.10335 - acc: 0.9998 -- iter: 26/26\n",
            "--\n",
            "Training Step: 786  | total loss: \u001b[1m\u001b[32m0.10245\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 786 | loss: 0.10245 - acc: 0.9998 -- iter: 26/26\n",
            "--\n",
            "Training Step: 787  | total loss: \u001b[1m\u001b[32m0.10158\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 787 | loss: 0.10158 - acc: 0.9998 -- iter: 26/26\n",
            "--\n",
            "Training Step: 788  | total loss: \u001b[1m\u001b[32m0.10072\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 788 | loss: 0.10072 - acc: 0.9998 -- iter: 26/26\n",
            "--\n",
            "Training Step: 789  | total loss: \u001b[1m\u001b[32m0.09989\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 789 | loss: 0.09989 - acc: 0.9998 -- iter: 26/26\n",
            "--\n",
            "Training Step: 790  | total loss: \u001b[1m\u001b[32m0.09907\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 790 | loss: 0.09907 - acc: 0.9999 -- iter: 26/26\n",
            "--\n",
            "Training Step: 791  | total loss: \u001b[1m\u001b[32m0.09826\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 791 | loss: 0.09826 - acc: 0.9999 -- iter: 26/26\n",
            "--\n",
            "Training Step: 792  | total loss: \u001b[1m\u001b[32m0.09747\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 792 | loss: 0.09747 - acc: 0.9999 -- iter: 26/26\n",
            "--\n",
            "Training Step: 793  | total loss: \u001b[1m\u001b[32m0.09670\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 793 | loss: 0.09670 - acc: 0.9999 -- iter: 26/26\n",
            "--\n",
            "Training Step: 794  | total loss: \u001b[1m\u001b[32m0.09594\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 794 | loss: 0.09594 - acc: 0.9999 -- iter: 26/26\n",
            "--\n",
            "Training Step: 795  | total loss: \u001b[1m\u001b[32m0.09519\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 795 | loss: 0.09519 - acc: 0.9999 -- iter: 26/26\n",
            "--\n",
            "Training Step: 796  | total loss: \u001b[1m\u001b[32m0.09445\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 796 | loss: 0.09445 - acc: 0.9999 -- iter: 26/26\n",
            "--\n",
            "Training Step: 797  | total loss: \u001b[1m\u001b[32m0.09373\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 797 | loss: 0.09373 - acc: 0.9999 -- iter: 26/26\n",
            "--\n",
            "Training Step: 798  | total loss: \u001b[1m\u001b[32m0.09302\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 798 | loss: 0.09302 - acc: 0.9999 -- iter: 26/26\n",
            "--\n",
            "Training Step: 799  | total loss: \u001b[1m\u001b[32m0.09232\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 799 | loss: 0.09232 - acc: 0.9999 -- iter: 26/26\n",
            "--\n",
            "Training Step: 800  | total loss: \u001b[1m\u001b[32m0.09163\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 800 | loss: 0.09163 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 801  | total loss: \u001b[1m\u001b[32m0.09095\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 801 | loss: 0.09095 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 802  | total loss: \u001b[1m\u001b[32m0.09028\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 802 | loss: 0.09028 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 803  | total loss: \u001b[1m\u001b[32m0.08962\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 803 | loss: 0.08962 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 804  | total loss: \u001b[1m\u001b[32m0.08896\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 804 | loss: 0.08896 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 805  | total loss: \u001b[1m\u001b[32m0.08832\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 805 | loss: 0.08832 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 806  | total loss: \u001b[1m\u001b[32m0.08769\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 806 | loss: 0.08769 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 807  | total loss: \u001b[1m\u001b[32m0.08706\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 807 | loss: 0.08706 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 808  | total loss: \u001b[1m\u001b[32m0.08644\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 808 | loss: 0.08644 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 809  | total loss: \u001b[1m\u001b[32m0.08583\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 809 | loss: 0.08583 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 810  | total loss: \u001b[1m\u001b[32m0.08523\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 810 | loss: 0.08523 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 811  | total loss: \u001b[1m\u001b[32m0.08463\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 811 | loss: 0.08463 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 812  | total loss: \u001b[1m\u001b[32m0.08405\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 812 | loss: 0.08405 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 813  | total loss: \u001b[1m\u001b[32m0.08347\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 813 | loss: 0.08347 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 814  | total loss: \u001b[1m\u001b[32m0.08289\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 814 | loss: 0.08289 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 815  | total loss: \u001b[1m\u001b[32m0.08233\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 815 | loss: 0.08233 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 816  | total loss: \u001b[1m\u001b[32m0.08177\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 816 | loss: 0.08177 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 817  | total loss: \u001b[1m\u001b[32m0.08122\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 817 | loss: 0.08122 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 818  | total loss: \u001b[1m\u001b[32m0.08067\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 818 | loss: 0.08067 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 819  | total loss: \u001b[1m\u001b[32m0.08013\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 819 | loss: 0.08013 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 820  | total loss: \u001b[1m\u001b[32m0.07960\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 820 | loss: 0.07960 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 821  | total loss: \u001b[1m\u001b[32m0.07907\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 821 | loss: 0.07907 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 822  | total loss: \u001b[1m\u001b[32m0.07855\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 822 | loss: 0.07855 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 823  | total loss: \u001b[1m\u001b[32m0.07803\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 823 | loss: 0.07803 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 824  | total loss: \u001b[1m\u001b[32m0.07752\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 824 | loss: 0.07752 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 825  | total loss: \u001b[1m\u001b[32m0.07702\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 825 | loss: 0.07702 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 826  | total loss: \u001b[1m\u001b[32m0.07652\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 826 | loss: 0.07652 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 827  | total loss: \u001b[1m\u001b[32m0.07602\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 827 | loss: 0.07602 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 828  | total loss: \u001b[1m\u001b[32m0.07554\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 828 | loss: 0.07554 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 829  | total loss: \u001b[1m\u001b[32m0.07505\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 829 | loss: 0.07505 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 830  | total loss: \u001b[1m\u001b[32m0.07458\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 830 | loss: 0.07458 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 831  | total loss: \u001b[1m\u001b[32m0.07411\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 831 | loss: 0.07411 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 832  | total loss: \u001b[1m\u001b[32m0.07364\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 832 | loss: 0.07364 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 833  | total loss: \u001b[1m\u001b[32m0.07318\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 833 | loss: 0.07318 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 834  | total loss: \u001b[1m\u001b[32m0.07272\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 834 | loss: 0.07272 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 835  | total loss: \u001b[1m\u001b[32m0.07227\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 835 | loss: 0.07227 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 836  | total loss: \u001b[1m\u001b[32m0.07182\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 836 | loss: 0.07182 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 837  | total loss: \u001b[1m\u001b[32m0.07138\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 837 | loss: 0.07138 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 838  | total loss: \u001b[1m\u001b[32m0.63027\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 838 | loss: 0.63027 - acc: 0.9077 -- iter: 26/26\n",
            "--\n",
            "Training Step: 839  | total loss: \u001b[1m\u001b[32m0.57400\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 839 | loss: 0.57400 - acc: 0.9169 -- iter: 26/26\n",
            "--\n",
            "Training Step: 840  | total loss: \u001b[1m\u001b[32m1.08278\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 840 | loss: 1.08278 - acc: 0.8445 -- iter: 26/26\n",
            "--\n",
            "Training Step: 841  | total loss: \u001b[1m\u001b[32m0.98142\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 841 | loss: 0.98142 - acc: 0.8600 -- iter: 26/26\n",
            "--\n",
            "Training Step: 842  | total loss: \u001b[1m\u001b[32m1.40364\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 842 | loss: 1.40364 - acc: 0.7856 -- iter: 26/26\n",
            "--\n",
            "Training Step: 843  | total loss: \u001b[1m\u001b[32m1.27051\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 843 | loss: 1.27051 - acc: 0.8070 -- iter: 26/26\n",
            "--\n",
            "Training Step: 844  | total loss: \u001b[1m\u001b[32m1.62789\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 844 | loss: 1.62789 - acc: 0.7378 -- iter: 26/26\n",
            "--\n",
            "Training Step: 845  | total loss: \u001b[1m\u001b[32m1.47275\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 845 | loss: 1.47275 - acc: 0.7641 -- iter: 26/26\n",
            "--\n",
            "Training Step: 846  | total loss: \u001b[1m\u001b[32m1.72138\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 846 | loss: 1.72138 - acc: 0.7146 -- iter: 26/26\n",
            "--\n",
            "Training Step: 847  | total loss: \u001b[1m\u001b[32m1.55740\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 847 | loss: 1.55740 - acc: 0.7431 -- iter: 26/26\n",
            "--\n",
            "Training Step: 848  | total loss: \u001b[1m\u001b[32m1.85356\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 848 | loss: 1.85356 - acc: 0.6880 -- iter: 26/26\n",
            "--\n",
            "Training Step: 849  | total loss: \u001b[1m\u001b[32m1.67693\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 849 | loss: 1.67693 - acc: 0.7192 -- iter: 26/26\n",
            "--\n",
            "Training Step: 850  | total loss: \u001b[1m\u001b[32m1.97377\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 850 | loss: 1.97377 - acc: 0.6512 -- iter: 26/26\n",
            "--\n",
            "Training Step: 851  | total loss: \u001b[1m\u001b[32m1.78579\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 851 | loss: 1.78579 - acc: 0.6860 -- iter: 26/26\n",
            "--\n",
            "Training Step: 852  | total loss: \u001b[1m\u001b[32m2.05993\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 852 | loss: 2.05993 - acc: 0.6290 -- iter: 26/26\n",
            "--\n",
            "Training Step: 853  | total loss: \u001b[1m\u001b[32m1.86410\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 853 | loss: 1.86410 - acc: 0.6661 -- iter: 26/26\n",
            "--\n",
            "Training Step: 854  | total loss: \u001b[1m\u001b[32m2.14632\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 854 | loss: 2.14632 - acc: 0.6149 -- iter: 26/26\n",
            "--\n",
            "Training Step: 855  | total loss: \u001b[1m\u001b[32m1.94273\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 855 | loss: 1.94273 - acc: 0.6534 -- iter: 26/26\n",
            "--\n",
            "Training Step: 856  | total loss: \u001b[1m\u001b[32m2.29179\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 856 | loss: 2.29179 - acc: 0.5919 -- iter: 26/26\n",
            "--\n",
            "Training Step: 857  | total loss: \u001b[1m\u001b[32m2.07464\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 857 | loss: 2.07464 - acc: 0.6327 -- iter: 26/26\n",
            "--\n",
            "Training Step: 858  | total loss: \u001b[1m\u001b[32m2.31718\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 858 | loss: 2.31718 - acc: 0.5771 -- iter: 26/26\n",
            "--\n",
            "Training Step: 859  | total loss: \u001b[1m\u001b[32m2.09859\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 859 | loss: 2.09859 - acc: 0.6194 -- iter: 26/26\n",
            "--\n",
            "Training Step: 860  | total loss: \u001b[1m\u001b[32m1.90240\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 860 | loss: 1.90240 - acc: 0.6575 -- iter: 26/26\n",
            "--\n",
            "Training Step: 861  | total loss: \u001b[1m\u001b[32m1.72630\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 861 | loss: 1.72630 - acc: 0.6917 -- iter: 26/26\n",
            "--\n",
            "Training Step: 862  | total loss: \u001b[1m\u001b[32m1.94865\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 862 | loss: 1.94865 - acc: 0.6418 -- iter: 26/26\n",
            "--\n",
            "Training Step: 863  | total loss: \u001b[1m\u001b[32m1.76888\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 863 | loss: 1.76888 - acc: 0.6776 -- iter: 26/26\n",
            "--\n",
            "Training Step: 864  | total loss: \u001b[1m\u001b[32m1.60755\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 864 | loss: 1.60755 - acc: 0.7098 -- iter: 26/26\n",
            "--\n",
            "Training Step: 865  | total loss: \u001b[1m\u001b[32m1.46277\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 865 | loss: 1.46277 - acc: 0.7389 -- iter: 26/26\n",
            "--\n",
            "Training Step: 866  | total loss: \u001b[1m\u001b[32m1.69210\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 866 | loss: 1.69210 - acc: 0.6804 -- iter: 26/26\n",
            "--\n",
            "Training Step: 867  | total loss: \u001b[1m\u001b[32m1.53967\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 867 | loss: 1.53967 - acc: 0.7123 -- iter: 26/26\n",
            "--\n",
            "Training Step: 868  | total loss: \u001b[1m\u001b[32m1.40288\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 868 | loss: 1.40288 - acc: 0.7411 -- iter: 26/26\n",
            "--\n",
            "Training Step: 869  | total loss: \u001b[1m\u001b[32m1.28010\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 869 | loss: 1.28010 - acc: 0.7670 -- iter: 26/26\n",
            "--\n",
            "Training Step: 870  | total loss: \u001b[1m\u001b[32m1.16990\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 870 | loss: 1.16990 - acc: 0.7903 -- iter: 26/26\n",
            "--\n",
            "Training Step: 871  | total loss: \u001b[1m\u001b[32m1.07095\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 871 | loss: 1.07095 - acc: 0.8113 -- iter: 26/26\n",
            "--\n",
            "Training Step: 872  | total loss: \u001b[1m\u001b[32m1.40045\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 872 | loss: 1.40045 - acc: 0.7417 -- iter: 26/26\n",
            "--\n",
            "Training Step: 873  | total loss: \u001b[1m\u001b[32m1.27899\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 873 | loss: 1.27899 - acc: 0.7675 -- iter: 26/26\n",
            "--\n",
            "Training Step: 874  | total loss: \u001b[1m\u001b[32m1.16997\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 874 | loss: 1.16997 - acc: 0.7907 -- iter: 26/26\n",
            "--\n",
            "Training Step: 875  | total loss: \u001b[1m\u001b[32m1.07210\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 875 | loss: 1.07210 - acc: 0.8117 -- iter: 26/26\n",
            "--\n",
            "Training Step: 876  | total loss: \u001b[1m\u001b[32m1.24752\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 876 | loss: 1.24752 - acc: 0.7613 -- iter: 26/26\n",
            "--\n",
            "Training Step: 877  | total loss: \u001b[1m\u001b[32m1.14238\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 877 | loss: 1.14238 - acc: 0.7851 -- iter: 26/26\n",
            "--\n",
            "Training Step: 878  | total loss: \u001b[1m\u001b[32m1.35090\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 878 | loss: 1.35090 - acc: 0.7374 -- iter: 26/26\n",
            "--\n",
            "Training Step: 879  | total loss: \u001b[1m\u001b[32m1.23602\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 879 | loss: 1.23602 - acc: 0.7637 -- iter: 26/26\n",
            "--\n",
            "Training Step: 880  | total loss: \u001b[1m\u001b[32m1.42552\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 880 | loss: 1.42552 - acc: 0.6988 -- iter: 26/26\n",
            "--\n",
            "Training Step: 881  | total loss: \u001b[1m\u001b[32m1.30388\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 881 | loss: 1.30388 - acc: 0.7290 -- iter: 26/26\n",
            "--\n",
            "Training Step: 882  | total loss: \u001b[1m\u001b[32m1.53342\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 882 | loss: 1.53342 - acc: 0.6637 -- iter: 26/26\n",
            "--\n",
            "Training Step: 883  | total loss: \u001b[1m\u001b[32m1.40183\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 883 | loss: 1.40183 - acc: 0.6974 -- iter: 26/26\n",
            "--\n",
            "Training Step: 884  | total loss: \u001b[1m\u001b[32m1.28382\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 884 | loss: 1.28382 - acc: 0.7276 -- iter: 26/26\n",
            "--\n",
            "Training Step: 885  | total loss: \u001b[1m\u001b[32m1.17796\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 885 | loss: 1.17796 - acc: 0.7549 -- iter: 26/26\n",
            "--\n",
            "Training Step: 886  | total loss: \u001b[1m\u001b[32m1.35980\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 886 | loss: 1.35980 - acc: 0.7025 -- iter: 26/26\n",
            "--\n",
            "Training Step: 887  | total loss: \u001b[1m\u001b[32m1.24702\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 887 | loss: 1.24702 - acc: 0.7322 -- iter: 26/26\n",
            "--\n",
            "Training Step: 888  | total loss: \u001b[1m\u001b[32m1.44123\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 888 | loss: 1.44123 - acc: 0.6744 -- iter: 26/26\n",
            "--\n",
            "Training Step: 889  | total loss: \u001b[1m\u001b[32m1.32107\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 889 | loss: 1.32107 - acc: 0.7069 -- iter: 26/26\n",
            "--\n",
            "Training Step: 890  | total loss: \u001b[1m\u001b[32m1.21331\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 890 | loss: 1.21331 - acc: 0.7362 -- iter: 26/26\n",
            "--\n",
            "Training Step: 891  | total loss: \u001b[1m\u001b[32m1.11661\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 891 | loss: 1.11661 - acc: 0.7626 -- iter: 26/26\n",
            "--\n",
            "Training Step: 892  | total loss: \u001b[1m\u001b[32m1.02980\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 892 | loss: 1.02980 - acc: 0.7864 -- iter: 26/26\n",
            "--\n",
            "Training Step: 893  | total loss: \u001b[1m\u001b[32m0.95184\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 893 | loss: 0.95184 - acc: 0.8077 -- iter: 26/26\n",
            "--\n",
            "Training Step: 894  | total loss: \u001b[1m\u001b[32m0.88177\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 894 | loss: 0.88177 - acc: 0.8270 -- iter: 26/26\n",
            "--\n",
            "Training Step: 895  | total loss: \u001b[1m\u001b[32m0.81875\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 895 | loss: 0.81875 - acc: 0.8443 -- iter: 26/26\n",
            "--\n",
            "Training Step: 896  | total loss: \u001b[1m\u001b[32m1.06345\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 896 | loss: 1.06345 - acc: 0.7714 -- iter: 26/26\n",
            "--\n",
            "Training Step: 897  | total loss: \u001b[1m\u001b[32m0.98244\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 897 | loss: 0.98244 - acc: 0.7942 -- iter: 26/26\n",
            "--\n",
            "Training Step: 898  | total loss: \u001b[1m\u001b[32m0.90965\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 898 | loss: 0.90965 - acc: 0.8148 -- iter: 26/26\n",
            "--\n",
            "Training Step: 899  | total loss: \u001b[1m\u001b[32m0.84420\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 899 | loss: 0.84420 - acc: 0.8333 -- iter: 26/26\n",
            "--\n",
            "Training Step: 900  | total loss: \u001b[1m\u001b[32m0.78531\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 900 | loss: 0.78531 - acc: 0.8500 -- iter: 26/26\n",
            "--\n",
            "Training Step: 901  | total loss: \u001b[1m\u001b[32m0.73228\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 901 | loss: 0.73228 - acc: 0.8650 -- iter: 26/26\n",
            "--\n",
            "Training Step: 902  | total loss: \u001b[1m\u001b[32m0.68447\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 902 | loss: 0.68447 - acc: 0.8785 -- iter: 26/26\n",
            "--\n",
            "Training Step: 903  | total loss: \u001b[1m\u001b[32m0.64133\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 903 | loss: 0.64133 - acc: 0.8906 -- iter: 26/26\n",
            "--\n",
            "Training Step: 904  | total loss: \u001b[1m\u001b[32m0.60236\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 904 | loss: 0.60236 - acc: 0.9016 -- iter: 26/26\n",
            "--\n",
            "Training Step: 905  | total loss: \u001b[1m\u001b[32m0.56712\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 905 | loss: 0.56712 - acc: 0.9114 -- iter: 26/26\n",
            "--\n",
            "Training Step: 906  | total loss: \u001b[1m\u001b[32m0.53521\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 906 | loss: 0.53521 - acc: 0.9203 -- iter: 26/26\n",
            "--\n",
            "Training Step: 907  | total loss: \u001b[1m\u001b[32m0.50627\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 907 | loss: 0.50627 - acc: 0.9283 -- iter: 26/26\n",
            "--\n",
            "Training Step: 908  | total loss: \u001b[1m\u001b[32m0.48000\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 908 | loss: 0.48000 - acc: 0.9354 -- iter: 26/26\n",
            "--\n",
            "Training Step: 909  | total loss: \u001b[1m\u001b[32m0.45612\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 909 | loss: 0.45612 - acc: 0.9419 -- iter: 26/26\n",
            "--\n",
            "Training Step: 910  | total loss: \u001b[1m\u001b[32m0.43436\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 910 | loss: 0.43436 - acc: 0.9477 -- iter: 26/26\n",
            "--\n",
            "Training Step: 911  | total loss: \u001b[1m\u001b[32m0.41452\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 911 | loss: 0.41452 - acc: 0.9529 -- iter: 26/26\n",
            "--\n",
            "Training Step: 912  | total loss: \u001b[1m\u001b[32m0.39640\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 912 | loss: 0.39640 - acc: 0.9576 -- iter: 26/26\n",
            "--\n",
            "Training Step: 913  | total loss: \u001b[1m\u001b[32m0.37981\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 913 | loss: 0.37981 - acc: 0.9619 -- iter: 26/26\n",
            "--\n",
            "Training Step: 914  | total loss: \u001b[1m\u001b[32m0.36460\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 914 | loss: 0.36460 - acc: 0.9657 -- iter: 26/26\n",
            "--\n",
            "Training Step: 915  | total loss: \u001b[1m\u001b[32m0.35063\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 915 | loss: 0.35063 - acc: 0.9691 -- iter: 26/26\n",
            "--\n",
            "Training Step: 916  | total loss: \u001b[1m\u001b[32m0.33778\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 916 | loss: 0.33778 - acc: 0.9722 -- iter: 26/26\n",
            "--\n",
            "Training Step: 917  | total loss: \u001b[1m\u001b[32m0.32593\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 917 | loss: 0.32593 - acc: 0.9750 -- iter: 26/26\n",
            "--\n",
            "Training Step: 918  | total loss: \u001b[1m\u001b[32m0.63883\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 918 | loss: 0.63883 - acc: 0.8929 -- iter: 26/26\n",
            "--\n",
            "Training Step: 919  | total loss: \u001b[1m\u001b[32m0.59650\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 919 | loss: 0.59650 - acc: 0.9036 -- iter: 26/26\n",
            "--\n",
            "Training Step: 920  | total loss: \u001b[1m\u001b[32m0.55828\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 920 | loss: 0.55828 - acc: 0.9132 -- iter: 26/26\n",
            "--\n",
            "Training Step: 921  | total loss: \u001b[1m\u001b[32m0.52374\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 921 | loss: 0.52374 - acc: 0.9219 -- iter: 26/26\n",
            "--\n",
            "Training Step: 922  | total loss: \u001b[1m\u001b[32m0.49251\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 922 | loss: 0.49251 - acc: 0.9297 -- iter: 26/26\n",
            "--\n",
            "Training Step: 923  | total loss: \u001b[1m\u001b[32m0.46424\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 923 | loss: 0.46424 - acc: 0.9367 -- iter: 26/26\n",
            "--\n",
            "Training Step: 924  | total loss: \u001b[1m\u001b[32m0.43863\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 924 | loss: 0.43863 - acc: 0.9431 -- iter: 26/26\n",
            "--\n",
            "Training Step: 925  | total loss: \u001b[1m\u001b[32m0.41540\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 925 | loss: 0.41540 - acc: 0.9488 -- iter: 26/26\n",
            "--\n",
            "Training Step: 926  | total loss: \u001b[1m\u001b[32m0.39431\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 926 | loss: 0.39431 - acc: 0.9539 -- iter: 26/26\n",
            "--\n",
            "Training Step: 927  | total loss: \u001b[1m\u001b[32m0.37515\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 927 | loss: 0.37515 - acc: 0.9585 -- iter: 26/26\n",
            "--\n",
            "Training Step: 928  | total loss: \u001b[1m\u001b[32m0.35772\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 928 | loss: 0.35772 - acc: 0.9626 -- iter: 26/26\n",
            "--\n",
            "Training Step: 929  | total loss: \u001b[1m\u001b[32m0.34183\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 929 | loss: 0.34183 - acc: 0.9664 -- iter: 26/26\n",
            "--\n",
            "Training Step: 930  | total loss: \u001b[1m\u001b[32m0.32735\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 930 | loss: 0.32735 - acc: 0.9697 -- iter: 26/26\n",
            "--\n",
            "Training Step: 931  | total loss: \u001b[1m\u001b[32m0.31412\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 931 | loss: 0.31412 - acc: 0.9728 -- iter: 26/26\n",
            "--\n",
            "Training Step: 932  | total loss: \u001b[1m\u001b[32m0.30202\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 932 | loss: 0.30202 - acc: 0.9755 -- iter: 26/26\n",
            "--\n",
            "Training Step: 933  | total loss: \u001b[1m\u001b[32m0.29094\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 933 | loss: 0.29094 - acc: 0.9779 -- iter: 26/26\n",
            "--\n",
            "Training Step: 934  | total loss: \u001b[1m\u001b[32m0.28077\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 934 | loss: 0.28077 - acc: 0.9801 -- iter: 26/26\n",
            "--\n",
            "Training Step: 935  | total loss: \u001b[1m\u001b[32m0.27141\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 935 | loss: 0.27141 - acc: 0.9821 -- iter: 26/26\n",
            "--\n",
            "Training Step: 936  | total loss: \u001b[1m\u001b[32m0.26280\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 936 | loss: 0.26280 - acc: 0.9839 -- iter: 26/26\n",
            "--\n",
            "Training Step: 937  | total loss: \u001b[1m\u001b[32m0.25485\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 937 | loss: 0.25485 - acc: 0.9855 -- iter: 26/26\n",
            "--\n",
            "Training Step: 938  | total loss: \u001b[1m\u001b[32m0.24750\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 938 | loss: 0.24750 - acc: 0.9870 -- iter: 26/26\n",
            "--\n",
            "Training Step: 939  | total loss: \u001b[1m\u001b[32m0.24069\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 939 | loss: 0.24069 - acc: 0.9883 -- iter: 26/26\n",
            "--\n",
            "Training Step: 940  | total loss: \u001b[1m\u001b[32m0.23436\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 940 | loss: 0.23436 - acc: 0.9895 -- iter: 26/26\n",
            "--\n",
            "Training Step: 941  | total loss: \u001b[1m\u001b[32m0.22848\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 941 | loss: 0.22848 - acc: 0.9905 -- iter: 26/26\n",
            "--\n",
            "Training Step: 942  | total loss: \u001b[1m\u001b[32m0.22300\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 942 | loss: 0.22300 - acc: 0.9915 -- iter: 26/26\n",
            "--\n",
            "Training Step: 943  | total loss: \u001b[1m\u001b[32m0.21788\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 943 | loss: 0.21788 - acc: 0.9923 -- iter: 26/26\n",
            "--\n",
            "Training Step: 944  | total loss: \u001b[1m\u001b[32m0.21308\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 944 | loss: 0.21308 - acc: 0.9931 -- iter: 26/26\n",
            "--\n",
            "Training Step: 945  | total loss: \u001b[1m\u001b[32m0.20858\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 945 | loss: 0.20858 - acc: 0.9938 -- iter: 26/26\n",
            "--\n",
            "Training Step: 946  | total loss: \u001b[1m\u001b[32m0.20434\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 946 | loss: 0.20434 - acc: 0.9944 -- iter: 26/26\n",
            "--\n",
            "Training Step: 947  | total loss: \u001b[1m\u001b[32m0.20036\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 947 | loss: 0.20036 - acc: 0.9950 -- iter: 26/26\n",
            "--\n",
            "Training Step: 948  | total loss: \u001b[1m\u001b[32m0.19659\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 948 | loss: 0.19659 - acc: 0.9955 -- iter: 26/26\n",
            "--\n",
            "Training Step: 949  | total loss: \u001b[1m\u001b[32m0.19302\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 949 | loss: 0.19302 - acc: 0.9959 -- iter: 26/26\n",
            "--\n",
            "Training Step: 950  | total loss: \u001b[1m\u001b[32m0.18964\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 950 | loss: 0.18964 - acc: 0.9963 -- iter: 26/26\n",
            "--\n",
            "Training Step: 951  | total loss: \u001b[1m\u001b[32m0.18642\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 951 | loss: 0.18642 - acc: 0.9967 -- iter: 26/26\n",
            "--\n",
            "Training Step: 952  | total loss: \u001b[1m\u001b[32m0.18336\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 952 | loss: 0.18336 - acc: 0.9970 -- iter: 26/26\n",
            "--\n",
            "Training Step: 953  | total loss: \u001b[1m\u001b[32m0.18045\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 953 | loss: 0.18045 - acc: 0.9973 -- iter: 26/26\n",
            "--\n",
            "Training Step: 954  | total loss: \u001b[1m\u001b[32m0.17766\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 954 | loss: 0.17766 - acc: 0.9976 -- iter: 26/26\n",
            "--\n",
            "Training Step: 955  | total loss: \u001b[1m\u001b[32m0.17499\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 955 | loss: 0.17499 - acc: 0.9978 -- iter: 26/26\n",
            "--\n",
            "Training Step: 956  | total loss: \u001b[1m\u001b[32m0.45002\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 956 | loss: 0.45002 - acc: 0.9211 -- iter: 26/26\n",
            "--\n",
            "Training Step: 957  | total loss: \u001b[1m\u001b[32m0.41991\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 957 | loss: 0.41991 - acc: 0.9290 -- iter: 26/26\n",
            "--\n",
            "Training Step: 958  | total loss: \u001b[1m\u001b[32m0.39274\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 958 | loss: 0.39274 - acc: 0.9361 -- iter: 26/26\n",
            "--\n",
            "Training Step: 959  | total loss: \u001b[1m\u001b[32m0.36821\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 959 | loss: 0.36821 - acc: 0.9425 -- iter: 26/26\n",
            "--\n",
            "Training Step: 960  | total loss: \u001b[1m\u001b[32m0.34606\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 960 | loss: 0.34606 - acc: 0.9482 -- iter: 26/26\n",
            "--\n",
            "Training Step: 961  | total loss: \u001b[1m\u001b[32m0.32605\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 961 | loss: 0.32605 - acc: 0.9534 -- iter: 26/26\n",
            "--\n",
            "Training Step: 962  | total loss: \u001b[1m\u001b[32m0.30794\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 962 | loss: 0.30794 - acc: 0.9581 -- iter: 26/26\n",
            "--\n",
            "Training Step: 963  | total loss: \u001b[1m\u001b[32m0.29156\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 963 | loss: 0.29156 - acc: 0.9623 -- iter: 26/26\n",
            "--\n",
            "Training Step: 964  | total loss: \u001b[1m\u001b[32m0.27671\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 964 | loss: 0.27671 - acc: 0.9660 -- iter: 26/26\n",
            "--\n",
            "Training Step: 965  | total loss: \u001b[1m\u001b[32m0.26325\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 965 | loss: 0.26325 - acc: 0.9694 -- iter: 26/26\n",
            "--\n",
            "Training Step: 966  | total loss: \u001b[1m\u001b[32m0.25103\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 966 | loss: 0.25103 - acc: 0.9725 -- iter: 26/26\n",
            "--\n",
            "Training Step: 967  | total loss: \u001b[1m\u001b[32m0.23993\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 967 | loss: 0.23993 - acc: 0.9752 -- iter: 26/26\n",
            "--\n",
            "Training Step: 968  | total loss: \u001b[1m\u001b[32m0.22983\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 968 | loss: 0.22983 - acc: 0.9777 -- iter: 26/26\n",
            "--\n",
            "Training Step: 969  | total loss: \u001b[1m\u001b[32m0.22063\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 969 | loss: 0.22063 - acc: 0.9800 -- iter: 26/26\n",
            "--\n",
            "Training Step: 970  | total loss: \u001b[1m\u001b[32m0.21225\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 970 | loss: 0.21225 - acc: 0.9820 -- iter: 26/26\n",
            "--\n",
            "Training Step: 971  | total loss: \u001b[1m\u001b[32m0.20459\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 971 | loss: 0.20459 - acc: 0.9838 -- iter: 26/26\n",
            "--\n",
            "Training Step: 972  | total loss: \u001b[1m\u001b[32m0.19758\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 972 | loss: 0.19758 - acc: 0.9854 -- iter: 26/26\n",
            "--\n",
            "Training Step: 973  | total loss: \u001b[1m\u001b[32m0.19116\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 973 | loss: 0.19116 - acc: 0.9868 -- iter: 26/26\n",
            "--\n",
            "Training Step: 974  | total loss: \u001b[1m\u001b[32m0.18528\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 974 | loss: 0.18528 - acc: 0.9882 -- iter: 26/26\n",
            "--\n",
            "Training Step: 975  | total loss: \u001b[1m\u001b[32m0.17987\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 975 | loss: 0.17987 - acc: 0.9893 -- iter: 26/26\n",
            "--\n",
            "Training Step: 976  | total loss: \u001b[1m\u001b[32m0.17489\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 976 | loss: 0.17489 - acc: 0.9904 -- iter: 26/26\n",
            "--\n",
            "Training Step: 977  | total loss: \u001b[1m\u001b[32m0.17030\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 977 | loss: 0.17030 - acc: 0.9914 -- iter: 26/26\n",
            "--\n",
            "Training Step: 978  | total loss: \u001b[1m\u001b[32m0.16606\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 978 | loss: 0.16606 - acc: 0.9922 -- iter: 26/26\n",
            "--\n",
            "Training Step: 979  | total loss: \u001b[1m\u001b[32m0.16213\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 979 | loss: 0.16213 - acc: 0.9930 -- iter: 26/26\n",
            "--\n",
            "Training Step: 980  | total loss: \u001b[1m\u001b[32m0.15848\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 980 | loss: 0.15848 - acc: 0.9937 -- iter: 26/26\n",
            "--\n",
            "Training Step: 981  | total loss: \u001b[1m\u001b[32m0.15509\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 981 | loss: 0.15509 - acc: 0.9943 -- iter: 26/26\n",
            "--\n",
            "Training Step: 982  | total loss: \u001b[1m\u001b[32m0.15193\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 982 | loss: 0.15193 - acc: 0.9949 -- iter: 26/26\n",
            "--\n",
            "Training Step: 983  | total loss: \u001b[1m\u001b[32m0.14898\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 983 | loss: 0.14898 - acc: 0.9954 -- iter: 26/26\n",
            "--\n",
            "Training Step: 984  | total loss: \u001b[1m\u001b[32m0.14622\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 984 | loss: 0.14622 - acc: 0.9959 -- iter: 26/26\n",
            "--\n",
            "Training Step: 985  | total loss: \u001b[1m\u001b[32m0.14363\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 985 | loss: 0.14363 - acc: 0.9963 -- iter: 26/26\n",
            "--\n",
            "Training Step: 986  | total loss: \u001b[1m\u001b[32m0.52734\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 986 | loss: 0.52734 - acc: 0.9197 -- iter: 26/26\n",
            "--\n",
            "Training Step: 987  | total loss: \u001b[1m\u001b[32m0.48653\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 987 | loss: 0.48653 - acc: 0.9278 -- iter: 26/26\n",
            "--\n",
            "Training Step: 988  | total loss: \u001b[1m\u001b[32m0.44981\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 988 | loss: 0.44981 - acc: 0.9350 -- iter: 26/26\n",
            "--\n",
            "Training Step: 989  | total loss: \u001b[1m\u001b[32m0.41674\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 989 | loss: 0.41674 - acc: 0.9415 -- iter: 26/26\n",
            "--\n",
            "Training Step: 990  | total loss: \u001b[1m\u001b[32m0.38696\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 990 | loss: 0.38696 - acc: 0.9473 -- iter: 26/26\n",
            "--\n",
            "Training Step: 991  | total loss: \u001b[1m\u001b[32m0.36013\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 991 | loss: 0.36013 - acc: 0.9526 -- iter: 26/26\n",
            "--\n",
            "Training Step: 992  | total loss: \u001b[1m\u001b[32m0.33594\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 992 | loss: 0.33594 - acc: 0.9573 -- iter: 26/26\n",
            "--\n",
            "Training Step: 993  | total loss: \u001b[1m\u001b[32m0.31413\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 993 | loss: 0.31413 - acc: 0.9616 -- iter: 26/26\n",
            "--\n",
            "Training Step: 994  | total loss: \u001b[1m\u001b[32m0.29446\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 994 | loss: 0.29446 - acc: 0.9654 -- iter: 26/26\n",
            "--\n",
            "Training Step: 995  | total loss: \u001b[1m\u001b[32m0.27669\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 995 | loss: 0.27669 - acc: 0.9689 -- iter: 26/26\n",
            "--\n",
            "Training Step: 996  | total loss: \u001b[1m\u001b[32m0.26065\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 996 | loss: 0.26065 - acc: 0.9720 -- iter: 26/26\n",
            "--\n",
            "Training Step: 997  | total loss: \u001b[1m\u001b[32m0.24615\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 997 | loss: 0.24615 - acc: 0.9748 -- iter: 26/26\n",
            "--\n",
            "Training Step: 998  | total loss: \u001b[1m\u001b[32m0.23304\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 998 | loss: 0.23304 - acc: 0.9773 -- iter: 26/26\n",
            "--\n",
            "Training Step: 999  | total loss: \u001b[1m\u001b[32m0.22117\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 999 | loss: 0.22117 - acc: 0.9796 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1000  | total loss: \u001b[1m\u001b[32m0.21042\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1000 | loss: 0.21042 - acc: 0.9816 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1001  | total loss: \u001b[1m\u001b[32m0.20068\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 1001 | loss: 0.20068 - acc: 0.9835 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1002  | total loss: \u001b[1m\u001b[32m0.19184\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1002 | loss: 0.19184 - acc: 0.9851 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1003  | total loss: \u001b[1m\u001b[32m0.18381\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 1003 | loss: 0.18381 - acc: 0.9866 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1004  | total loss: \u001b[1m\u001b[32m0.17650\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 1004 | loss: 0.17650 - acc: 0.9880 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1005  | total loss: \u001b[1m\u001b[32m0.16986\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1005 | loss: 0.16986 - acc: 0.9892 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1006  | total loss: \u001b[1m\u001b[32m0.16380\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1006 | loss: 0.16380 - acc: 0.9902 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1007  | total loss: \u001b[1m\u001b[32m0.15827\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1007 | loss: 0.15827 - acc: 0.9912 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1008  | total loss: \u001b[1m\u001b[32m0.15322\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1008 | loss: 0.15322 - acc: 0.9921 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1009  | total loss: \u001b[1m\u001b[32m0.14859\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1009 | loss: 0.14859 - acc: 0.9929 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1010  | total loss: \u001b[1m\u001b[32m0.14435\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1010 | loss: 0.14435 - acc: 0.9936 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1011  | total loss: \u001b[1m\u001b[32m0.14046\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1011 | loss: 0.14046 - acc: 0.9942 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1012  | total loss: \u001b[1m\u001b[32m0.13688\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1012 | loss: 0.13688 - acc: 0.9948 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1013  | total loss: \u001b[1m\u001b[32m0.13358\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1013 | loss: 0.13358 - acc: 0.9953 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1014  | total loss: \u001b[1m\u001b[32m0.56400\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 1014 | loss: 0.56400 - acc: 0.8958 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1015  | total loss: \u001b[1m\u001b[32m0.51794\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1015 | loss: 0.51794 - acc: 0.9062 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1016  | total loss: \u001b[1m\u001b[32m0.47651\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 1016 | loss: 0.47651 - acc: 0.9156 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1017  | total loss: \u001b[1m\u001b[32m0.43923\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1017 | loss: 0.43923 - acc: 0.9240 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1018  | total loss: \u001b[1m\u001b[32m0.40568\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1018 | loss: 0.40568 - acc: 0.9316 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1019  | total loss: \u001b[1m\u001b[32m0.37549\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1019 | loss: 0.37549 - acc: 0.9385 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1020  | total loss: \u001b[1m\u001b[32m0.34830\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 1020 | loss: 0.34830 - acc: 0.9446 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1021  | total loss: \u001b[1m\u001b[32m0.32381\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 1021 | loss: 0.32381 - acc: 0.9502 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1022  | total loss: \u001b[1m\u001b[32m0.30175\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1022 | loss: 0.30175 - acc: 0.9551 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1023  | total loss: \u001b[1m\u001b[32m0.28186\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 1023 | loss: 0.28186 - acc: 0.9596 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1024  | total loss: \u001b[1m\u001b[32m0.26393\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1024 | loss: 0.26393 - acc: 0.9637 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1025  | total loss: \u001b[1m\u001b[32m0.24775\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 1025 | loss: 0.24775 - acc: 0.9673 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1026  | total loss: \u001b[1m\u001b[32m0.23315\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 1026 | loss: 0.23315 - acc: 0.9706 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1027  | total loss: \u001b[1m\u001b[32m0.21996\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1027 | loss: 0.21996 - acc: 0.9735 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1028  | total loss: \u001b[1m\u001b[32m0.20804\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1028 | loss: 0.20804 - acc: 0.9762 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1029  | total loss: \u001b[1m\u001b[32m0.19726\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 1029 | loss: 0.19726 - acc: 0.9785 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1030  | total loss: \u001b[1m\u001b[32m0.18750\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 1030 | loss: 0.18750 - acc: 0.9807 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1031  | total loss: \u001b[1m\u001b[32m0.17867\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1031 | loss: 0.17867 - acc: 0.9826 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1032  | total loss: \u001b[1m\u001b[32m0.17066\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 1032 | loss: 0.17066 - acc: 0.9844 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1033  | total loss: \u001b[1m\u001b[32m0.16339\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 1033 | loss: 0.16339 - acc: 0.9859 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1034  | total loss: \u001b[1m\u001b[32m0.15679\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1034 | loss: 0.15679 - acc: 0.9873 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1035  | total loss: \u001b[1m\u001b[32m0.15079\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1035 | loss: 0.15079 - acc: 0.9886 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1036  | total loss: \u001b[1m\u001b[32m0.14533\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 1036 | loss: 0.14533 - acc: 0.9897 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1037  | total loss: \u001b[1m\u001b[32m0.14035\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1037 | loss: 0.14035 - acc: 0.9908 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1038  | total loss: \u001b[1m\u001b[32m0.13580\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1038 | loss: 0.13580 - acc: 0.9917 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1039  | total loss: \u001b[1m\u001b[32m0.13165\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1039 | loss: 0.13165 - acc: 0.9925 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1040  | total loss: \u001b[1m\u001b[32m0.12785\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1040 | loss: 0.12785 - acc: 0.9933 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1041  | total loss: \u001b[1m\u001b[32m0.12436\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 1041 | loss: 0.12436 - acc: 0.9939 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1042  | total loss: \u001b[1m\u001b[32m0.12116\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1042 | loss: 0.12116 - acc: 0.9945 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1043  | total loss: \u001b[1m\u001b[32m0.11822\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 1043 | loss: 0.11822 - acc: 0.9951 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1044  | total loss: \u001b[1m\u001b[32m0.11550\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1044 | loss: 0.11550 - acc: 0.9956 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1045  | total loss: \u001b[1m\u001b[32m0.11300\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 1045 | loss: 0.11300 - acc: 0.9960 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1046  | total loss: \u001b[1m\u001b[32m0.11068\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1046 | loss: 0.11068 - acc: 0.9964 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1047  | total loss: \u001b[1m\u001b[32m0.10853\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1047 | loss: 0.10853 - acc: 0.9968 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1048  | total loss: \u001b[1m\u001b[32m0.10653\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1048 | loss: 0.10653 - acc: 0.9971 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1049  | total loss: \u001b[1m\u001b[32m0.10467\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 1049 | loss: 0.10467 - acc: 0.9974 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1050  | total loss: \u001b[1m\u001b[32m0.10294\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1050 | loss: 0.10294 - acc: 0.9977 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1051  | total loss: \u001b[1m\u001b[32m0.10132\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1051 | loss: 0.10132 - acc: 0.9979 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1052  | total loss: \u001b[1m\u001b[32m0.09979\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 1052 | loss: 0.09979 - acc: 0.9981 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1053  | total loss: \u001b[1m\u001b[32m0.09836\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1053 | loss: 0.09836 - acc: 0.9983 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1054  | total loss: \u001b[1m\u001b[32m0.09702\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1054 | loss: 0.09702 - acc: 0.9985 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1055  | total loss: \u001b[1m\u001b[32m0.09574\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 1055 | loss: 0.09574 - acc: 0.9986 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1056  | total loss: \u001b[1m\u001b[32m0.09454\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1056 | loss: 0.09454 - acc: 0.9988 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1057  | total loss: \u001b[1m\u001b[32m0.09340\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1057 | loss: 0.09340 - acc: 0.9989 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1058  | total loss: \u001b[1m\u001b[32m0.09231\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1058 | loss: 0.09231 - acc: 0.9990 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1059  | total loss: \u001b[1m\u001b[32m0.09128\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1059 | loss: 0.09128 - acc: 0.9991 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1060  | total loss: \u001b[1m\u001b[32m0.47413\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 1060 | loss: 0.47413 - acc: 0.9223 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1061  | total loss: \u001b[1m\u001b[32m0.43488\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 1061 | loss: 0.43488 - acc: 0.9300 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1062  | total loss: \u001b[1m\u001b[32m0.39958\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1062 | loss: 0.39958 - acc: 0.9370 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1063  | total loss: \u001b[1m\u001b[32m0.36782\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 1063 | loss: 0.36782 - acc: 0.9433 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1064  | total loss: \u001b[1m\u001b[32m0.33924\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1064 | loss: 0.33924 - acc: 0.9490 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1065  | total loss: \u001b[1m\u001b[32m0.31352\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 1065 | loss: 0.31352 - acc: 0.9541 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1066  | total loss: \u001b[1m\u001b[32m0.29037\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1066 | loss: 0.29037 - acc: 0.9587 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1067  | total loss: \u001b[1m\u001b[32m0.26953\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1067 | loss: 0.26953 - acc: 0.9628 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1068  | total loss: \u001b[1m\u001b[32m0.25075\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1068 | loss: 0.25075 - acc: 0.9665 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1069  | total loss: \u001b[1m\u001b[32m0.23383\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 1069 | loss: 0.23383 - acc: 0.9699 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1070  | total loss: \u001b[1m\u001b[32m0.21859\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1070 | loss: 0.21859 - acc: 0.9729 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1071  | total loss: \u001b[1m\u001b[32m0.20484\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1071 | loss: 0.20484 - acc: 0.9756 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1072  | total loss: \u001b[1m\u001b[32m0.19243\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 1072 | loss: 0.19243 - acc: 0.9780 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1073  | total loss: \u001b[1m\u001b[32m0.18124\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1073 | loss: 0.18124 - acc: 0.9802 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1074  | total loss: \u001b[1m\u001b[32m0.17113\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 1074 | loss: 0.17113 - acc: 0.9822 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1075  | total loss: \u001b[1m\u001b[32m0.16200\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 1075 | loss: 0.16200 - acc: 0.9840 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1076  | total loss: \u001b[1m\u001b[32m0.15374\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 1076 | loss: 0.15374 - acc: 0.9856 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1077  | total loss: \u001b[1m\u001b[32m0.14627\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 1077 | loss: 0.14627 - acc: 0.9870 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1078  | total loss: \u001b[1m\u001b[32m0.13950\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 1078 | loss: 0.13950 - acc: 0.9883 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1079  | total loss: \u001b[1m\u001b[32m0.13337\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1079 | loss: 0.13337 - acc: 0.9895 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1080  | total loss: \u001b[1m\u001b[32m0.12781\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 1080 | loss: 0.12781 - acc: 0.9905 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1081  | total loss: \u001b[1m\u001b[32m0.55623\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 1081 | loss: 0.55623 - acc: 0.9039 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1082  | total loss: \u001b[1m\u001b[32m0.55623\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1082 | loss: 0.55623 - acc: 0.9039 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1083  | total loss: \u001b[1m\u001b[32m0.46527\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1083 | loss: 0.46527 - acc: 0.9135 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1084  | total loss: \u001b[1m\u001b[32m0.46527\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 1084 | loss: 0.46527 - acc: 0.9221 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1085  | total loss: \u001b[1m\u001b[32m0.42654\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1085 | loss: 0.42654 - acc: 0.9299 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1086  | total loss: \u001b[1m\u001b[32m0.39169\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 1086 | loss: 0.39169 - acc: 0.9369 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1087  | total loss: \u001b[1m\u001b[32m0.36034\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1087 | loss: 0.36034 - acc: 0.9432 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1088  | total loss: \u001b[1m\u001b[32m0.33213\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1088 | loss: 0.33213 - acc: 0.9489 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1089  | total loss: \u001b[1m\u001b[32m0.30674\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1089 | loss: 0.30674 - acc: 0.9540 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1090  | total loss: \u001b[1m\u001b[32m0.28388\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1090 | loss: 0.28388 - acc: 0.9586 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1091  | total loss: \u001b[1m\u001b[32m0.26330\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1091 | loss: 0.26330 - acc: 0.9628 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1092  | total loss: \u001b[1m\u001b[32m0.24477\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 1092 | loss: 0.24477 - acc: 0.9665 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1093  | total loss: \u001b[1m\u001b[32m0.22806\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1093 | loss: 0.22806 - acc: 0.9698 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1094  | total loss: \u001b[1m\u001b[32m0.21301\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 1094 | loss: 0.21301 - acc: 0.9729 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1095  | total loss: \u001b[1m\u001b[32m0.19944\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1095 | loss: 0.19944 - acc: 0.9756 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1096  | total loss: \u001b[1m\u001b[32m0.18719\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 1096 | loss: 0.18719 - acc: 0.9780 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1097  | total loss: \u001b[1m\u001b[32m0.17615\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1097 | loss: 0.17615 - acc: 0.9802 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1098  | total loss: \u001b[1m\u001b[32m0.16617\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 1098 | loss: 0.16617 - acc: 0.9822 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1099  | total loss: \u001b[1m\u001b[32m0.15716\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1099 | loss: 0.15716 - acc: 0.9840 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1100  | total loss: \u001b[1m\u001b[32m0.14901\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1100 | loss: 0.14901 - acc: 0.9856 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1101  | total loss: \u001b[1m\u001b[32m0.14164\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1101 | loss: 0.14164 - acc: 0.9870 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1102  | total loss: \u001b[1m\u001b[32m0.13497\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1102 | loss: 0.13497 - acc: 0.9883 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1103  | total loss: \u001b[1m\u001b[32m0.12893\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1103 | loss: 0.12893 - acc: 0.9895 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1104  | total loss: \u001b[1m\u001b[32m0.12345\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1104 | loss: 0.12345 - acc: 0.9905 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1105  | total loss: \u001b[1m\u001b[32m0.11848\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1105 | loss: 0.11848 - acc: 0.9915 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1106  | total loss: \u001b[1m\u001b[32m0.11396\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1106 | loss: 0.11396 - acc: 0.9923 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1107  | total loss: \u001b[1m\u001b[32m0.10986\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1107 | loss: 0.10986 - acc: 0.9931 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1108  | total loss: \u001b[1m\u001b[32m0.10612\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1108 | loss: 0.10612 - acc: 0.9938 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1109  | total loss: \u001b[1m\u001b[32m0.10272\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1109 | loss: 0.10272 - acc: 0.9944 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1110  | total loss: \u001b[1m\u001b[32m0.09961\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1110 | loss: 0.09961 - acc: 0.9950 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1111  | total loss: \u001b[1m\u001b[32m0.09677\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1111 | loss: 0.09677 - acc: 0.9955 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1112  | total loss: \u001b[1m\u001b[32m0.09417\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1112 | loss: 0.09417 - acc: 0.9959 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1113  | total loss: \u001b[1m\u001b[32m0.09178\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1113 | loss: 0.09178 - acc: 0.9963 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1114  | total loss: \u001b[1m\u001b[32m0.08960\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1114 | loss: 0.08960 - acc: 0.9967 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1115  | total loss: \u001b[1m\u001b[32m0.08759\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 1115 | loss: 0.08759 - acc: 0.9970 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1116  | total loss: \u001b[1m\u001b[32m0.08574\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1116 | loss: 0.08574 - acc: 0.9973 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1117  | total loss: \u001b[1m\u001b[32m0.08403\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1117 | loss: 0.08403 - acc: 0.9976 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1118  | total loss: \u001b[1m\u001b[32m0.08245\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1118 | loss: 0.08245 - acc: 0.9978 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1119  | total loss: \u001b[1m\u001b[32m0.08098\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1119 | loss: 0.08098 - acc: 0.9981 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1120  | total loss: \u001b[1m\u001b[32m0.07962\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1120 | loss: 0.07962 - acc: 0.9982 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1121  | total loss: \u001b[1m\u001b[32m0.07836\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 1121 | loss: 0.07836 - acc: 0.9984 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1122  | total loss: \u001b[1m\u001b[32m0.07718\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1122 | loss: 0.07718 - acc: 0.9986 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1123  | total loss: \u001b[1m\u001b[32m0.07608\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1123 | loss: 0.07608 - acc: 0.9987 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1124  | total loss: \u001b[1m\u001b[32m0.07504\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1124 | loss: 0.07504 - acc: 0.9988 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1125  | total loss: \u001b[1m\u001b[32m0.07407\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1125 | loss: 0.07407 - acc: 0.9990 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1126  | total loss: \u001b[1m\u001b[32m0.07316\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1126 | loss: 0.07316 - acc: 0.9991 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1127  | total loss: \u001b[1m\u001b[32m0.07230\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1127 | loss: 0.07230 - acc: 0.9992 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1128  | total loss: \u001b[1m\u001b[32m0.07148\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1128 | loss: 0.07148 - acc: 0.9992 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1129  | total loss: \u001b[1m\u001b[32m0.07071\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1129 | loss: 0.07071 - acc: 0.9993 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1130  | total loss: \u001b[1m\u001b[32m0.06998\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1130 | loss: 0.06998 - acc: 0.9994 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1131  | total loss: \u001b[1m\u001b[32m0.06928\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1131 | loss: 0.06928 - acc: 0.9994 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1132  | total loss: \u001b[1m\u001b[32m0.06861\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1132 | loss: 0.06861 - acc: 0.9995 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1133  | total loss: \u001b[1m\u001b[32m0.06798\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1133 | loss: 0.06798 - acc: 0.9996 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1134  | total loss: \u001b[1m\u001b[32m0.06736\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1134 | loss: 0.06736 - acc: 0.9996 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1135  | total loss: \u001b[1m\u001b[32m0.06678\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 1135 | loss: 0.06678 - acc: 0.9996 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1136  | total loss: \u001b[1m\u001b[32m0.06621\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1136 | loss: 0.06621 - acc: 0.9997 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1137  | total loss: \u001b[1m\u001b[32m0.06567\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 1137 | loss: 0.06567 - acc: 0.9997 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1138  | total loss: \u001b[1m\u001b[32m0.06514\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1138 | loss: 0.06514 - acc: 0.9997 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1139  | total loss: \u001b[1m\u001b[32m0.06463\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 1139 | loss: 0.06463 - acc: 0.9998 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1140  | total loss: \u001b[1m\u001b[32m0.06414\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1140 | loss: 0.06414 - acc: 0.9998 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1141  | total loss: \u001b[1m\u001b[32m0.06366\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1141 | loss: 0.06366 - acc: 0.9998 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1142  | total loss: \u001b[1m\u001b[32m0.06319\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1142 | loss: 0.06319 - acc: 0.9998 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1143  | total loss: \u001b[1m\u001b[32m0.06274\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 1143 | loss: 0.06274 - acc: 0.9998 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1144  | total loss: \u001b[1m\u001b[32m0.06230\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1144 | loss: 0.06230 - acc: 0.9999 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1145  | total loss: \u001b[1m\u001b[32m0.06187\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1145 | loss: 0.06187 - acc: 0.9999 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1146  | total loss: \u001b[1m\u001b[32m0.06145\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1146 | loss: 0.06145 - acc: 0.9999 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1147  | total loss: \u001b[1m\u001b[32m0.06103\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1147 | loss: 0.06103 - acc: 0.9999 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1148  | total loss: \u001b[1m\u001b[32m0.06063\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1148 | loss: 0.06063 - acc: 0.9999 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1149  | total loss: \u001b[1m\u001b[32m0.06023\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 1149 | loss: 0.06023 - acc: 0.9999 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1150  | total loss: \u001b[1m\u001b[32m0.05984\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1150 | loss: 0.05984 - acc: 0.9999 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1151  | total loss: \u001b[1m\u001b[32m0.05946\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1151 | loss: 0.05946 - acc: 0.9999 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1152  | total loss: \u001b[1m\u001b[32m0.05909\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1152 | loss: 0.05909 - acc: 0.9999 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1153  | total loss: \u001b[1m\u001b[32m0.05872\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 1153 | loss: 0.05872 - acc: 0.9999 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1154  | total loss: \u001b[1m\u001b[32m0.05836\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1154 | loss: 0.05836 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1155  | total loss: \u001b[1m\u001b[32m0.05800\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1155 | loss: 0.05800 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1156  | total loss: \u001b[1m\u001b[32m0.05765\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 1156 | loss: 0.05765 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1157  | total loss: \u001b[1m\u001b[32m0.05731\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 1157 | loss: 0.05731 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1158  | total loss: \u001b[1m\u001b[32m0.05697\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1158 | loss: 0.05697 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1159  | total loss: \u001b[1m\u001b[32m0.05663\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1159 | loss: 0.05663 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1160  | total loss: \u001b[1m\u001b[32m0.05630\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 1160 | loss: 0.05630 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1161  | total loss: \u001b[1m\u001b[32m0.05597\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1161 | loss: 0.05597 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1162  | total loss: \u001b[1m\u001b[32m0.05565\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 1162 | loss: 0.05565 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1163  | total loss: \u001b[1m\u001b[32m0.05533\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 1163 | loss: 0.05533 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1164  | total loss: \u001b[1m\u001b[32m0.05501\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 1164 | loss: 0.05501 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1165  | total loss: \u001b[1m\u001b[32m0.05470\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 1165 | loss: 0.05470 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1166  | total loss: \u001b[1m\u001b[32m0.05439\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1166 | loss: 0.05439 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1167  | total loss: \u001b[1m\u001b[32m0.05409\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1167 | loss: 0.05409 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1168  | total loss: \u001b[1m\u001b[32m0.05379\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1168 | loss: 0.05379 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1169  | total loss: \u001b[1m\u001b[32m0.05349\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1169 | loss: 0.05349 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1170  | total loss: \u001b[1m\u001b[32m0.05320\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1170 | loss: 0.05320 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1171  | total loss: \u001b[1m\u001b[32m0.05290\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1171 | loss: 0.05290 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1172  | total loss: \u001b[1m\u001b[32m0.05262\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 1172 | loss: 0.05262 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1173  | total loss: \u001b[1m\u001b[32m0.05233\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1173 | loss: 0.05233 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1174  | total loss: \u001b[1m\u001b[32m0.05205\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 1174 | loss: 0.05205 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1175  | total loss: \u001b[1m\u001b[32m0.05177\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1175 | loss: 0.05177 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1176  | total loss: \u001b[1m\u001b[32m0.05149\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1176 | loss: 0.05149 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1177  | total loss: \u001b[1m\u001b[32m0.05122\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 1177 | loss: 0.05122 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1178  | total loss: \u001b[1m\u001b[32m0.05094\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 1178 | loss: 0.05094 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1179  | total loss: \u001b[1m\u001b[32m0.05068\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1179 | loss: 0.05068 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1180  | total loss: \u001b[1m\u001b[32m0.05041\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1180 | loss: 0.05041 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1181  | total loss: \u001b[1m\u001b[32m0.05014\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 1181 | loss: 0.05014 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1182  | total loss: \u001b[1m\u001b[32m0.04988\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 1182 | loss: 0.04988 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1183  | total loss: \u001b[1m\u001b[32m0.04962\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 1183 | loss: 0.04962 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1184  | total loss: \u001b[1m\u001b[32m0.04937\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1184 | loss: 0.04937 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1185  | total loss: \u001b[1m\u001b[32m0.04911\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 1185 | loss: 0.04911 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1186  | total loss: \u001b[1m\u001b[32m0.04886\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 1186 | loss: 0.04886 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1187  | total loss: \u001b[1m\u001b[32m0.04861\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1187 | loss: 0.04861 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1188  | total loss: \u001b[1m\u001b[32m0.04836\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 1188 | loss: 0.04836 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1189  | total loss: \u001b[1m\u001b[32m0.04811\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1189 | loss: 0.04811 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1190  | total loss: \u001b[1m\u001b[32m0.04787\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1190 | loss: 0.04787 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1191  | total loss: \u001b[1m\u001b[32m0.04763\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 1191 | loss: 0.04763 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1192  | total loss: \u001b[1m\u001b[32m0.04739\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1192 | loss: 0.04739 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1193  | total loss: \u001b[1m\u001b[32m0.04715\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1193 | loss: 0.04715 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1194  | total loss: \u001b[1m\u001b[32m0.04691\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1194 | loss: 0.04691 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1195  | total loss: \u001b[1m\u001b[32m0.04668\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1195 | loss: 0.04668 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1196  | total loss: \u001b[1m\u001b[32m0.04645\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 1196 | loss: 0.04645 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1197  | total loss: \u001b[1m\u001b[32m0.04622\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1197 | loss: 0.04622 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1198  | total loss: \u001b[1m\u001b[32m0.04599\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1198 | loss: 0.04599 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1199  | total loss: \u001b[1m\u001b[32m0.04576\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 1199 | loss: 0.04576 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1200  | total loss: \u001b[1m\u001b[32m0.04554\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1200 | loss: 0.04554 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1201  | total loss: \u001b[1m\u001b[32m0.04531\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 1201 | loss: 0.04531 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1202  | total loss: \u001b[1m\u001b[32m0.04509\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1202 | loss: 0.04509 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1203  | total loss: \u001b[1m\u001b[32m0.04487\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1203 | loss: 0.04487 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1204  | total loss: \u001b[1m\u001b[32m0.04465\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 1204 | loss: 0.04465 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1205  | total loss: \u001b[1m\u001b[32m0.04444\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 1205 | loss: 0.04444 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1206  | total loss: \u001b[1m\u001b[32m0.04422\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1206 | loss: 0.04422 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1207  | total loss: \u001b[1m\u001b[32m0.04401\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1207 | loss: 0.04401 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1208  | total loss: \u001b[1m\u001b[32m0.04380\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 1208 | loss: 0.04380 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1209  | total loss: \u001b[1m\u001b[32m0.04359\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1209 | loss: 0.04359 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1210  | total loss: \u001b[1m\u001b[32m0.04338\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1210 | loss: 0.04338 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1211  | total loss: \u001b[1m\u001b[32m0.04317\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 1211 | loss: 0.04317 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1212  | total loss: \u001b[1m\u001b[32m0.04297\u001b[0m\u001b[0m | time: 0.012s\n",
            "| Adam | epoch: 1212 | loss: 0.04297 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1213  | total loss: \u001b[1m\u001b[32m0.04277\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 1213 | loss: 0.04277 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1214  | total loss: \u001b[1m\u001b[32m0.04256\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 1214 | loss: 0.04256 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1215  | total loss: \u001b[1m\u001b[32m0.04236\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 1215 | loss: 0.04236 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1216  | total loss: \u001b[1m\u001b[32m0.04216\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 1216 | loss: 0.04216 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1217  | total loss: \u001b[1m\u001b[32m0.04197\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1217 | loss: 0.04197 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1218  | total loss: \u001b[1m\u001b[32m0.04177\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1218 | loss: 0.04177 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1219  | total loss: \u001b[1m\u001b[32m0.04158\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1219 | loss: 0.04158 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1220  | total loss: \u001b[1m\u001b[32m0.04138\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1220 | loss: 0.04138 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1221  | total loss: \u001b[1m\u001b[32m0.04119\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1221 | loss: 0.04119 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1222  | total loss: \u001b[1m\u001b[32m0.04100\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1222 | loss: 0.04100 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1223  | total loss: \u001b[1m\u001b[32m0.04081\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1223 | loss: 0.04081 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1224  | total loss: \u001b[1m\u001b[32m0.04062\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1224 | loss: 0.04062 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1225  | total loss: \u001b[1m\u001b[32m0.04044\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1225 | loss: 0.04044 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1226  | total loss: \u001b[1m\u001b[32m0.04025\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1226 | loss: 0.04025 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1227  | total loss: \u001b[1m\u001b[32m0.04007\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1227 | loss: 0.04007 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1228  | total loss: \u001b[1m\u001b[32m0.03989\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1228 | loss: 0.03989 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1229  | total loss: \u001b[1m\u001b[32m0.03971\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1229 | loss: 0.03971 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1230  | total loss: \u001b[1m\u001b[32m0.03953\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1230 | loss: 0.03953 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1231  | total loss: \u001b[1m\u001b[32m0.03935\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 1231 | loss: 0.03935 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1232  | total loss: \u001b[1m\u001b[32m0.03917\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1232 | loss: 0.03917 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1233  | total loss: \u001b[1m\u001b[32m0.03900\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1233 | loss: 0.03900 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1234  | total loss: \u001b[1m\u001b[32m0.03882\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1234 | loss: 0.03882 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1235  | total loss: \u001b[1m\u001b[32m0.03865\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 1235 | loss: 0.03865 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1236  | total loss: \u001b[1m\u001b[32m0.03848\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1236 | loss: 0.03848 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1237  | total loss: \u001b[1m\u001b[32m0.03831\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1237 | loss: 0.03831 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1238  | total loss: \u001b[1m\u001b[32m0.03814\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1238 | loss: 0.03814 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1239  | total loss: \u001b[1m\u001b[32m0.03797\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1239 | loss: 0.03797 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1240  | total loss: \u001b[1m\u001b[32m0.03780\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1240 | loss: 0.03780 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1241  | total loss: \u001b[1m\u001b[32m0.03763\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1241 | loss: 0.03763 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1242  | total loss: \u001b[1m\u001b[32m0.03747\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1242 | loss: 0.03747 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1243  | total loss: \u001b[1m\u001b[32m0.03731\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 1243 | loss: 0.03731 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1244  | total loss: \u001b[1m\u001b[32m0.03714\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1244 | loss: 0.03714 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1245  | total loss: \u001b[1m\u001b[32m0.03698\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 1245 | loss: 0.03698 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1246  | total loss: \u001b[1m\u001b[32m0.03682\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1246 | loss: 0.03682 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1247  | total loss: \u001b[1m\u001b[32m0.03666\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1247 | loss: 0.03666 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1248  | total loss: \u001b[1m\u001b[32m0.03651\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 1248 | loss: 0.03651 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1249  | total loss: \u001b[1m\u001b[32m0.03635\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1249 | loss: 0.03635 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1250  | total loss: \u001b[1m\u001b[32m0.62552\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1250 | loss: 0.62552 - acc: 0.9077 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1251  | total loss: \u001b[1m\u001b[32m0.56649\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1251 | loss: 0.56649 - acc: 0.9169 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1252  | total loss: \u001b[1m\u001b[32m0.51340\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1252 | loss: 0.51340 - acc: 0.9252 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1253  | total loss: \u001b[1m\u001b[32m0.46566\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 1253 | loss: 0.46566 - acc: 0.9327 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1254  | total loss: \u001b[1m\u001b[32m0.42272\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1254 | loss: 0.42272 - acc: 0.9394 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1255  | total loss: \u001b[1m\u001b[32m0.38410\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1255 | loss: 0.38410 - acc: 0.9455 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1256  | total loss: \u001b[1m\u001b[32m0.34937\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1256 | loss: 0.34937 - acc: 0.9509 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1257  | total loss: \u001b[1m\u001b[32m0.31813\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1257 | loss: 0.31813 - acc: 0.9558 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1258  | total loss: \u001b[1m\u001b[32m0.29003\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1258 | loss: 0.29003 - acc: 0.9603 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1259  | total loss: \u001b[1m\u001b[32m0.26475\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 1259 | loss: 0.26475 - acc: 0.9642 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1260  | total loss: \u001b[1m\u001b[32m0.24201\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1260 | loss: 0.24201 - acc: 0.9678 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1261  | total loss: \u001b[1m\u001b[32m0.22155\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 1261 | loss: 0.22155 - acc: 0.9710 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1262  | total loss: \u001b[1m\u001b[32m0.20314\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 1262 | loss: 0.20314 - acc: 0.9739 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1263  | total loss: \u001b[1m\u001b[32m0.18658\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1263 | loss: 0.18658 - acc: 0.9765 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1264  | total loss: \u001b[1m\u001b[32m0.17167\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1264 | loss: 0.17167 - acc: 0.9789 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1265  | total loss: \u001b[1m\u001b[32m0.15825\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 1265 | loss: 0.15825 - acc: 0.9810 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1266  | total loss: \u001b[1m\u001b[32m0.14617\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1266 | loss: 0.14617 - acc: 0.9829 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1267  | total loss: \u001b[1m\u001b[32m0.13529\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1267 | loss: 0.13529 - acc: 0.9846 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1268  | total loss: \u001b[1m\u001b[32m0.12550\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1268 | loss: 0.12550 - acc: 0.9861 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1269  | total loss: \u001b[1m\u001b[32m0.11667\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1269 | loss: 0.11667 - acc: 0.9875 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1270  | total loss: \u001b[1m\u001b[32m0.10873\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1270 | loss: 0.10873 - acc: 0.9888 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1271  | total loss: \u001b[1m\u001b[32m0.10156\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1271 | loss: 0.10156 - acc: 0.9899 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1272  | total loss: \u001b[1m\u001b[32m0.09511\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1272 | loss: 0.09511 - acc: 0.9909 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1273  | total loss: \u001b[1m\u001b[32m0.08928\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 1273 | loss: 0.08928 - acc: 0.9918 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1274  | total loss: \u001b[1m\u001b[32m0.08403\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 1274 | loss: 0.08403 - acc: 0.9926 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1275  | total loss: \u001b[1m\u001b[32m0.07929\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 1275 | loss: 0.07929 - acc: 0.9934 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1276  | total loss: \u001b[1m\u001b[32m0.07502\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1276 | loss: 0.07502 - acc: 0.9940 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1277  | total loss: \u001b[1m\u001b[32m0.07115\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1277 | loss: 0.07115 - acc: 0.9946 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1278  | total loss: \u001b[1m\u001b[32m0.06767\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 1278 | loss: 0.06767 - acc: 0.9952 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1279  | total loss: \u001b[1m\u001b[32m0.06451\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1279 | loss: 0.06451 - acc: 0.9957 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1280  | total loss: \u001b[1m\u001b[32m0.06166\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1280 | loss: 0.06166 - acc: 0.9961 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1281  | total loss: \u001b[1m\u001b[32m0.05908\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1281 | loss: 0.05908 - acc: 0.9965 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1282  | total loss: \u001b[1m\u001b[32m0.05674\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 1282 | loss: 0.05674 - acc: 0.9968 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1283  | total loss: \u001b[1m\u001b[32m0.05462\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1283 | loss: 0.05462 - acc: 0.9971 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1284  | total loss: \u001b[1m\u001b[32m0.05270\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 1284 | loss: 0.05270 - acc: 0.9974 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1285  | total loss: \u001b[1m\u001b[32m0.05096\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1285 | loss: 0.05096 - acc: 0.9977 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1286  | total loss: \u001b[1m\u001b[32m0.04938\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 1286 | loss: 0.04938 - acc: 0.9979 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1287  | total loss: \u001b[1m\u001b[32m0.04794\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1287 | loss: 0.04794 - acc: 0.9981 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1288  | total loss: \u001b[1m\u001b[32m0.04663\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1288 | loss: 0.04663 - acc: 0.9983 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1289  | total loss: \u001b[1m\u001b[32m0.04543\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1289 | loss: 0.04543 - acc: 0.9985 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1290  | total loss: \u001b[1m\u001b[32m0.04434\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1290 | loss: 0.04434 - acc: 0.9986 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1291  | total loss: \u001b[1m\u001b[32m0.04335\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1291 | loss: 0.04335 - acc: 0.9988 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1292  | total loss: \u001b[1m\u001b[32m0.04244\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1292 | loss: 0.04244 - acc: 0.9989 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1293  | total loss: \u001b[1m\u001b[32m0.04161\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 1293 | loss: 0.04161 - acc: 0.9990 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1294  | total loss: \u001b[1m\u001b[32m0.04085\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1294 | loss: 0.04085 - acc: 0.9991 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1295  | total loss: \u001b[1m\u001b[32m0.04014\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1295 | loss: 0.04014 - acc: 0.9992 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1296  | total loss: \u001b[1m\u001b[32m0.03950\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 1296 | loss: 0.03950 - acc: 0.9993 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1297  | total loss: \u001b[1m\u001b[32m0.03890\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1297 | loss: 0.03890 - acc: 0.9993 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1298  | total loss: \u001b[1m\u001b[32m0.03836\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1298 | loss: 0.03836 - acc: 0.9994 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1299  | total loss: \u001b[1m\u001b[32m0.03785\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1299 | loss: 0.03785 - acc: 0.9995 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1300  | total loss: \u001b[1m\u001b[32m0.03738\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1300 | loss: 0.03738 - acc: 0.9995 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1301  | total loss: \u001b[1m\u001b[32m0.03694\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1301 | loss: 0.03694 - acc: 0.9996 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1302  | total loss: \u001b[1m\u001b[32m0.03653\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1302 | loss: 0.03653 - acc: 0.9996 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1303  | total loss: \u001b[1m\u001b[32m0.03615\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1303 | loss: 0.03615 - acc: 0.9997 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1304  | total loss: \u001b[1m\u001b[32m0.03579\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1304 | loss: 0.03579 - acc: 0.9997 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1305  | total loss: \u001b[1m\u001b[32m0.03546\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1305 | loss: 0.03546 - acc: 0.9997 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1306  | total loss: \u001b[1m\u001b[32m0.03515\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 1306 | loss: 0.03515 - acc: 0.9997 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1307  | total loss: \u001b[1m\u001b[32m0.03485\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1307 | loss: 0.03485 - acc: 0.9998 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1308  | total loss: \u001b[1m\u001b[32m0.03457\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1308 | loss: 0.03457 - acc: 0.9998 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1309  | total loss: \u001b[1m\u001b[32m0.03431\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1309 | loss: 0.03431 - acc: 0.9998 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1310  | total loss: \u001b[1m\u001b[32m0.03406\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1310 | loss: 0.03406 - acc: 0.9998 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1311  | total loss: \u001b[1m\u001b[32m0.03382\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1311 | loss: 0.03382 - acc: 0.9999 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1312  | total loss: \u001b[1m\u001b[32m0.03359\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1312 | loss: 0.03359 - acc: 0.9999 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1313  | total loss: \u001b[1m\u001b[32m0.03337\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1313 | loss: 0.03337 - acc: 0.9999 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1314  | total loss: \u001b[1m\u001b[32m0.03317\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1314 | loss: 0.03317 - acc: 0.9999 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1315  | total loss: \u001b[1m\u001b[32m0.03297\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1315 | loss: 0.03297 - acc: 0.9999 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1316  | total loss: \u001b[1m\u001b[32m0.03277\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1316 | loss: 0.03277 - acc: 0.9999 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1317  | total loss: \u001b[1m\u001b[32m0.03259\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1317 | loss: 0.03259 - acc: 0.9999 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1318  | total loss: \u001b[1m\u001b[32m0.03241\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1318 | loss: 0.03241 - acc: 0.9999 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1319  | total loss: \u001b[1m\u001b[32m0.03223\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1319 | loss: 0.03223 - acc: 0.9999 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1320  | total loss: \u001b[1m\u001b[32m0.03206\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1320 | loss: 0.03206 - acc: 0.9999 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1321  | total loss: \u001b[1m\u001b[32m0.03190\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1321 | loss: 0.03190 - acc: 0.9999 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1322  | total loss: \u001b[1m\u001b[32m0.03174\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1322 | loss: 0.03174 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1323  | total loss: \u001b[1m\u001b[32m0.03158\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1323 | loss: 0.03158 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1324  | total loss: \u001b[1m\u001b[32m0.03143\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1324 | loss: 0.03143 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1325  | total loss: \u001b[1m\u001b[32m0.03128\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1325 | loss: 0.03128 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1326  | total loss: \u001b[1m\u001b[32m0.03114\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1326 | loss: 0.03114 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1327  | total loss: \u001b[1m\u001b[32m0.03099\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 1327 | loss: 0.03099 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1328  | total loss: \u001b[1m\u001b[32m0.03085\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1328 | loss: 0.03085 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1329  | total loss: \u001b[1m\u001b[32m0.03071\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 1329 | loss: 0.03071 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1330  | total loss: \u001b[1m\u001b[32m0.03058\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1330 | loss: 0.03058 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1331  | total loss: \u001b[1m\u001b[32m0.03044\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1331 | loss: 0.03044 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1332  | total loss: \u001b[1m\u001b[32m0.03031\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1332 | loss: 0.03031 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1333  | total loss: \u001b[1m\u001b[32m0.03018\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 1333 | loss: 0.03018 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1334  | total loss: \u001b[1m\u001b[32m0.03006\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1334 | loss: 0.03006 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1335  | total loss: \u001b[1m\u001b[32m0.02993\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1335 | loss: 0.02993 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1336  | total loss: \u001b[1m\u001b[32m0.02980\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1336 | loss: 0.02980 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1337  | total loss: \u001b[1m\u001b[32m0.02968\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1337 | loss: 0.02968 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1338  | total loss: \u001b[1m\u001b[32m0.02956\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1338 | loss: 0.02956 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1339  | total loss: \u001b[1m\u001b[32m0.02944\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1339 | loss: 0.02944 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1340  | total loss: \u001b[1m\u001b[32m0.02932\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1340 | loss: 0.02932 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1341  | total loss: \u001b[1m\u001b[32m0.02920\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 1341 | loss: 0.02920 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1342  | total loss: \u001b[1m\u001b[32m0.02908\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1342 | loss: 0.02908 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1343  | total loss: \u001b[1m\u001b[32m0.02897\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1343 | loss: 0.02897 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1344  | total loss: \u001b[1m\u001b[32m0.02885\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1344 | loss: 0.02885 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1345  | total loss: \u001b[1m\u001b[32m0.02874\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1345 | loss: 0.02874 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1346  | total loss: \u001b[1m\u001b[32m0.02863\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 1346 | loss: 0.02863 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1347  | total loss: \u001b[1m\u001b[32m0.02851\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1347 | loss: 0.02851 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1348  | total loss: \u001b[1m\u001b[32m0.02840\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 1348 | loss: 0.02840 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1349  | total loss: \u001b[1m\u001b[32m0.02829\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 1349 | loss: 0.02829 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1350  | total loss: \u001b[1m\u001b[32m0.54980\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1350 | loss: 0.54980 - acc: 0.9192 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1351  | total loss: \u001b[1m\u001b[32m0.49757\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 1351 | loss: 0.49757 - acc: 0.9273 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1352  | total loss: \u001b[1m\u001b[32m0.45060\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1352 | loss: 0.45060 - acc: 0.9346 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1353  | total loss: \u001b[1m\u001b[32m0.40835\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 1353 | loss: 0.40835 - acc: 0.9411 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1354  | total loss: \u001b[1m\u001b[32m0.37034\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 1354 | loss: 0.37034 - acc: 0.9470 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1355  | total loss: \u001b[1m\u001b[32m0.33616\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 1355 | loss: 0.33616 - acc: 0.9523 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1356  | total loss: \u001b[1m\u001b[32m0.30542\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1356 | loss: 0.30542 - acc: 0.9571 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1357  | total loss: \u001b[1m\u001b[32m0.27776\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1357 | loss: 0.27776 - acc: 0.9614 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1358  | total loss: \u001b[1m\u001b[32m0.25288\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 1358 | loss: 0.25288 - acc: 0.9652 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1359  | total loss: \u001b[1m\u001b[32m0.23050\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1359 | loss: 0.23050 - acc: 0.9687 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1360  | total loss: \u001b[1m\u001b[32m0.21037\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1360 | loss: 0.21037 - acc: 0.9718 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1361  | total loss: \u001b[1m\u001b[32m0.19226\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1361 | loss: 0.19226 - acc: 0.9747 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1362  | total loss: \u001b[1m\u001b[32m0.17596\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 1362 | loss: 0.17596 - acc: 0.9772 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1363  | total loss: \u001b[1m\u001b[32m0.16129\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1363 | loss: 0.16129 - acc: 0.9795 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1364  | total loss: \u001b[1m\u001b[32m0.14809\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1364 | loss: 0.14809 - acc: 0.9815 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1365  | total loss: \u001b[1m\u001b[32m0.13622\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 1365 | loss: 0.13622 - acc: 0.9834 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1366  | total loss: \u001b[1m\u001b[32m0.12552\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1366 | loss: 0.12552 - acc: 0.9850 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1367  | total loss: \u001b[1m\u001b[32m0.11590\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1367 | loss: 0.11590 - acc: 0.9865 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1368  | total loss: \u001b[1m\u001b[32m0.10723\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1368 | loss: 0.10723 - acc: 0.9879 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1369  | total loss: \u001b[1m\u001b[32m0.09942\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 1369 | loss: 0.09942 - acc: 0.9891 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1370  | total loss: \u001b[1m\u001b[32m0.09239\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 1370 | loss: 0.09239 - acc: 0.9902 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1371  | total loss: \u001b[1m\u001b[32m0.08606\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1371 | loss: 0.08606 - acc: 0.9912 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1372  | total loss: \u001b[1m\u001b[32m0.08035\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 1372 | loss: 0.08035 - acc: 0.9920 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1373  | total loss: \u001b[1m\u001b[32m0.07521\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 1373 | loss: 0.07521 - acc: 0.9928 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1374  | total loss: \u001b[1m\u001b[32m0.07057\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 1374 | loss: 0.07057 - acc: 0.9936 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1375  | total loss: \u001b[1m\u001b[32m0.06639\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1375 | loss: 0.06639 - acc: 0.9942 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1376  | total loss: \u001b[1m\u001b[32m0.06262\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1376 | loss: 0.06262 - acc: 0.9948 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1377  | total loss: \u001b[1m\u001b[32m0.05922\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 1377 | loss: 0.05922 - acc: 0.9953 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1378  | total loss: \u001b[1m\u001b[32m0.05614\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 1378 | loss: 0.05614 - acc: 0.9958 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1379  | total loss: \u001b[1m\u001b[32m0.05337\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1379 | loss: 0.05337 - acc: 0.9962 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1380  | total loss: \u001b[1m\u001b[32m0.05086\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1380 | loss: 0.05086 - acc: 0.9966 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1381  | total loss: \u001b[1m\u001b[32m0.04860\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 1381 | loss: 0.04860 - acc: 0.9969 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1382  | total loss: \u001b[1m\u001b[32m0.04655\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1382 | loss: 0.04655 - acc: 0.9972 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1383  | total loss: \u001b[1m\u001b[32m0.04469\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1383 | loss: 0.04469 - acc: 0.9975 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1384  | total loss: \u001b[1m\u001b[32m0.04302\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1384 | loss: 0.04302 - acc: 0.9978 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1385  | total loss: \u001b[1m\u001b[32m0.04149\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 1385 | loss: 0.04149 - acc: 0.9980 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1386  | total loss: \u001b[1m\u001b[32m0.04012\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1386 | loss: 0.04012 - acc: 0.9982 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1387  | total loss: \u001b[1m\u001b[32m0.03886\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1387 | loss: 0.03886 - acc: 0.9984 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1388  | total loss: \u001b[1m\u001b[32m0.03773\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1388 | loss: 0.03773 - acc: 0.9985 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1389  | total loss: \u001b[1m\u001b[32m0.03670\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1389 | loss: 0.03670 - acc: 0.9987 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1390  | total loss: \u001b[1m\u001b[32m0.03576\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 1390 | loss: 0.03576 - acc: 0.9988 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1391  | total loss: \u001b[1m\u001b[32m0.03490\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1391 | loss: 0.03490 - acc: 0.9989 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1392  | total loss: \u001b[1m\u001b[32m0.03412\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1392 | loss: 0.03412 - acc: 0.9990 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1393  | total loss: \u001b[1m\u001b[32m0.03341\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 1393 | loss: 0.03341 - acc: 0.9991 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1394  | total loss: \u001b[1m\u001b[32m0.03275\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1394 | loss: 0.03275 - acc: 0.9992 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1395  | total loss: \u001b[1m\u001b[32m0.03216\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1395 | loss: 0.03216 - acc: 0.9993 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1396  | total loss: \u001b[1m\u001b[32m0.03161\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1396 | loss: 0.03161 - acc: 0.9994 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1397  | total loss: \u001b[1m\u001b[32m0.03111\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1397 | loss: 0.03111 - acc: 0.9994 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1398  | total loss: \u001b[1m\u001b[32m0.63924\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 1398 | loss: 0.63924 - acc: 0.9110 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1399  | total loss: \u001b[1m\u001b[32m0.57800\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1399 | loss: 0.57800 - acc: 0.9199 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1400  | total loss: \u001b[1m\u001b[32m0.52293\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1400 | loss: 0.52293 - acc: 0.9279 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1401  | total loss: \u001b[1m\u001b[32m0.47339\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1401 | loss: 0.47339 - acc: 0.9351 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1402  | total loss: \u001b[1m\u001b[32m1.00796\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 1402 | loss: 1.00796 - acc: 0.8647 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1403  | total loss: \u001b[1m\u001b[32m0.91003\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1403 | loss: 0.91003 - acc: 0.8782 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1404  | total loss: \u001b[1m\u001b[32m1.41355\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 1404 | loss: 1.41355 - acc: 0.7981 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1405  | total loss: \u001b[1m\u001b[32m1.27526\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 1405 | loss: 1.27526 - acc: 0.8183 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1406  | total loss: \u001b[1m\u001b[32m1.74851\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1406 | loss: 1.74851 - acc: 0.7442 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1407  | total loss: \u001b[1m\u001b[32m1.57702\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1407 | loss: 1.57702 - acc: 0.7697 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1408  | total loss: \u001b[1m\u001b[32m1.92236\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1408 | loss: 1.92236 - acc: 0.7158 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1409  | total loss: \u001b[1m\u001b[32m1.73387\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1409 | loss: 1.73387 - acc: 0.7443 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1410  | total loss: \u001b[1m\u001b[32m2.04823\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 1410 | loss: 2.04823 - acc: 0.6891 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1411  | total loss: \u001b[1m\u001b[32m1.84761\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 1411 | loss: 1.84761 - acc: 0.7202 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1412  | total loss: \u001b[1m\u001b[32m2.14621\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1412 | loss: 2.14621 - acc: 0.6674 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1413  | total loss: \u001b[1m\u001b[32m1.93634\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1413 | loss: 1.93634 - acc: 0.7006 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1414  | total loss: \u001b[1m\u001b[32m2.23428\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 1414 | loss: 2.23428 - acc: 0.6460 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1415  | total loss: \u001b[1m\u001b[32m2.01626\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 1415 | loss: 2.01626 - acc: 0.6814 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1416  | total loss: \u001b[1m\u001b[32m2.15108\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1416 | loss: 2.15108 - acc: 0.6440 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1417  | total loss: \u001b[1m\u001b[32m1.94211\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1417 | loss: 1.94211 - acc: 0.6796 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1418  | total loss: \u001b[1m\u001b[32m2.21043\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 1418 | loss: 2.21043 - acc: 0.6347 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1419  | total loss: \u001b[1m\u001b[32m1.99633\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 1419 | loss: 1.99633 - acc: 0.6712 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1420  | total loss: \u001b[1m\u001b[32m2.15466\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1420 | loss: 2.15466 - acc: 0.6349 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1421  | total loss: \u001b[1m\u001b[32m1.94707\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 1421 | loss: 1.94707 - acc: 0.6714 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1422  | total loss: \u001b[1m\u001b[32m2.10536\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1422 | loss: 2.10536 - acc: 0.6312 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1423  | total loss: \u001b[1m\u001b[32m1.90370\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 1423 | loss: 1.90370 - acc: 0.6681 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1424  | total loss: \u001b[1m\u001b[32m2.09639\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 1424 | loss: 2.09639 - acc: 0.6128 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1425  | total loss: \u001b[1m\u001b[32m1.89671\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1425 | loss: 1.89671 - acc: 0.6515 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1426  | total loss: \u001b[1m\u001b[32m2.10069\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1426 | loss: 2.10069 - acc: 0.6056 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1427  | total loss: \u001b[1m\u001b[32m1.90177\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1427 | loss: 1.90177 - acc: 0.6450 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1428  | total loss: \u001b[1m\u001b[32m2.14034\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1428 | loss: 2.14034 - acc: 0.5882 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1429  | total loss: \u001b[1m\u001b[32m1.93878\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1429 | loss: 1.93878 - acc: 0.6294 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1430  | total loss: \u001b[1m\u001b[32m2.10599\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1430 | loss: 2.10599 - acc: 0.5780 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1431  | total loss: \u001b[1m\u001b[32m1.90935\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1431 | loss: 1.90935 - acc: 0.6202 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1432  | total loss: \u001b[1m\u001b[32m2.10285\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 1432 | loss: 2.10285 - acc: 0.5774 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1433  | total loss: \u001b[1m\u001b[32m1.90809\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1433 | loss: 1.90809 - acc: 0.6197 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1434  | total loss: \u001b[1m\u001b[32m2.09734\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1434 | loss: 2.09734 - acc: 0.5808 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1435  | total loss: \u001b[1m\u001b[32m1.90474\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1435 | loss: 1.90474 - acc: 0.6227 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1436  | total loss: \u001b[1m\u001b[32m2.07269\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1436 | loss: 2.07269 - acc: 0.5720 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1437  | total loss: \u001b[1m\u001b[32m1.88424\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 1437 | loss: 1.88424 - acc: 0.6148 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1438  | total loss: \u001b[1m\u001b[32m1.71544\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1438 | loss: 1.71544 - acc: 0.6533 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1439  | total loss: \u001b[1m\u001b[32m1.56422\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1439 | loss: 1.56422 - acc: 0.6880 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1440  | total loss: \u001b[1m\u001b[32m1.71632\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1440 | loss: 1.71632 - acc: 0.6422 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1441  | total loss: \u001b[1m\u001b[32m1.56630\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 1441 | loss: 1.56630 - acc: 0.6780 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1442  | total loss: \u001b[1m\u001b[32m1.73779\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1442 | loss: 1.73779 - acc: 0.6256 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1443  | total loss: \u001b[1m\u001b[32m1.58696\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1443 | loss: 1.58696 - acc: 0.6630 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1444  | total loss: \u001b[1m\u001b[32m1.71110\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 1444 | loss: 1.71110 - acc: 0.6160 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1445  | total loss: \u001b[1m\u001b[32m1.56428\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1445 | loss: 1.56428 - acc: 0.6544 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1446  | total loss: \u001b[1m\u001b[32m1.71479\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1446 | loss: 1.71479 - acc: 0.6005 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1447  | total loss: \u001b[1m\u001b[32m1.56891\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1447 | loss: 1.56891 - acc: 0.6404 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1448  | total loss: \u001b[1m\u001b[32m1.71749\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1448 | loss: 1.71749 - acc: 0.5918 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1449  | total loss: \u001b[1m\u001b[32m1.57267\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 1449 | loss: 1.57267 - acc: 0.6326 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1450  | total loss: \u001b[1m\u001b[32m1.44293\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1450 | loss: 1.44293 - acc: 0.6693 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1451  | total loss: \u001b[1m\u001b[32m1.32662\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 1451 | loss: 1.32662 - acc: 0.7024 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1452  | total loss: \u001b[1m\u001b[32m1.22228\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1452 | loss: 1.22228 - acc: 0.7322 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1453  | total loss: \u001b[1m\u001b[32m1.12857\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1453 | loss: 1.12857 - acc: 0.7589 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1454  | total loss: \u001b[1m\u001b[32m1.04435\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 1454 | loss: 1.04435 - acc: 0.7830 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1455  | total loss: \u001b[1m\u001b[32m0.96857\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1455 | loss: 0.96857 - acc: 0.8047 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1456  | total loss: \u001b[1m\u001b[32m0.90029\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 1456 | loss: 0.90029 - acc: 0.8243 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1457  | total loss: \u001b[1m\u001b[32m0.83870\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 1457 | loss: 0.83870 - acc: 0.8418 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1458  | total loss: \u001b[1m\u001b[32m0.78308\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 1458 | loss: 0.78308 - acc: 0.8577 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1459  | total loss: \u001b[1m\u001b[32m0.73277\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1459 | loss: 0.73277 - acc: 0.8719 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1460  | total loss: \u001b[1m\u001b[32m0.68720\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 1460 | loss: 0.68720 - acc: 0.8847 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1461  | total loss: \u001b[1m\u001b[32m0.64586\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 1461 | loss: 0.64586 - acc: 0.8962 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1462  | total loss: \u001b[1m\u001b[32m0.60829\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 1462 | loss: 0.60829 - acc: 0.9066 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1463  | total loss: \u001b[1m\u001b[32m0.57410\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 1463 | loss: 0.57410 - acc: 0.9159 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1464  | total loss: \u001b[1m\u001b[32m0.54293\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 1464 | loss: 0.54293 - acc: 0.9244 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1465  | total loss: \u001b[1m\u001b[32m0.51446\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1465 | loss: 0.51446 - acc: 0.9319 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1466  | total loss: \u001b[1m\u001b[32m0.48843\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1466 | loss: 0.48843 - acc: 0.9387 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1467  | total loss: \u001b[1m\u001b[32m0.46456\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1467 | loss: 0.46456 - acc: 0.9449 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1468  | total loss: \u001b[1m\u001b[32m0.44266\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1468 | loss: 0.44266 - acc: 0.9504 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1469  | total loss: \u001b[1m\u001b[32m0.42251\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1469 | loss: 0.42251 - acc: 0.9553 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1470  | total loss: \u001b[1m\u001b[32m0.40394\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1470 | loss: 0.40394 - acc: 0.9598 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1471  | total loss: \u001b[1m\u001b[32m0.38680\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1471 | loss: 0.38680 - acc: 0.9638 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1472  | total loss: \u001b[1m\u001b[32m0.37095\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1472 | loss: 0.37095 - acc: 0.9674 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1473  | total loss: \u001b[1m\u001b[32m0.35626\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1473 | loss: 0.35626 - acc: 0.9707 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1474  | total loss: \u001b[1m\u001b[32m0.34263\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1474 | loss: 0.34263 - acc: 0.9736 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1475  | total loss: \u001b[1m\u001b[32m0.32995\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1475 | loss: 0.32995 - acc: 0.9763 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1476  | total loss: \u001b[1m\u001b[32m0.31813\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1476 | loss: 0.31813 - acc: 0.9786 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1477  | total loss: \u001b[1m\u001b[32m0.30711\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1477 | loss: 0.30711 - acc: 0.9808 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1478  | total loss: \u001b[1m\u001b[32m0.29680\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1478 | loss: 0.29680 - acc: 0.9827 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1479  | total loss: \u001b[1m\u001b[32m0.28715\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1479 | loss: 0.28715 - acc: 0.9844 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1480  | total loss: \u001b[1m\u001b[32m0.27809\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 1480 | loss: 0.27809 - acc: 0.9860 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1481  | total loss: \u001b[1m\u001b[32m0.26960\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1481 | loss: 0.26960 - acc: 0.9874 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1482  | total loss: \u001b[1m\u001b[32m0.26161\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 1482 | loss: 0.26161 - acc: 0.9886 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1483  | total loss: \u001b[1m\u001b[32m0.25409\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1483 | loss: 0.25409 - acc: 0.9898 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1484  | total loss: \u001b[1m\u001b[32m0.56094\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1484 | loss: 0.56094 - acc: 0.9100 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1485  | total loss: \u001b[1m\u001b[32m0.52304\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1485 | loss: 0.52304 - acc: 0.9190 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1486  | total loss: \u001b[1m\u001b[32m0.48878\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1486 | loss: 0.48878 - acc: 0.9271 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1487  | total loss: \u001b[1m\u001b[32m0.45779\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1487 | loss: 0.45779 - acc: 0.9344 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1488  | total loss: \u001b[1m\u001b[32m0.73645\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1488 | loss: 0.73645 - acc: 0.8564 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1489  | total loss: \u001b[1m\u001b[32m0.68054\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1489 | loss: 0.68054 - acc: 0.8707 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1490  | total loss: \u001b[1m\u001b[32m0.92297\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 1490 | loss: 0.92297 - acc: 0.8029 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1491  | total loss: \u001b[1m\u001b[32m0.84849\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 1491 | loss: 0.84849 - acc: 0.8226 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1492  | total loss: \u001b[1m\u001b[32m1.12062\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1492 | loss: 1.12062 - acc: 0.7519 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1493  | total loss: \u001b[1m\u001b[32m1.02670\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1493 | loss: 1.02670 - acc: 0.7767 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1494  | total loss: \u001b[1m\u001b[32m1.28817\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1494 | loss: 1.28817 - acc: 0.7106 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1495  | total loss: \u001b[1m\u001b[32m1.17803\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1495 | loss: 1.17803 - acc: 0.7395 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1496  | total loss: \u001b[1m\u001b[32m1.40362\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1496 | loss: 1.40362 - acc: 0.6809 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1497  | total loss: \u001b[1m\u001b[32m1.28267\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 1497 | loss: 1.28267 - acc: 0.7128 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1498  | total loss: \u001b[1m\u001b[32m1.46325\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 1498 | loss: 1.46325 - acc: 0.6608 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1499  | total loss: \u001b[1m\u001b[32m1.33719\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 1499 | loss: 1.33719 - acc: 0.6947 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1500  | total loss: \u001b[1m\u001b[32m1.49822\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1500 | loss: 1.49822 - acc: 0.6406 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1501  | total loss: \u001b[1m\u001b[32m1.36960\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1501 | loss: 1.36960 - acc: 0.6766 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1502  | total loss: \u001b[1m\u001b[32m1.25430\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1502 | loss: 1.25430 - acc: 0.7089 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1503  | total loss: \u001b[1m\u001b[32m1.15091\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1503 | loss: 1.15091 - acc: 0.7380 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1504  | total loss: \u001b[1m\u001b[32m1.05815\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 1504 | loss: 1.05815 - acc: 0.7642 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1505  | total loss: \u001b[1m\u001b[32m0.97492\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1505 | loss: 0.97492 - acc: 0.7878 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1506  | total loss: \u001b[1m\u001b[32m0.90018\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1506 | loss: 0.90018 - acc: 0.8090 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1507  | total loss: \u001b[1m\u001b[32m0.83303\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1507 | loss: 0.83303 - acc: 0.8281 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1508  | total loss: \u001b[1m\u001b[32m1.01636\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1508 | loss: 1.01636 - acc: 0.7722 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1509  | total loss: \u001b[1m\u001b[32m0.93785\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1509 | loss: 0.93785 - acc: 0.7950 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1510  | total loss: \u001b[1m\u001b[32m1.14281\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1510 | loss: 1.14281 - acc: 0.7347 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1511  | total loss: \u001b[1m\u001b[32m1.05209\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1511 | loss: 1.05209 - acc: 0.7613 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1512  | total loss: \u001b[1m\u001b[32m1.18968\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1512 | loss: 1.18968 - acc: 0.7082 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1513  | total loss: \u001b[1m\u001b[32m1.09483\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1513 | loss: 1.09483 - acc: 0.7374 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1514  | total loss: \u001b[1m\u001b[32m1.00975\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1514 | loss: 1.00975 - acc: 0.7636 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1515  | total loss: \u001b[1m\u001b[32m0.93336\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1515 | loss: 0.93336 - acc: 0.7873 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1516  | total loss: \u001b[1m\u001b[32m1.09298\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1516 | loss: 1.09298 - acc: 0.7393 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1517  | total loss: \u001b[1m\u001b[32m1.00866\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1517 | loss: 1.00866 - acc: 0.7654 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1518  | total loss: \u001b[1m\u001b[32m1.22357\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 1518 | loss: 1.22357 - acc: 0.7004 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1519  | total loss: \u001b[1m\u001b[32m1.12674\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1519 | loss: 1.12674 - acc: 0.7304 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1520  | total loss: \u001b[1m\u001b[32m1.03987\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 1520 | loss: 1.03987 - acc: 0.7573 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1521  | total loss: \u001b[1m\u001b[32m0.96189\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1521 | loss: 0.96189 - acc: 0.7816 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1522  | total loss: \u001b[1m\u001b[32m1.15782\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1522 | loss: 1.15782 - acc: 0.7188 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1523  | total loss: \u001b[1m\u001b[32m1.06846\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1523 | loss: 1.06846 - acc: 0.7469 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1524  | total loss: \u001b[1m\u001b[32m0.98823\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1524 | loss: 0.98823 - acc: 0.7722 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1525  | total loss: \u001b[1m\u001b[32m0.91615\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 1525 | loss: 0.91615 - acc: 0.7950 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1526  | total loss: \u001b[1m\u001b[32m1.11692\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1526 | loss: 1.11692 - acc: 0.7386 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1527  | total loss: \u001b[1m\u001b[32m1.03226\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1527 | loss: 1.03226 - acc: 0.7647 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1528  | total loss: \u001b[1m\u001b[32m1.19676\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1528 | loss: 1.19676 - acc: 0.7113 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1529  | total loss: \u001b[1m\u001b[32m1.10455\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1529 | loss: 1.10455 - acc: 0.7402 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1530  | total loss: \u001b[1m\u001b[32m1.26697\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1530 | loss: 1.26697 - acc: 0.6854 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1531  | total loss: \u001b[1m\u001b[32m1.16829\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 1531 | loss: 1.16829 - acc: 0.7169 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1532  | total loss: \u001b[1m\u001b[32m1.29959\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 1532 | loss: 1.29959 - acc: 0.6760 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1533  | total loss: \u001b[1m\u001b[32m1.19828\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1533 | loss: 1.19828 - acc: 0.7084 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1534  | total loss: \u001b[1m\u001b[32m1.35519\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1534 | loss: 1.35519 - acc: 0.6568 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1535  | total loss: \u001b[1m\u001b[32m1.24903\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1535 | loss: 1.24903 - acc: 0.6911 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1536  | total loss: \u001b[1m\u001b[32m1.36673\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1536 | loss: 1.36673 - acc: 0.6450 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1537  | total loss: \u001b[1m\u001b[32m1.26017\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1537 | loss: 1.26017 - acc: 0.6805 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1538  | total loss: \u001b[1m\u001b[32m1.43096\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1538 | loss: 1.43096 - acc: 0.6279 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1539  | total loss: \u001b[1m\u001b[32m1.31881\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1539 | loss: 1.31881 - acc: 0.6651 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1540  | total loss: \u001b[1m\u001b[32m1.50657\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1540 | loss: 1.50657 - acc: 0.6063 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1541  | total loss: \u001b[1m\u001b[32m1.38783\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1541 | loss: 1.38783 - acc: 0.6456 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1542  | total loss: \u001b[1m\u001b[32m1.51418\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1542 | loss: 1.51418 - acc: 0.6042 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1543  | total loss: \u001b[1m\u001b[32m1.39569\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1543 | loss: 1.39569 - acc: 0.6437 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1544  | total loss: \u001b[1m\u001b[32m1.51022\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1544 | loss: 1.51022 - acc: 0.5986 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1545  | total loss: \u001b[1m\u001b[32m1.39313\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1545 | loss: 1.39313 - acc: 0.6387 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1546  | total loss: \u001b[1m\u001b[32m1.51868\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1546 | loss: 1.51868 - acc: 0.5979 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1547  | total loss: \u001b[1m\u001b[32m1.40176\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1547 | loss: 1.40176 - acc: 0.6381 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1548  | total loss: \u001b[1m\u001b[32m1.51955\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1548 | loss: 1.51955 - acc: 0.5897 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1549  | total loss: \u001b[1m\u001b[32m1.40354\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1549 | loss: 1.40354 - acc: 0.6307 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1550  | total loss: \u001b[1m\u001b[32m1.50623\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1550 | loss: 1.50623 - acc: 0.5869 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1551  | total loss: \u001b[1m\u001b[32m1.39251\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1551 | loss: 1.39251 - acc: 0.6282 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1552  | total loss: \u001b[1m\u001b[32m1.51208\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1552 | loss: 1.51208 - acc: 0.5885 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1553  | total loss: \u001b[1m\u001b[32m1.39868\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1553 | loss: 1.39868 - acc: 0.6296 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1554  | total loss: \u001b[1m\u001b[32m1.29701\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1554 | loss: 1.29701 - acc: 0.6667 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1555  | total loss: \u001b[1m\u001b[32m1.20576\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1555 | loss: 1.20576 - acc: 0.7000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1556  | total loss: \u001b[1m\u001b[32m1.36448\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1556 | loss: 1.36448 - acc: 0.6377 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1557  | total loss: \u001b[1m\u001b[32m1.26699\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1557 | loss: 1.26699 - acc: 0.6739 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1558  | total loss: \u001b[1m\u001b[32m1.17947\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 1558 | loss: 1.17947 - acc: 0.7065 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1559  | total loss: \u001b[1m\u001b[32m1.10082\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1559 | loss: 1.10082 - acc: 0.7359 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1560  | total loss: \u001b[1m\u001b[32m1.22103\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1560 | loss: 1.22103 - acc: 0.6892 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1561  | total loss: \u001b[1m\u001b[32m1.13841\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 1561 | loss: 1.13841 - acc: 0.7203 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1562  | total loss: \u001b[1m\u001b[32m1.06411\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1562 | loss: 1.06411 - acc: 0.7483 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1563  | total loss: \u001b[1m\u001b[32m0.99722\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1563 | loss: 0.99722 - acc: 0.7734 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1564  | total loss: \u001b[1m\u001b[32m0.93689\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1564 | loss: 0.93689 - acc: 0.7961 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1565  | total loss: \u001b[1m\u001b[32m0.88240\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1565 | loss: 0.88240 - acc: 0.8165 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1566  | total loss: \u001b[1m\u001b[32m1.03771\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1566 | loss: 1.03771 - acc: 0.7425 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1567  | total loss: \u001b[1m\u001b[32m0.97281\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1567 | loss: 0.97281 - acc: 0.7683 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1568  | total loss: \u001b[1m\u001b[32m0.91426\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1568 | loss: 0.91426 - acc: 0.7914 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1569  | total loss: \u001b[1m\u001b[32m0.86135\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1569 | loss: 0.86135 - acc: 0.8123 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1570  | total loss: \u001b[1m\u001b[32m0.81345\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 1570 | loss: 0.81345 - acc: 0.8311 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1571  | total loss: \u001b[1m\u001b[32m0.77000\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1571 | loss: 0.77000 - acc: 0.8480 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1572  | total loss: \u001b[1m\u001b[32m0.95005\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1572 | loss: 0.95005 - acc: 0.7747 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1573  | total loss: \u001b[1m\u001b[32m0.89239\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1573 | loss: 0.89239 - acc: 0.7972 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1574  | total loss: \u001b[1m\u001b[32m0.84026\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 1574 | loss: 0.84026 - acc: 0.8175 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1575  | total loss: \u001b[1m\u001b[32m0.79306\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1575 | loss: 0.79306 - acc: 0.8358 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1576  | total loss: \u001b[1m\u001b[32m0.75024\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 1576 | loss: 0.75024 - acc: 0.8522 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1577  | total loss: \u001b[1m\u001b[32m0.71132\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1577 | loss: 0.71132 - acc: 0.8670 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1578  | total loss: \u001b[1m\u001b[32m0.89331\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1578 | loss: 0.89331 - acc: 0.7957 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1579  | total loss: \u001b[1m\u001b[32m0.83948\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 1579 | loss: 0.83948 - acc: 0.8161 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1580  | total loss: \u001b[1m\u001b[32m0.79078\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 1580 | loss: 0.79078 - acc: 0.8345 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1581  | total loss: \u001b[1m\u001b[32m0.74666\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1581 | loss: 0.74666 - acc: 0.8510 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1582  | total loss: \u001b[1m\u001b[32m0.70661\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 1582 | loss: 0.70661 - acc: 0.8659 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1583  | total loss: \u001b[1m\u001b[32m0.67019\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 1583 | loss: 0.67019 - acc: 0.8793 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1584  | total loss: \u001b[1m\u001b[32m0.86281\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 1584 | loss: 0.86281 - acc: 0.8106 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1585  | total loss: \u001b[1m\u001b[32m0.81018\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1585 | loss: 0.81018 - acc: 0.8296 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1586  | total loss: \u001b[1m\u001b[32m1.03922\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1586 | loss: 1.03922 - acc: 0.7582 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1587  | total loss: \u001b[1m\u001b[32m0.96873\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 1587 | loss: 0.96873 - acc: 0.7823 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1588  | total loss: \u001b[1m\u001b[32m1.13620\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1588 | loss: 1.13620 - acc: 0.7195 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1589  | total loss: \u001b[1m\u001b[32m1.05606\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1589 | loss: 1.05606 - acc: 0.7475 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1590  | total loss: \u001b[1m\u001b[32m1.23234\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1590 | loss: 1.23234 - acc: 0.6920 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1591  | total loss: \u001b[1m\u001b[32m1.14283\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1591 | loss: 1.14283 - acc: 0.7228 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1592  | total loss: \u001b[1m\u001b[32m1.06240\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 1592 | loss: 1.06240 - acc: 0.7505 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1593  | total loss: \u001b[1m\u001b[32m0.99006\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1593 | loss: 0.99006 - acc: 0.7755 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1594  | total loss: \u001b[1m\u001b[32m0.92491\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1594 | loss: 0.92491 - acc: 0.7979 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1595  | total loss: \u001b[1m\u001b[32m0.86618\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1595 | loss: 0.86618 - acc: 0.8181 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1596  | total loss: \u001b[1m\u001b[32m1.05316\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 1596 | loss: 1.05316 - acc: 0.7517 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1597  | total loss: \u001b[1m\u001b[32m0.98149\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1597 | loss: 0.98149 - acc: 0.7765 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1598  | total loss: \u001b[1m\u001b[32m1.13535\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1598 | loss: 1.13535 - acc: 0.7181 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1599  | total loss: \u001b[1m\u001b[32m1.05558\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1599 | loss: 1.05558 - acc: 0.7463 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1600  | total loss: \u001b[1m\u001b[32m1.23335\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1600 | loss: 1.23335 - acc: 0.6832 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1601  | total loss: \u001b[1m\u001b[32m1.14412\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 1601 | loss: 1.14412 - acc: 0.7149 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1602  | total loss: \u001b[1m\u001b[32m1.28270\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 1602 | loss: 1.28270 - acc: 0.6626 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1603  | total loss: \u001b[1m\u001b[32m1.18902\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1603 | loss: 1.18902 - acc: 0.6964 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1604  | total loss: \u001b[1m\u001b[32m1.32238\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 1604 | loss: 1.32238 - acc: 0.6498 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1605  | total loss: \u001b[1m\u001b[32m1.22529\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1605 | loss: 1.22529 - acc: 0.6848 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1606  | total loss: \u001b[1m\u001b[32m1.35894\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1606 | loss: 1.35894 - acc: 0.6356 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1607  | total loss: \u001b[1m\u001b[32m1.25880\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1607 | loss: 1.25880 - acc: 0.6720 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1608  | total loss: \u001b[1m\u001b[32m1.16892\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1608 | loss: 1.16892 - acc: 0.7048 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1609  | total loss: \u001b[1m\u001b[32m1.08818\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1609 | loss: 1.08818 - acc: 0.7343 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1610  | total loss: \u001b[1m\u001b[32m1.27052\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 1610 | loss: 1.27052 - acc: 0.6686 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1611  | total loss: \u001b[1m\u001b[32m1.17993\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1611 | loss: 1.17993 - acc: 0.7017 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1612  | total loss: \u001b[1m\u001b[32m1.09855\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 1612 | loss: 1.09855 - acc: 0.7316 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1613  | total loss: \u001b[1m\u001b[32m1.02537\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1613 | loss: 1.02537 - acc: 0.7584 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1614  | total loss: \u001b[1m\u001b[32m1.13966\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1614 | loss: 1.13966 - acc: 0.7133 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1615  | total loss: \u001b[1m\u001b[32m1.06244\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1615 | loss: 1.06244 - acc: 0.7420 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1616  | total loss: \u001b[1m\u001b[32m0.99295\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1616 | loss: 0.99295 - acc: 0.7678 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1617  | total loss: \u001b[1m\u001b[32m0.93033\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1617 | loss: 0.93033 - acc: 0.7910 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1618  | total loss: \u001b[1m\u001b[32m1.06533\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1618 | loss: 1.06533 - acc: 0.7311 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1619  | total loss: \u001b[1m\u001b[32m0.99534\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1619 | loss: 0.99534 - acc: 0.7580 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1620  | total loss: \u001b[1m\u001b[32m1.13522\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1620 | loss: 1.13522 - acc: 0.7015 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1621  | total loss: \u001b[1m\u001b[32m1.05826\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1621 | loss: 1.05826 - acc: 0.7313 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1622  | total loss: \u001b[1m\u001b[32m1.16948\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1622 | loss: 1.16948 - acc: 0.6851 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1623  | total loss: \u001b[1m\u001b[32m1.08922\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1623 | loss: 1.08922 - acc: 0.7166 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1624  | total loss: \u001b[1m\u001b[32m1.23546\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 1624 | loss: 1.23546 - acc: 0.6565 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1625  | total loss: \u001b[1m\u001b[32m1.14888\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1625 | loss: 1.14888 - acc: 0.6908 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1626  | total loss: \u001b[1m\u001b[32m1.07108\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 1626 | loss: 1.07108 - acc: 0.7217 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1627  | total loss: \u001b[1m\u001b[32m1.00109\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 1627 | loss: 1.00109 - acc: 0.7496 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1628  | total loss: \u001b[1m\u001b[32m0.93805\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1628 | loss: 0.93805 - acc: 0.7746 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1629  | total loss: \u001b[1m\u001b[32m0.88119\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1629 | loss: 0.88119 - acc: 0.7972 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1630  | total loss: \u001b[1m\u001b[32m1.06778\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 1630 | loss: 1.06778 - acc: 0.7290 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1631  | total loss: \u001b[1m\u001b[32m0.99777\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 1631 | loss: 0.99777 - acc: 0.7561 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1632  | total loss: \u001b[1m\u001b[32m1.14588\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 1632 | loss: 1.14588 - acc: 0.6959 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1633  | total loss: \u001b[1m\u001b[32m1.06810\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1633 | loss: 1.06810 - acc: 0.7263 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1634  | total loss: \u001b[1m\u001b[32m1.18182\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1634 | loss: 1.18182 - acc: 0.6844 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1635  | total loss: \u001b[1m\u001b[32m1.10061\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1635 | loss: 1.10061 - acc: 0.7160 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1636  | total loss: \u001b[1m\u001b[32m1.25400\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1636 | loss: 1.25400 - acc: 0.6598 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1637  | total loss: \u001b[1m\u001b[32m1.16585\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1637 | loss: 1.16585 - acc: 0.6938 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1638  | total loss: \u001b[1m\u001b[32m1.08664\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1638 | loss: 1.08664 - acc: 0.7244 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1639  | total loss: \u001b[1m\u001b[32m1.01538\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1639 | loss: 1.01538 - acc: 0.7520 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1640  | total loss: \u001b[1m\u001b[32m1.22121\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1640 | loss: 1.22121 - acc: 0.6806 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1641  | total loss: \u001b[1m\u001b[32m1.13663\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1641 | loss: 1.13663 - acc: 0.7126 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1642  | total loss: \u001b[1m\u001b[32m1.26312\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 1642 | loss: 1.26312 - acc: 0.6644 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1643  | total loss: \u001b[1m\u001b[32m1.17466\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1643 | loss: 1.17466 - acc: 0.6979 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1644  | total loss: \u001b[1m\u001b[32m1.29024\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1644 | loss: 1.29024 - acc: 0.6589 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1645  | total loss: \u001b[1m\u001b[32m1.19945\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1645 | loss: 1.19945 - acc: 0.6930 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1646  | total loss: \u001b[1m\u001b[32m1.33691\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1646 | loss: 1.33691 - acc: 0.6353 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1647  | total loss: \u001b[1m\u001b[32m1.24192\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1647 | loss: 1.24192 - acc: 0.6717 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1648  | total loss: \u001b[1m\u001b[32m1.38227\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1648 | loss: 1.38227 - acc: 0.6199 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1649  | total loss: \u001b[1m\u001b[32m1.28333\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1649 | loss: 1.28333 - acc: 0.6579 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1650  | total loss: \u001b[1m\u001b[32m1.43236\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1650 | loss: 1.43236 - acc: 0.5998 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1651  | total loss: \u001b[1m\u001b[32m1.32909\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1651 | loss: 1.32909 - acc: 0.6399 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1652  | total loss: \u001b[1m\u001b[32m1.23645\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1652 | loss: 1.23645 - acc: 0.6759 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1653  | total loss: \u001b[1m\u001b[32m1.15326\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1653 | loss: 1.15326 - acc: 0.7083 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1654  | total loss: \u001b[1m\u001b[32m1.29625\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1654 | loss: 1.29625 - acc: 0.6490 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1655  | total loss: \u001b[1m\u001b[32m1.20740\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1655 | loss: 1.20740 - acc: 0.6841 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1656  | total loss: \u001b[1m\u001b[32m1.12759\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1656 | loss: 1.12759 - acc: 0.7157 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1657  | total loss: \u001b[1m\u001b[32m1.05579\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1657 | loss: 1.05579 - acc: 0.7441 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1658  | total loss: \u001b[1m\u001b[32m1.21359\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1658 | loss: 1.21359 - acc: 0.6812 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1659  | total loss: \u001b[1m\u001b[32m1.13326\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1659 | loss: 1.13326 - acc: 0.7131 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1660  | total loss: \u001b[1m\u001b[32m1.06098\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1660 | loss: 1.06098 - acc: 0.7418 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1661  | total loss: \u001b[1m\u001b[32m0.99586\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1661 | loss: 0.99586 - acc: 0.7676 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1662  | total loss: \u001b[1m\u001b[32m0.93709\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1662 | loss: 0.93709 - acc: 0.7909 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1663  | total loss: \u001b[1m\u001b[32m0.88397\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1663 | loss: 0.88397 - acc: 0.8118 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1664  | total loss: \u001b[1m\u001b[32m1.02080\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1664 | loss: 1.02080 - acc: 0.7498 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1665  | total loss: \u001b[1m\u001b[32m0.95889\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1665 | loss: 0.95889 - acc: 0.7748 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1666  | total loss: \u001b[1m\u001b[32m0.90298\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 1666 | loss: 0.90298 - acc: 0.7974 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1667  | total loss: \u001b[1m\u001b[32m0.85239\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1667 | loss: 0.85239 - acc: 0.8176 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1668  | total loss: \u001b[1m\u001b[32m0.80654\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1668 | loss: 0.80654 - acc: 0.8359 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1669  | total loss: \u001b[1m\u001b[32m0.76489\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 1669 | loss: 0.76489 - acc: 0.8523 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1670  | total loss: \u001b[1m\u001b[32m0.92653\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1670 | loss: 0.92653 - acc: 0.7863 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1671  | total loss: \u001b[1m\u001b[32m0.87227\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 1671 | loss: 0.87227 - acc: 0.8077 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1672  | total loss: \u001b[1m\u001b[32m0.82318\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1672 | loss: 0.82318 - acc: 0.8269 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1673  | total loss: \u001b[1m\u001b[32m0.77868\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1673 | loss: 0.77868 - acc: 0.8442 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1674  | total loss: \u001b[1m\u001b[32m0.73827\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1674 | loss: 0.73827 - acc: 0.8598 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1675  | total loss: \u001b[1m\u001b[32m0.70150\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 1675 | loss: 0.70150 - acc: 0.8738 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1676  | total loss: \u001b[1m\u001b[32m0.83358\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1676 | loss: 0.83358 - acc: 0.8172 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1677  | total loss: \u001b[1m\u001b[32m0.78656\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 1677 | loss: 0.78656 - acc: 0.8355 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1678  | total loss: \u001b[1m\u001b[32m0.93102\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1678 | loss: 0.93102 - acc: 0.7750 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1679  | total loss: \u001b[1m\u001b[32m0.87379\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1679 | loss: 0.87379 - acc: 0.7975 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1680  | total loss: \u001b[1m\u001b[32m1.04425\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1680 | loss: 1.04425 - acc: 0.7331 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1681  | total loss: \u001b[1m\u001b[32m0.97548\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 1681 | loss: 0.97548 - acc: 0.7598 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1682  | total loss: \u001b[1m\u001b[32m0.91349\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1682 | loss: 0.91349 - acc: 0.7838 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1683  | total loss: \u001b[1m\u001b[32m0.85755\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1683 | loss: 0.85755 - acc: 0.8055 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1684  | total loss: \u001b[1m\u001b[32m0.99751\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 1684 | loss: 0.99751 - acc: 0.7480 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1685  | total loss: \u001b[1m\u001b[32m0.93290\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1685 | loss: 0.93290 - acc: 0.7732 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1686  | total loss: \u001b[1m\u001b[32m1.12030\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1686 | loss: 1.12030 - acc: 0.6997 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1687  | total loss: \u001b[1m\u001b[32m1.04336\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1687 | loss: 1.04336 - acc: 0.7297 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1688  | total loss: \u001b[1m\u001b[32m1.17011\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1688 | loss: 1.17011 - acc: 0.6760 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1689  | total loss: \u001b[1m\u001b[32m1.08834\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1689 | loss: 1.08834 - acc: 0.7084 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1690  | total loss: \u001b[1m\u001b[32m1.26330\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1690 | loss: 1.26330 - acc: 0.6491 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1691  | total loss: \u001b[1m\u001b[32m1.17250\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1691 | loss: 1.17250 - acc: 0.6842 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1692  | total loss: \u001b[1m\u001b[32m1.09093\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1692 | loss: 1.09093 - acc: 0.7158 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1693  | total loss: \u001b[1m\u001b[32m1.01756\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1693 | loss: 1.01756 - acc: 0.7442 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1694  | total loss: \u001b[1m\u001b[32m1.13616\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1694 | loss: 1.13616 - acc: 0.6929 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1695  | total loss: \u001b[1m\u001b[32m1.05836\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 1695 | loss: 1.05836 - acc: 0.7236 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1696  | total loss: \u001b[1m\u001b[32m0.98836\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1696 | loss: 0.98836 - acc: 0.7512 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1697  | total loss: \u001b[1m\u001b[32m0.92531\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1697 | loss: 0.92531 - acc: 0.7761 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1698  | total loss: \u001b[1m\u001b[32m1.10733\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1698 | loss: 1.10733 - acc: 0.7062 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1699  | total loss: \u001b[1m\u001b[32m1.03234\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 1699 | loss: 1.03234 - acc: 0.7356 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1700  | total loss: \u001b[1m\u001b[32m1.18343\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1700 | loss: 1.18343 - acc: 0.6812 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1701  | total loss: \u001b[1m\u001b[32m1.10098\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1701 | loss: 1.10098 - acc: 0.7131 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1702  | total loss: \u001b[1m\u001b[32m1.21907\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 1702 | loss: 1.21907 - acc: 0.6649 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1703  | total loss: \u001b[1m\u001b[32m1.13334\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1703 | loss: 1.13334 - acc: 0.6984 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1704  | total loss: \u001b[1m\u001b[32m1.05628\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1704 | loss: 1.05628 - acc: 0.7285 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1705  | total loss: \u001b[1m\u001b[32m0.98695\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 1705 | loss: 0.98695 - acc: 0.7557 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1706  | total loss: \u001b[1m\u001b[32m1.14624\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 1706 | loss: 1.14624 - acc: 0.6917 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1707  | total loss: \u001b[1m\u001b[32m1.06797\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1707 | loss: 1.06797 - acc: 0.7225 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1708  | total loss: \u001b[1m\u001b[32m1.19488\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 1708 | loss: 1.19488 - acc: 0.6733 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1709  | total loss: \u001b[1m\u001b[32m1.11196\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 1709 | loss: 1.11196 - acc: 0.7060 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1710  | total loss: \u001b[1m\u001b[32m1.03742\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 1710 | loss: 1.03742 - acc: 0.7354 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1711  | total loss: \u001b[1m\u001b[32m0.97033\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 1711 | loss: 0.97033 - acc: 0.7619 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1712  | total loss: \u001b[1m\u001b[32m1.09330\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 1712 | loss: 1.09330 - acc: 0.7203 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1713  | total loss: \u001b[1m\u001b[32m1.02059\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1713 | loss: 1.02059 - acc: 0.7483 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1714  | total loss: \u001b[1m\u001b[32m1.18428\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 1714 | loss: 1.18428 - acc: 0.6888 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1715  | total loss: \u001b[1m\u001b[32m1.10257\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 1715 | loss: 1.10257 - acc: 0.7199 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1716  | total loss: \u001b[1m\u001b[32m1.19554\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 1716 | loss: 1.19554 - acc: 0.6864 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1717  | total loss: \u001b[1m\u001b[32m1.11292\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 1717 | loss: 1.11292 - acc: 0.7178 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1718  | total loss: \u001b[1m\u001b[32m1.24037\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 1718 | loss: 1.24037 - acc: 0.6652 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1719  | total loss: \u001b[1m\u001b[32m1.15354\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1719 | loss: 1.15354 - acc: 0.6987 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1720  | total loss: \u001b[1m\u001b[32m1.07549\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 1720 | loss: 1.07549 - acc: 0.7288 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1721  | total loss: \u001b[1m\u001b[32m1.00526\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1721 | loss: 1.00526 - acc: 0.7559 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1722  | total loss: \u001b[1m\u001b[32m1.11032\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1722 | loss: 1.11032 - acc: 0.7111 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1723  | total loss: \u001b[1m\u001b[32m1.03660\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 1723 | loss: 1.03660 - acc: 0.7400 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1724  | total loss: \u001b[1m\u001b[32m0.97023\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1724 | loss: 0.97023 - acc: 0.7660 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1725  | total loss: \u001b[1m\u001b[32m0.91040\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 1725 | loss: 0.91040 - acc: 0.7894 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1726  | total loss: \u001b[1m\u001b[32m0.85638\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1726 | loss: 0.85638 - acc: 0.8105 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1727  | total loss: \u001b[1m\u001b[32m0.80754\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 1727 | loss: 0.80754 - acc: 0.8294 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1728  | total loss: \u001b[1m\u001b[32m1.00198\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 1728 | loss: 1.00198 - acc: 0.7503 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1729  | total loss: \u001b[1m\u001b[32m0.93822\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1729 | loss: 0.93822 - acc: 0.7753 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1730  | total loss: \u001b[1m\u001b[32m1.06634\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 1730 | loss: 1.06634 - acc: 0.7208 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1731  | total loss: \u001b[1m\u001b[32m0.99598\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1731 | loss: 0.99598 - acc: 0.7488 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1732  | total loss: \u001b[1m\u001b[32m1.16558\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1732 | loss: 1.16558 - acc: 0.6931 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1733  | total loss: \u001b[1m\u001b[32m1.08532\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 1733 | loss: 1.08532 - acc: 0.7238 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1734  | total loss: \u001b[1m\u001b[32m1.21796\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1734 | loss: 1.21796 - acc: 0.6745 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1735  | total loss: \u001b[1m\u001b[32m1.13267\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1735 | loss: 1.13267 - acc: 0.7070 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1736  | total loss: \u001b[1m\u001b[32m1.30245\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1736 | loss: 1.30245 - acc: 0.6402 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1737  | total loss: \u001b[1m\u001b[32m1.20908\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 1737 | loss: 1.20908 - acc: 0.6762 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1738  | total loss: \u001b[1m\u001b[32m1.33880\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1738 | loss: 1.33880 - acc: 0.6201 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1739  | total loss: \u001b[1m\u001b[32m1.24230\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 1739 | loss: 1.24230 - acc: 0.6581 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1740  | total loss: \u001b[1m\u001b[32m1.15567\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 1740 | loss: 1.15567 - acc: 0.6923 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1741  | total loss: \u001b[1m\u001b[32m1.07781\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1741 | loss: 1.07781 - acc: 0.7230 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1742  | total loss: \u001b[1m\u001b[32m1.18406\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1742 | loss: 1.18406 - acc: 0.6777 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1743  | total loss: \u001b[1m\u001b[32m1.10354\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1743 | loss: 1.10354 - acc: 0.7099 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1744  | total loss: \u001b[1m\u001b[32m1.22987\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 1744 | loss: 1.22987 - acc: 0.6620 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1745  | total loss: \u001b[1m\u001b[32m1.14503\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 1745 | loss: 1.14503 - acc: 0.6958 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1746  | total loss: \u001b[1m\u001b[32m1.27818\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1746 | loss: 1.27818 - acc: 0.6454 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1747  | total loss: \u001b[1m\u001b[32m1.18886\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1747 | loss: 1.18886 - acc: 0.6809 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1748  | total loss: \u001b[1m\u001b[32m1.30497\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 1748 | loss: 1.30497 - acc: 0.6282 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1749  | total loss: \u001b[1m\u001b[32m1.21338\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1749 | loss: 1.21338 - acc: 0.6654 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1750  | total loss: \u001b[1m\u001b[32m1.26436\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 1750 | loss: 1.26436 - acc: 0.6411 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1751  | total loss: \u001b[1m\u001b[32m1.17721\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 1751 | loss: 1.17721 - acc: 0.6770 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1752  | total loss: \u001b[1m\u001b[32m1.29232\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 1752 | loss: 1.29232 - acc: 0.6286 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1753  | total loss: \u001b[1m\u001b[32m1.20272\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 1753 | loss: 1.20272 - acc: 0.6657 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1754  | total loss: \u001b[1m\u001b[32m1.30686\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1754 | loss: 1.30686 - acc: 0.6261 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1755  | total loss: \u001b[1m\u001b[32m1.21621\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 1755 | loss: 1.21621 - acc: 0.6634 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1756  | total loss: \u001b[1m\u001b[32m1.36511\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1756 | loss: 1.36511 - acc: 0.6086 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1757  | total loss: \u001b[1m\u001b[32m1.26912\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 1757 | loss: 1.26912 - acc: 0.6478 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1758  | total loss: \u001b[1m\u001b[32m1.36586\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 1758 | loss: 1.36586 - acc: 0.6022 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1759  | total loss: \u001b[1m\u001b[32m1.27032\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1759 | loss: 1.27032 - acc: 0.6420 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1760  | total loss: \u001b[1m\u001b[32m1.38115\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 1760 | loss: 1.38115 - acc: 0.6009 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1761  | total loss: \u001b[1m\u001b[32m1.28461\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 1761 | loss: 1.28461 - acc: 0.6408 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1762  | total loss: \u001b[1m\u001b[32m1.37460\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 1762 | loss: 1.37460 - acc: 0.5998 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1763  | total loss: \u001b[1m\u001b[32m1.27925\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1763 | loss: 1.27925 - acc: 0.6398 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1764  | total loss: \u001b[1m\u001b[32m1.38588\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1764 | loss: 1.38588 - acc: 0.5912 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1765  | total loss: \u001b[1m\u001b[32m1.28993\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 1765 | loss: 1.28993 - acc: 0.6321 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1766  | total loss: \u001b[1m\u001b[32m1.39053\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 1766 | loss: 1.39053 - acc: 0.5881 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1767  | total loss: \u001b[1m\u001b[32m1.29463\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1767 | loss: 1.29463 - acc: 0.6293 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1768  | total loss: \u001b[1m\u001b[32m1.39406\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 1768 | loss: 1.39406 - acc: 0.5779 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1769  | total loss: \u001b[1m\u001b[32m1.29832\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1769 | loss: 1.29832 - acc: 0.6201 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1770  | total loss: \u001b[1m\u001b[32m1.41170\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1770 | loss: 1.41170 - acc: 0.5696 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1771  | total loss: \u001b[1m\u001b[32m1.31471\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 1771 | loss: 1.31471 - acc: 0.6127 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1772  | total loss: \u001b[1m\u001b[32m1.41576\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 1772 | loss: 1.41576 - acc: 0.5668 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1773  | total loss: \u001b[1m\u001b[32m1.31887\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 1773 | loss: 1.31887 - acc: 0.6101 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1774  | total loss: \u001b[1m\u001b[32m1.44088\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 1774 | loss: 1.44088 - acc: 0.5606 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1775  | total loss: \u001b[1m\u001b[32m1.34196\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 1775 | loss: 1.34196 - acc: 0.6046 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1776  | total loss: \u001b[1m\u001b[32m1.25313\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1776 | loss: 1.25313 - acc: 0.6441 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1777  | total loss: \u001b[1m\u001b[32m1.17325\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 1777 | loss: 1.17325 - acc: 0.6797 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1778  | total loss: \u001b[1m\u001b[32m1.10131\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 1778 | loss: 1.10131 - acc: 0.7117 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1779  | total loss: \u001b[1m\u001b[32m1.03643\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1779 | loss: 1.03643 - acc: 0.7406 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1780  | total loss: \u001b[1m\u001b[32m0.97781\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 1780 | loss: 0.97781 - acc: 0.7665 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1781  | total loss: \u001b[1m\u001b[32m0.92475\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 1781 | loss: 0.92475 - acc: 0.7899 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1782  | total loss: \u001b[1m\u001b[32m0.87663\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1782 | loss: 0.87663 - acc: 0.8109 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1783  | total loss: \u001b[1m\u001b[32m0.83291\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 1783 | loss: 0.83291 - acc: 0.8298 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1784  | total loss: \u001b[1m\u001b[32m0.79309\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 1784 | loss: 0.79309 - acc: 0.8468 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1785  | total loss: \u001b[1m\u001b[32m0.75676\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 1785 | loss: 0.75676 - acc: 0.8621 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1786  | total loss: \u001b[1m\u001b[32m0.72351\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1786 | loss: 0.72351 - acc: 0.8759 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1787  | total loss: \u001b[1m\u001b[32m0.69301\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1787 | loss: 0.69301 - acc: 0.8883 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1788  | total loss: \u001b[1m\u001b[32m0.66497\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1788 | loss: 0.66497 - acc: 0.8995 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1789  | total loss: \u001b[1m\u001b[32m0.63910\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1789 | loss: 0.63910 - acc: 0.9095 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1790  | total loss: \u001b[1m\u001b[32m0.61520\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 1790 | loss: 0.61520 - acc: 0.9186 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1791  | total loss: \u001b[1m\u001b[32m0.59305\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 1791 | loss: 0.59305 - acc: 0.9267 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1792  | total loss: \u001b[1m\u001b[32m0.57246\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 1792 | loss: 0.57246 - acc: 0.9341 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1793  | total loss: \u001b[1m\u001b[32m0.55329\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 1793 | loss: 0.55329 - acc: 0.9406 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1794  | total loss: \u001b[1m\u001b[32m0.53538\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1794 | loss: 0.53538 - acc: 0.9466 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1795  | total loss: \u001b[1m\u001b[32m0.51861\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 1795 | loss: 0.51861 - acc: 0.9519 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1796  | total loss: \u001b[1m\u001b[32m0.50287\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1796 | loss: 0.50287 - acc: 0.9567 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1797  | total loss: \u001b[1m\u001b[32m0.48806\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1797 | loss: 0.48806 - acc: 0.9611 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1798  | total loss: \u001b[1m\u001b[32m0.47408\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1798 | loss: 0.47408 - acc: 0.9650 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1799  | total loss: \u001b[1m\u001b[32m0.46087\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1799 | loss: 0.46087 - acc: 0.9685 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1800  | total loss: \u001b[1m\u001b[32m0.44835\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1800 | loss: 0.44835 - acc: 0.9716 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1801  | total loss: \u001b[1m\u001b[32m0.43646\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 1801 | loss: 0.43646 - acc: 0.9745 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1802  | total loss: \u001b[1m\u001b[32m0.42516\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1802 | loss: 0.42516 - acc: 0.9770 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1803  | total loss: \u001b[1m\u001b[32m0.41438\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1803 | loss: 0.41438 - acc: 0.9793 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1804  | total loss: \u001b[1m\u001b[32m0.40409\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 1804 | loss: 0.40409 - acc: 0.9814 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1805  | total loss: \u001b[1m\u001b[32m0.39426\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 1805 | loss: 0.39426 - acc: 0.9832 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1806  | total loss: \u001b[1m\u001b[32m0.38484\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 1806 | loss: 0.38484 - acc: 0.9849 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1807  | total loss: \u001b[1m\u001b[32m0.37580\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 1807 | loss: 0.37580 - acc: 0.9864 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1808  | total loss: \u001b[1m\u001b[32m0.36713\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 1808 | loss: 0.36713 - acc: 0.9878 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1809  | total loss: \u001b[1m\u001b[32m0.35879\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1809 | loss: 0.35879 - acc: 0.9890 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1810  | total loss: \u001b[1m\u001b[32m0.35076\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1810 | loss: 0.35076 - acc: 0.9901 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1811  | total loss: \u001b[1m\u001b[32m0.34303\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 1811 | loss: 0.34303 - acc: 0.9911 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1812  | total loss: \u001b[1m\u001b[32m0.33557\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 1812 | loss: 0.33557 - acc: 0.9920 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1813  | total loss: \u001b[1m\u001b[32m0.32838\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1813 | loss: 0.32838 - acc: 0.9928 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1814  | total loss: \u001b[1m\u001b[32m0.32143\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 1814 | loss: 0.32143 - acc: 0.9935 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1815  | total loss: \u001b[1m\u001b[32m0.31471\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1815 | loss: 0.31471 - acc: 0.9942 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1816  | total loss: \u001b[1m\u001b[32m0.56238\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1816 | loss: 0.56238 - acc: 0.9063 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1817  | total loss: \u001b[1m\u001b[32m0.53084\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 1817 | loss: 0.53084 - acc: 0.9157 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1818  | total loss: \u001b[1m\u001b[32m0.50218\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 1818 | loss: 0.50218 - acc: 0.9241 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1819  | total loss: \u001b[1m\u001b[32m0.47610\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 1819 | loss: 0.47610 - acc: 0.9317 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1820  | total loss: \u001b[1m\u001b[32m0.45234\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 1820 | loss: 0.45234 - acc: 0.9385 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1821  | total loss: \u001b[1m\u001b[32m0.43065\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1821 | loss: 0.43065 - acc: 0.9447 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1822  | total loss: \u001b[1m\u001b[32m0.41082\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1822 | loss: 0.41082 - acc: 0.9502 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1823  | total loss: \u001b[1m\u001b[32m0.39267\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 1823 | loss: 0.39267 - acc: 0.9552 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1824  | total loss: \u001b[1m\u001b[32m0.58288\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 1824 | loss: 0.58288 - acc: 0.8904 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1825  | total loss: \u001b[1m\u001b[32m0.54704\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 1825 | loss: 0.54704 - acc: 0.9014 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1826  | total loss: \u001b[1m\u001b[32m0.51460\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 1826 | loss: 0.51460 - acc: 0.9112 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1827  | total loss: \u001b[1m\u001b[32m0.48519\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 1827 | loss: 0.48519 - acc: 0.9201 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1828  | total loss: \u001b[1m\u001b[32m0.45850\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 1828 | loss: 0.45850 - acc: 0.9281 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1829  | total loss: \u001b[1m\u001b[32m0.43425\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 1829 | loss: 0.43425 - acc: 0.9353 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1830  | total loss: \u001b[1m\u001b[32m0.41219\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1830 | loss: 0.41219 - acc: 0.9418 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1831  | total loss: \u001b[1m\u001b[32m0.39210\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1831 | loss: 0.39210 - acc: 0.9476 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1832  | total loss: \u001b[1m\u001b[32m0.37376\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1832 | loss: 0.37376 - acc: 0.9528 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1833  | total loss: \u001b[1m\u001b[32m0.35701\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 1833 | loss: 0.35701 - acc: 0.9575 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1834  | total loss: \u001b[1m\u001b[32m0.34168\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1834 | loss: 0.34168 - acc: 0.9618 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1835  | total loss: \u001b[1m\u001b[32m0.32762\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1835 | loss: 0.32762 - acc: 0.9656 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1836  | total loss: \u001b[1m\u001b[32m0.31472\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 1836 | loss: 0.31472 - acc: 0.9691 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1837  | total loss: \u001b[1m\u001b[32m0.30284\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 1837 | loss: 0.30284 - acc: 0.9721 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1838  | total loss: \u001b[1m\u001b[32m0.29190\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1838 | loss: 0.29190 - acc: 0.9749 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1839  | total loss: \u001b[1m\u001b[32m0.28179\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 1839 | loss: 0.28179 - acc: 0.9774 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1840  | total loss: \u001b[1m\u001b[32m0.27243\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1840 | loss: 0.27243 - acc: 0.9797 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1841  | total loss: \u001b[1m\u001b[32m0.26376\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1841 | loss: 0.26376 - acc: 0.9836 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1842  | total loss: \u001b[1m\u001b[32m0.25571\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1842 | loss: 0.25571 - acc: 0.9836 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1843  | total loss: \u001b[1m\u001b[32m0.24821\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1843 | loss: 0.24821 - acc: 0.9852 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1844  | total loss: \u001b[1m\u001b[32m0.58220\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1844 | loss: 0.58220 - acc: 0.8944 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1845  | total loss: \u001b[1m\u001b[32m0.54174\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1845 | loss: 0.54174 - acc: 0.9049 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1846  | total loss: \u001b[1m\u001b[32m0.50525\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1846 | loss: 0.50525 - acc: 0.9144 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1847  | total loss: \u001b[1m\u001b[32m0.47230\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1847 | loss: 0.47230 - acc: 0.9230 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1848  | total loss: \u001b[1m\u001b[32m0.44254\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 1848 | loss: 0.44254 - acc: 0.9307 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1849  | total loss: \u001b[1m\u001b[32m0.41563\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1849 | loss: 0.41563 - acc: 0.9376 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1850  | total loss: \u001b[1m\u001b[32m0.39128\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 1850 | loss: 0.39128 - acc: 0.9439 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1851  | total loss: \u001b[1m\u001b[32m0.36922\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 1851 | loss: 0.36922 - acc: 0.9495 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1852  | total loss: \u001b[1m\u001b[32m0.60452\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1852 | loss: 0.60452 - acc: 0.8815 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1853  | total loss: \u001b[1m\u001b[32m0.56095\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 1853 | loss: 0.56095 - acc: 0.8933 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1854  | total loss: \u001b[1m\u001b[32m0.52168\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 1854 | loss: 0.52168 - acc: 0.9040 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1855  | total loss: \u001b[1m\u001b[32m0.48627\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 1855 | loss: 0.48627 - acc: 0.9136 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1856  | total loss: \u001b[1m\u001b[32m0.45431\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 1856 | loss: 0.45431 - acc: 0.9222 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1857  | total loss: \u001b[1m\u001b[32m0.42546\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 1857 | loss: 0.42546 - acc: 0.9300 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1858  | total loss: \u001b[1m\u001b[32m0.39938\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 1858 | loss: 0.39938 - acc: 0.9370 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1859  | total loss: \u001b[1m\u001b[32m0.37579\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 1859 | loss: 0.37579 - acc: 0.9433 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1860  | total loss: \u001b[1m\u001b[32m0.35443\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 1860 | loss: 0.35443 - acc: 0.9490 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1861  | total loss: \u001b[1m\u001b[32m0.31752\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1861 | loss: 0.31752 - acc: 0.9541 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1862  | total loss: \u001b[1m\u001b[32m0.31752\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 1862 | loss: 0.31752 - acc: 0.9587 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1863  | total loss: \u001b[1m\u001b[32m0.30157\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 1863 | loss: 0.30157 - acc: 0.9628 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1864  | total loss: \u001b[1m\u001b[32m0.28706\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 1864 | loss: 0.28706 - acc: 0.9665 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1865  | total loss: \u001b[1m\u001b[32m0.27386\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 1865 | loss: 0.27386 - acc: 0.9699 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1866  | total loss: \u001b[1m\u001b[32m0.26181\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 1866 | loss: 0.26181 - acc: 0.9729 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1867  | total loss: \u001b[1m\u001b[32m0.24077\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1867 | loss: 0.24077 - acc: 0.9756 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1868  | total loss: \u001b[1m\u001b[32m0.23156\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 1868 | loss: 0.23156 - acc: 0.9780 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1869  | total loss: \u001b[1m\u001b[32m0.22311\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1869 | loss: 0.22311 - acc: 0.9802 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1870  | total loss: \u001b[1m\u001b[32m0.22311\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 1870 | loss: 0.22311 - acc: 0.9822 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1871  | total loss: \u001b[1m\u001b[32m0.21535\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1871 | loss: 0.21535 - acc: 0.9840 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1872  | total loss: \u001b[1m\u001b[32m0.20820\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1872 | loss: 0.20820 - acc: 0.9856 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1873  | total loss: \u001b[1m\u001b[32m0.20161\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 1873 | loss: 0.20161 - acc: 0.9870 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1874  | total loss: \u001b[1m\u001b[32m0.18988\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 1874 | loss: 0.18988 - acc: 0.9883 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1875  | total loss: \u001b[1m\u001b[32m0.18988\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1875 | loss: 0.18988 - acc: 0.9895 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1876  | total loss: \u001b[1m\u001b[32m0.18464\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 1876 | loss: 0.18464 - acc: 0.9905 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1877  | total loss: \u001b[1m\u001b[32m0.17977\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1877 | loss: 0.17977 - acc: 0.9915 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1878  | total loss: \u001b[1m\u001b[32m0.17524\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1878 | loss: 0.17524 - acc: 0.9923 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1879  | total loss: \u001b[1m\u001b[32m0.16705\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 1879 | loss: 0.16705 - acc: 0.9931 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1880  | total loss: \u001b[1m\u001b[32m0.16334\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 1880 | loss: 0.16334 - acc: 0.9938 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1881  | total loss: \u001b[1m\u001b[32m0.15985\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1881 | loss: 0.15985 - acc: 0.9944 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1882  | total loss: \u001b[1m\u001b[32m0.15657\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1882 | loss: 0.15657 - acc: 0.9950 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1883  | total loss: \u001b[1m\u001b[32m0.15347\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1883 | loss: 0.15347 - acc: 0.9955 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1884  | total loss: \u001b[1m\u001b[32m0.15054\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1884 | loss: 0.15054 - acc: 0.9959 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1885  | total loss: \u001b[1m\u001b[32m0.15054\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1885 | loss: 0.15054 - acc: 0.9963 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1886  | total loss: \u001b[1m\u001b[32m0.14777\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 1886 | loss: 0.14777 - acc: 0.9967 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1887  | total loss: \u001b[1m\u001b[32m0.14263\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1887 | loss: 0.14263 - acc: 0.9970 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1888  | total loss: \u001b[1m\u001b[32m0.14024\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1888 | loss: 0.14024 - acc: 0.9973 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1889  | total loss: \u001b[1m\u001b[32m0.14024\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1889 | loss: 0.14024 - acc: 0.9976 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1890  | total loss: \u001b[1m\u001b[32m0.13796\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1890 | loss: 0.13796 - acc: 0.9978 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1891  | total loss: \u001b[1m\u001b[32m0.13578\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1891 | loss: 0.13578 - acc: 0.9982 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1892  | total loss: \u001b[1m\u001b[32m0.13370\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1892 | loss: 0.13370 - acc: 0.9982 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1893  | total loss: \u001b[1m\u001b[32m0.13169\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1893 | loss: 0.13169 - acc: 0.9984 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1894  | total loss: \u001b[1m\u001b[32m0.12977\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1894 | loss: 0.12977 - acc: 0.9986 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1895  | total loss: \u001b[1m\u001b[32m0.12791\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 1895 | loss: 0.12791 - acc: 0.9987 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1896  | total loss: \u001b[1m\u001b[32m0.12613\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1896 | loss: 0.12613 - acc: 0.9989 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1897  | total loss: \u001b[1m\u001b[32m0.12441\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1897 | loss: 0.12441 - acc: 0.9990 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1898  | total loss: \u001b[1m\u001b[32m0.12274\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1898 | loss: 0.12274 - acc: 0.9991 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1899  | total loss: \u001b[1m\u001b[32m0.12113\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 1899 | loss: 0.12113 - acc: 0.9992 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1900  | total loss: \u001b[1m\u001b[32m0.11957\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1900 | loss: 0.11957 - acc: 0.9992 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1901  | total loss: \u001b[1m\u001b[32m0.11806\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1901 | loss: 0.11806 - acc: 0.9993 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1902  | total loss: \u001b[1m\u001b[32m0.11659\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1902 | loss: 0.11659 - acc: 0.9994 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1903  | total loss: \u001b[1m\u001b[32m0.11517\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 1903 | loss: 0.11517 - acc: 0.9995 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1904  | total loss: \u001b[1m\u001b[32m0.11378\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1904 | loss: 0.11378 - acc: 0.9995 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1905  | total loss: \u001b[1m\u001b[32m0.11243\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 1905 | loss: 0.11243 - acc: 0.9996 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1906  | total loss: \u001b[1m\u001b[32m0.11112\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 1906 | loss: 0.11112 - acc: 0.9996 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1907  | total loss: \u001b[1m\u001b[32m0.10984\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1907 | loss: 0.10984 - acc: 0.9996 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1908  | total loss: \u001b[1m\u001b[32m0.10859\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1908 | loss: 0.10859 - acc: 0.9997 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1909  | total loss: \u001b[1m\u001b[32m0.10737\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1909 | loss: 0.10737 - acc: 0.9997 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1910  | total loss: \u001b[1m\u001b[32m0.10618\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1910 | loss: 0.10618 - acc: 0.9997 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1911  | total loss: \u001b[1m\u001b[32m0.10502\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1911 | loss: 0.10502 - acc: 0.9998 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1912  | total loss: \u001b[1m\u001b[32m0.10389\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1912 | loss: 0.10389 - acc: 0.9998 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1913  | total loss: \u001b[1m\u001b[32m0.10278\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1913 | loss: 0.10278 - acc: 0.9998 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1914  | total loss: \u001b[1m\u001b[32m0.10170\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1914 | loss: 0.10170 - acc: 0.9998 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1915  | total loss: \u001b[1m\u001b[32m0.10064\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 1915 | loss: 0.10064 - acc: 0.9998 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1916  | total loss: \u001b[1m\u001b[32m0.09960\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1916 | loss: 0.09960 - acc: 0.9999 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1917  | total loss: \u001b[1m\u001b[32m0.09858\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1917 | loss: 0.09858 - acc: 0.9999 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1918  | total loss: \u001b[1m\u001b[32m0.09758\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1918 | loss: 0.09758 - acc: 0.9999 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1919  | total loss: \u001b[1m\u001b[32m0.09660\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1919 | loss: 0.09660 - acc: 0.9999 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1920  | total loss: \u001b[1m\u001b[32m0.09565\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1920 | loss: 0.09565 - acc: 0.9999 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1921  | total loss: \u001b[1m\u001b[32m0.09471\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1921 | loss: 0.09471 - acc: 0.9999 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1922  | total loss: \u001b[1m\u001b[32m0.09288\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 1922 | loss: 0.09288 - acc: 0.9999 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1923  | total loss: \u001b[1m\u001b[32m0.09288\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1923 | loss: 0.09288 - acc: 0.9999 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1924  | total loss: \u001b[1m\u001b[32m0.09199\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1924 | loss: 0.09199 - acc: 0.9999 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1925  | total loss: \u001b[1m\u001b[32m0.09111\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1925 | loss: 0.09111 - acc: 0.9999 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1926  | total loss: \u001b[1m\u001b[32m0.08941\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1926 | loss: 0.08941 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1927  | total loss: \u001b[1m\u001b[32m0.08941\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1927 | loss: 0.08941 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1928  | total loss: \u001b[1m\u001b[32m0.08858\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 1928 | loss: 0.08858 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1929  | total loss: \u001b[1m\u001b[32m0.08776\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 1929 | loss: 0.08776 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1930  | total loss: \u001b[1m\u001b[32m0.08695\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 1930 | loss: 0.08695 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1931  | total loss: \u001b[1m\u001b[32m0.08616\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1931 | loss: 0.08616 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1932  | total loss: \u001b[1m\u001b[32m0.08538\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 1932 | loss: 0.08538 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1933  | total loss: \u001b[1m\u001b[32m0.08462\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1933 | loss: 0.08462 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1934  | total loss: \u001b[1m\u001b[32m0.08386\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 1934 | loss: 0.08386 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1935  | total loss: \u001b[1m\u001b[32m0.08312\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1935 | loss: 0.08312 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1936  | total loss: \u001b[1m\u001b[32m0.08239\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 1936 | loss: 0.08239 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1937  | total loss: \u001b[1m\u001b[32m0.08167\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1937 | loss: 0.08167 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1938  | total loss: \u001b[1m\u001b[32m0.08096\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 1938 | loss: 0.08096 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1939  | total loss: \u001b[1m\u001b[32m0.08026\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1939 | loss: 0.08026 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1940  | total loss: \u001b[1m\u001b[32m0.07957\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1940 | loss: 0.07957 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1941  | total loss: \u001b[1m\u001b[32m0.07890\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1941 | loss: 0.07890 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1942  | total loss: \u001b[1m\u001b[32m0.07823\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1942 | loss: 0.07823 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1943  | total loss: \u001b[1m\u001b[32m0.07757\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1943 | loss: 0.07757 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1944  | total loss: \u001b[1m\u001b[32m0.07692\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1944 | loss: 0.07692 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1945  | total loss: \u001b[1m\u001b[32m0.07628\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1945 | loss: 0.07628 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1946  | total loss: \u001b[1m\u001b[32m0.07565\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 1946 | loss: 0.07565 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1947  | total loss: \u001b[1m\u001b[32m0.07503\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 1947 | loss: 0.07503 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1948  | total loss: \u001b[1m\u001b[32m0.07441\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1948 | loss: 0.07441 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1949  | total loss: \u001b[1m\u001b[32m0.07381\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 1949 | loss: 0.07381 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1950  | total loss: \u001b[1m\u001b[32m0.07321\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 1950 | loss: 0.07321 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1951  | total loss: \u001b[1m\u001b[32m0.07262\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 1951 | loss: 0.07262 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1952  | total loss: \u001b[1m\u001b[32m0.07204\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1952 | loss: 0.07204 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1953  | total loss: \u001b[1m\u001b[32m0.07147\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1953 | loss: 0.07147 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1954  | total loss: \u001b[1m\u001b[32m0.07091\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 1954 | loss: 0.07091 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1955  | total loss: \u001b[1m\u001b[32m0.07035\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1955 | loss: 0.07035 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1956  | total loss: \u001b[1m\u001b[32m0.06980\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 1956 | loss: 0.06980 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1957  | total loss: \u001b[1m\u001b[32m0.06926\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1957 | loss: 0.06926 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1958  | total loss: \u001b[1m\u001b[32m0.06872\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 1958 | loss: 0.06872 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1959  | total loss: \u001b[1m\u001b[32m0.06820\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1959 | loss: 0.06820 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1960  | total loss: \u001b[1m\u001b[32m0.06768\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1960 | loss: 0.06768 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1961  | total loss: \u001b[1m\u001b[32m0.06716\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 1961 | loss: 0.06716 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1962  | total loss: \u001b[1m\u001b[32m0.06666\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1962 | loss: 0.06666 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1963  | total loss: \u001b[1m\u001b[32m0.06616\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 1963 | loss: 0.06616 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1964  | total loss: \u001b[1m\u001b[32m0.06566\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 1964 | loss: 0.06566 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1965  | total loss: \u001b[1m\u001b[32m0.06518\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1965 | loss: 0.06518 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1966  | total loss: \u001b[1m\u001b[32m0.06470\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1966 | loss: 0.06470 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1967  | total loss: \u001b[1m\u001b[32m0.06423\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 1967 | loss: 0.06423 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1968  | total loss: \u001b[1m\u001b[32m0.06376\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 1968 | loss: 0.06376 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1969  | total loss: \u001b[1m\u001b[32m0.06330\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 1969 | loss: 0.06330 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1970  | total loss: \u001b[1m\u001b[32m0.06285\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1970 | loss: 0.06285 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1971  | total loss: \u001b[1m\u001b[32m0.06240\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 1971 | loss: 0.06240 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1972  | total loss: \u001b[1m\u001b[32m0.06196\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1972 | loss: 0.06196 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1973  | total loss: \u001b[1m\u001b[32m0.06153\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1973 | loss: 0.06153 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1974  | total loss: \u001b[1m\u001b[32m0.06109\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 1974 | loss: 0.06109 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1975  | total loss: \u001b[1m\u001b[32m0.06067\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1975 | loss: 0.06067 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1976  | total loss: \u001b[1m\u001b[32m0.06025\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 1976 | loss: 0.06025 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1977  | total loss: \u001b[1m\u001b[32m0.05984\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1977 | loss: 0.05984 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1978  | total loss: \u001b[1m\u001b[32m0.05943\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1978 | loss: 0.05943 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1979  | total loss: \u001b[1m\u001b[32m0.05902\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1979 | loss: 0.05902 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1980  | total loss: \u001b[1m\u001b[32m0.05862\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1980 | loss: 0.05862 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1981  | total loss: \u001b[1m\u001b[32m0.05823\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 1981 | loss: 0.05823 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1982  | total loss: \u001b[1m\u001b[32m0.05784\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1982 | loss: 0.05784 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1983  | total loss: \u001b[1m\u001b[32m0.05745\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1983 | loss: 0.05745 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1984  | total loss: \u001b[1m\u001b[32m0.05707\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 1984 | loss: 0.05707 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1985  | total loss: \u001b[1m\u001b[32m0.05670\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1985 | loss: 0.05670 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1986  | total loss: \u001b[1m\u001b[32m0.05632\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1986 | loss: 0.05632 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1987  | total loss: \u001b[1m\u001b[32m0.05596\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 1987 | loss: 0.05596 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1988  | total loss: \u001b[1m\u001b[32m0.05559\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1988 | loss: 0.05559 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1989  | total loss: \u001b[1m\u001b[32m0.05523\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1989 | loss: 0.05523 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1990  | total loss: \u001b[1m\u001b[32m0.05488\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1990 | loss: 0.05488 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1991  | total loss: \u001b[1m\u001b[32m0.05452\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1991 | loss: 0.05452 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1992  | total loss: \u001b[1m\u001b[32m0.05418\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1992 | loss: 0.05418 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1993  | total loss: \u001b[1m\u001b[32m0.05383\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1993 | loss: 0.05383 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1994  | total loss: \u001b[1m\u001b[32m0.05316\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1994 | loss: 0.05316 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1995  | total loss: \u001b[1m\u001b[32m0.05316\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1995 | loss: 0.05316 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1996  | total loss: \u001b[1m\u001b[32m0.05249\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1996 | loss: 0.05249 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1997  | total loss: \u001b[1m\u001b[32m0.05249\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1997 | loss: 0.05249 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1998  | total loss: \u001b[1m\u001b[32m0.05217\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 1998 | loss: 0.05217 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 1999  | total loss: \u001b[1m\u001b[32m0.05184\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1999 | loss: 0.05184 - acc: 1.0000 -- iter: 26/26\n",
            "--\n",
            "Training Step: 2000  | total loss: \u001b[1m\u001b[32m0.05153\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 2000 | loss: 0.05153 - acc: 1.0000 -- iter: 26/26\n",
            "--\n"
          ]
        }
      ],
      "source": [
        "if retrain_model:\n",
        "    #Here we train the neural network with the training data we created in the NLP stage\n",
        "\n",
        "    model.fit(x_train, y_train, n_epoch = 2000, batch_size = 32, show_metric = True)\n",
        "    model.save('model.tfl')\n",
        "else:\n",
        "    model.load('./model.tfl')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "-qeITHKBY66A"
      },
      "outputs": [],
      "source": [
        "def text_to_bag(text, all_words):\n",
        "    #Initialize the bag of words by creating an empty slot for every word in the vector\n",
        "    bag_of_words = [0 for i in range(len(all_words))]\n",
        "\n",
        "    #First we split up the input into individual words and stem them so they match the same format as in our vector\n",
        "    text_words = nltk.word_tokenize(text)\n",
        "    text_words = [stemmer.stem(word.lower()) for word in text_words]\n",
        "\n",
        "    #Now we create the bag of words by filling in a 1 for the words that the user used\n",
        "    for word in text_words:\n",
        "        if word in all_words:\n",
        "            bag_of_words[all_words.index(word)] = 1\n",
        "\n",
        "    #And return the bag of words\n",
        "    return np.array(bag_of_words)\n",
        "def chat():\n",
        "    #Starting message\n",
        "    print(\"Enter a message to talk to the bot [type quit to exit].\")\n",
        "\n",
        "    #Reset the context state since there is no context at the beginning of the conversation\n",
        "    context_state = None\n",
        "\n",
        "    #This is what the bot will say if it doesn't understand what the user is saying\n",
        "    default_responses = ['Sorry, Im not sure I know what you mean! You could try rephrasing that or saying something else!',\n",
        "                         'You confuse me human. Lets talk about something else.',\n",
        "                         'Im not sure what that means and I dont really care. Lets talk about something else',\n",
        "                         'I dont understand that! Try rephrasing or saying something else.']\n",
        "\n",
        "    #This chat loop will go on forever until the user types quit\n",
        "    while True:\n",
        "        user_chat = str(input('You: '))\n",
        "        if user_chat.lower() == 'quit':\n",
        "            break\n",
        "\n",
        "        #Convert chat to bag of words\n",
        "        user_chat_bag = text_to_bag(user_chat, all_words)\n",
        "\n",
        "        #Pass bag of words into our neural network\n",
        "        response = model.predict([user_chat_bag])[0]\n",
        "\n",
        "        #Get the intent that the bag of words is most highly correlated with\n",
        "        response_index = np.argmax(response)\n",
        "        response_tag = all_tags[response_index]\n",
        "\n",
        "        #If the neural network is fairly certain that it has chosen the right intent (and isnt randomly guessing)\n",
        "        #In this case, we will only get a response if the neural network is more than 80% certain\n",
        "        if response[response_index] > 0.8:\n",
        "            for intent in intents:\n",
        "                #Get the intent that is predicted\n",
        "                if intent['tag'] == response_tag:\n",
        "                    #Check if this response is associated with a specific context\n",
        "                    if 'context_filter' not in intent or 'context_filter' in intent and intent['context_filter'] == context_state:\n",
        "                        #Get all of the possible responses from this intent\n",
        "                        possible_responses = intent['responses']\n",
        "                        #If this intent is associated with a context set, then set the context state\n",
        "                        if 'context_set' in intent:\n",
        "                            context_state = intent['context_set']\n",
        "                        else:\n",
        "                            context_state = None\n",
        "                        #Select a random message from the intent responses\n",
        "                        print(random.choice(possible_responses))\n",
        "                    else:\n",
        "                        #Print a did not understand message\n",
        "                        print(random.choice(default_responses))\n",
        "        else:\n",
        "            #Print a did not understand message\n",
        "            print(random.choice(default_responses))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "HnJAdRtneh9v",
        "outputId": "43679e26-cdd9-4832-db28-5294775b75b2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter a message to talk to the bot [type quit to exit].\n",
            "Hello!\n",
            "Sorry, Im not sure I know what you mean! You could try rephrasing that or saying something else!\n",
            "We are open 7am-4pm Monday-Friday!\n"
          ]
        }
      ],
      "source": [
        "chat()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO68ovi7O1YqYroa5phGHuW",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}